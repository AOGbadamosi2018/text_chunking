[
  {
    "page_number": 1,
    "content": "AGE THE\nDATA SGIENGE\nINTERVIEW\n201 Real Interview  Questions  Asked By\nTop Tech Companies  & Wall Street Firms\nEx-Facebook,  Now Wall Street = = = +Ex-Facebook,  Ex-Data  Startup"
  },
  {
    "page_number": 2,
    "content": ""
  },
  {
    "page_number": 3,
    "content": "Praise  for Ace the Data Science  Interview\n“The advice  in this book directly  helped  me land my dream  job.”\n— Advitya  Gemawat,  ML Engineer,  Microsoft\n“Super  helpful  career  advice  on breaking  into data, and landing  your  first  job in the field.”\n— Prithika  Hariharan,  President  of Waterloo  Data Science  Club\nData Science  Intern,  Wish\n“Solving  the 201 interview  questions  is helpful  for people  in ALL industries,  not  just tech!”\n— Lars Hulstaert,  Senior  Data Scientist,  Johnson  & Johnson\n“FINALLY!  A book like Cracking  the Coding  Interview\nbut  for Data Science  & ML!”\n——~ Jack Morris,  AI Resident,  Google\n“The authors  explain  exactly  what hiring  managers  look  for —\na must readfor any data  job seeker.”\n— Michelle  Scarbrough\nFormer  Data Analytics  Manager,  F500 Co.\n“Ace the Data Science  Interview  provides  a comprehensive  overview  of the information  an\nacademic  needs to transition  into industry.  I highly  recommend  this book to any graduate  student\nlooking  to navigate  data science  job interviews  and  prepare  for the corporate  sector.”’\n— Lindsay  Warrenburg,  PhD; Data Scientist,  Sonde Health\nHead of Data Science  Projects  at The Erdos Institute\n“What  I found  most compelling  was the love story that unfolds  through  the book.\nFrom the first date to the data science  interview,  Ace reveals  his true character\nand what  follows  is incredible.  I’m thrilled  by their avant garde style\nthat uses career  advice  as a vehicle  for  fictional  narrative.\nOnce  you pick up on it, you feel as though  you're  in\non a secret even the authors  weren t aware of!”\n— Naveen  lyer, Former  Machine  Learning  Engineer,  Instagram\n“Covers  pretty much every topic I've been tested  on\nduring  data science  & analytics  interviews.”\n— Jeffrey Ugochukwu,  Data Analyst  Intern, Federal Reserve;  UC Davis Statistics’  23\n“An invaluable  resource  for the Data Science & ML community.”\n— Aishwarya  Srinivasan,  Al & ML Innovation  Leader,  IBM"
  },
  {
    "page_number": 4,
    "content": "“Highly  recommend  this for aspiring  or current  quantitative  finance professionals.  ”\n— Alex Wang, Portfolio  Analytics  Analyst,  BlackRock\n“I strongly  recommend  this book to both data science\naspirants  and  professionals  in the field.”\n— Chirag Subramanian,  Former  Data Scientist,  Amwins  Group\nGeorgia  Tech, MS in Analytics’  23\n“Perfectly  covers the many real-world  considerations\nwhich ML interview  questions  test  for.”\n— Neha Pusarla,  Data Science  Intern, Regeneron\nColumbia,  MS in Data Science’  21\n“Amazing  tips for creating  portfolio  projects,  and then\nleveraging  them to ace behavioral  interviews.”\n— Jordan Pierre, ML Engineer,  Nationwide  Insurance\n“Nick, Kevin, and this book have been extremely  helpful  resources\nas I navigate  my way into the world  of data science.”\n— Tania Dawood,  USC MS in Communications  Data Science  ‘23\n“Excellent  practice  to keep yourself  sharp  for Wall Street quant\nand data science  interviews!”\n— Mayank  Mahajan,  Data Scientist,  Blackstone\n“The authors  did an amazing  job presenting  the  frameworks  for\nsolving  practical  case study interview  questions  in simple,  digestible  terms. ”’\n—— Rayan Roy, University  of Waterloo  Statistics’  23\n“Navigates  the intricacies  of data science  interviews  without  getting  lost in them.”\n— Sourabh  Shekhar,  Former  Senior  Data Scientist,\nNeustar  & American  Express"
  },
  {
    "page_number": 5,
    "content": "AGE THE\nDATA SCIENCE\nINTERVIEW\n201 Real Interview  Questions  Asked By\nFAANG,  Tech Startups,  & Wall Street\nKEVIN HUO\nEx-Facebook,  Now Hedge Fund\nNICK SINGH\nEx-Facebook,  Now Career  Coach"
  },
  {
    "page_number": 6,
    "content": "About  Kevin Huo\nKevin Huo is currently  a Data Scientist  at a Hedge Fund, and previously  was a Data Scientist\nat Facebook  working on Facebook  Groups. He holds a degree in Computer  Science from the\nUniversity  of Pennsylvania  and a degree in Business  from Wharton.  In college, he interned  at\nFacebook,  Bloomberg,  and on Wall Street. On the side, he’s helped hundreds  of people land data\njobs at companies  including  Apple,  Lyft, and Citadel.\nAbout  Nick Singh\nNick Singh started his career as a Software  Engineer  on Facebook’s  Growth  Team, and most\nrecently,  worked at SafeGraph,  a geospatial  analytics  startup. He holds a degree in Systems\nEngineering  with a minor in Computer  Science  from the University  of Virginia.  In college,  he\ninterned  at Microsoft  and on the Data Infrastructure  team at Google’s  Nest Labs. His career  advice\nhas been read by over 10 million  people  on LinkedIn.\nAll rights reserved.  No part of this book may be used or reproduced  in any manner  without  written\npermission  except in the case of brief  quotations  in critical  articles  or reviews.\nThe author, and/or copyright  holder, assume  no responsibility  for the loss or damage  caused or\nallegedly  caused, directly  or indirectly,  by the use of information  contained  in this book. The\nauthors  specifically  disclaim  any liability  incurred  from the user or application  of the contents  of\nthis book.\nThroughout  this book, trademarked  names are referenced.  Rather  than using a trademark  symbol\nwith every occurence  of a trademarked  name, we state that we are using the names  in an editorial\nfashion  only and to the benefit  of the trademark  owner,  with no intention  of infringement  of the\ntrademark.\nCopyright  2021 Ace the Data Science  Interview.\nAll rights reserved.\nISBN 978-0-578-97383-8"
  },
  {
    "page_number": 7,
    "content": "For my family:  Bin, Jing, Matt, and Allen\n~ Kevin\nFor Mom  and Dad, Priya  and Dev, and my brother,  Naman—\nMy family,  who supports  me in every  endeavor\n~ Nick"
  },
  {
    "page_number": 8,
    "content": "Table of Contents\nINTFOCLUCTION  ......cccccccccccccceccccceccececacccesecccsecencececceseesesseaeeeeseesenseeseeseesneeeees vii-x\nCareer  Advice  to Ace the Data Science  Job Hunt\nChapter  1\n4 Resume  Principles  to Live by for Data Scientists  .............:: ee eeeeeeeeeeen  1-10\nChapter  2\nHow to Make Kick-Ass  Portfolio  Projects...........cccccceesssseeeseeeeeeeeeeeeneees  11-16\nChapter  3\nCold Email Your Way to Your Dream Job in Data... cee ec cece eens 17-26\nChapter  4\nAce the Behavioral  Interview  ..........c:cccccccccessssteeseeceeeaeeeseeceeeaeneseeeaeneees  27-34\nAce the Technical  Data Science  Interview\nChapter  5\nProbability  .......cccccccseccceeeccecceeeseeeeeeeeeseeseaeeeeeseesseeeseeeesseeeseessaaeeeeeees  35-52\nChapter  6\nStatiStiCS  oo... ccccccccscccceeccceesseesecseecsueseesecessceesesseseeeccseussaussuuceuneenecasenes  93-76\nChapter  7\nMachine  Learning........cccccsssscscsssccsssccceecesececeeceseeeceesssssssceeeeuaaaaanenenses  77-140\nChapter  8\nSOL & DB DeSIQN  200...  cccccccccssessssssssssseseseeeeececcessuseeauaaeeseeeeeesens  141-180\nChapter  9\nCOIN...  ccceeeesseceeesssssseececessssesseesesccseessssnsseeseeeeeseseseetetststttttsaaannes  181-232\nChapter  10\ned £0X0 [0 [08 S11 0 — 233-270\nChapter  11\nCase Studi@S...... ee cccccccccccccccssssseeeecesesseuuuunesessseseeuneeeeseseesereeccee. 271-290"
  },
  {
    "page_number": 9,
    "content": "Introduction\nData scientists  are not only privileged  to be solving  some of the most intellectually  stimulating  and\nimpactful  problems  in the world — they’re  getting  paid very well for it too. At Facebook,  the median\ntotal compensation  reported  by Levels.fyi  for a Senior Data Scientist  is a whopping  $253,000  per\nyear. According  to data on Glassdoor,  the median  base salary for a Quantitative  Researcher  at hedge\nfund Two Sigma is $173,000  per year, with opportunities  to DOUBLE  take-home  pay thanks to\ngenerous  performance  bonuses.\nGiven  how intellectually  stimulating  and damn lucrative  data science  is, it shouldn’t  be a surprise  that\ncompetition  for these top data jobs is fierce. Between  “entry-level”  positions  in data science  weirdly\nexpecting  multiple  years of experience,  and these entry-level  jobs themselves  being relatively  rare\nin this field saturated  with Ph.D. holders,  early-career  data scientists  face hurdles in even landing\ninterviews  at many firms.\nWorse,  job seekers  at all experience  levels  face obstacles  with online  applications,  likely  never  hearing\nback from most jobs they apply to. Sometimes,  this is due to undiagnosed  weaknesses  in a data\nscientist’s  resume,  causing  recruiters  to pass on talented  candidates.  But often, it’s simply because\ncandidates  aren’t able to stand out from the sea of  candidates  an online  job application  attracts.  Forget\nabout acing the data science  interview  — given the amount  of job hunting  challenges  a data scientist\nfaces,  just getting  an interview  al a top firm can be considered  an achievement  in itself.\nThen there’s the question  of actually  passing the rigorous  technical  interviews.  In an effort to\nminimize  false positives  (aka “bad hires”), top companies  run everyone  from interns to industry\nveterans  through  tough technical  challenges  to filter out weak candidates.  These interviews  cover\na lot of topics because  the data science  role 1s itself so nebulous  and varied — what one company\ncalls a data scientist,  another  company  might call a data analyst,  data engineer,  or machine  learning\nengineer.  Only after passing  these onerous  technical  interviews  — often three or four on the same\nday — can you land your dream  job in data science.\nWe know this all must sound  daunting.  Spoiler  alert: it is!\nThe good news is that in this book we teach you exactly how to navigate  the data science job\nsearch so that you can land more interviews  in the first place. We’ve put together  a shortlist  of the\nmost essential  topics to brush up on as you prepare for your interviews  so that you can ace these\ntough technical  questions.  Most importantly,  to put your technical  skills to the test, we included\n201 interview  questions  from real data scientist  interviews.  By solving  actual problems  from FANG\ncompanies,  Silicon  Valley darlings  like Airbnb  and Robinhood,  and Wall Street firms like Two Sigma\nand Citadel,  we’re confident  our book will prepare you to Ace the Data Science Interview  and help\nyou land your dream  job tn data.\nWho Are We?\nWho are we, and how’d we find ourselves  writing  this book?\nI (Nick) have worked in various data-related  roles. My first internship  was at a defense contractor,\nCCRi, where I did data science  work for the U.S. Intelligence  Community.  Later in college,  I interned\nas a software  engineer  at Microsoft  and at Google’s  Nest Labs, doing data infrastructure  engineering.\nAfter graduating  from the University  of Virginia  with a degree in systems  engineering,  | started my\nvii"
  },
  {
    "page_number": 10,
    "content": "full-time  career as a new grad software  engineer  on Facebook’s  Growth  team. There, I implemented\nfeatures  and ran A/B tests to boost new user retention.\nAfter Facebook,  I found myself hungry to learn the business side of data, so | joined geospatial\nanalytics  startup SafeGraph  as their first marketing  hire. There, I helped data science and machine\nlearning teams at Fortune 500 retailers, hedge funds, and ad-tech startups learn about SafeGraph’s\nlocation  analytics  datasets.\nOn the side, I started to write about my career  journey,  and all the lessons I learned from being both\na job applicant  and an interviewer.  Ten million  views on LinkedIn  later, it’s obvious  the advice struck\na nerve. From posting on LinkedIn,  and sending emails to my tech careers newsletter  with 45,000\nsubscribers,  I’ve been privileged  to meet and help thousands  of technical  folks along their career\njourney.  But, there was one BIG problem.\nAs a mentor, | was able to point software  engineers  and product managers  to many resources  for\ninterview  prep and career guidance,  like Cracking  the Coding  Interview,  Cracking  the PM Interview,\nand LeetCode.  But, from helping data scientists, | realized  just how confusing  the data science\ninterview  process  was, and how little quality material  was out there to help people land jobs in data.\nI reached  out to Kevin to see if he felt the same way.\nYou might be wondering...\nWhy’d  I turn to Kevin?\nFor several  reasons!  We’re  longtime  friends,  having  attended  high school  together  in Northern  Virginia\nat Thomas  Jetferson  High School for Science  and Technology.  Though  we went our separate  ways\nfor college,  we became  close friends  once again after becoming  roommates  in Palo Alto, California,\nwhen we both worked  for Facebook  as new grads, and bonded  over our shared love of Drake.\nBy living with Kevin, I learned  firsthand  three things about him:\n!. Kevin is an expert data scientist.\n2. Kevin loves helping  people.\n3. Kevin is a fantastic  freestyle  rapper.\nBecause  my rapping  skills paled in comparison  to Kevin’s,  and J can’t sing worth a damn (even\nthough my last name is Singh), it made sense to delay the mixtape  and instead  focus on our other\nshared  passion:  helping  people  navigate  the data science  job hunt.\nKevin has successfully  landed  multiple  offers in the data world. It started  in college,  when he interned\non the Ad Fraud team at Facebook.  After graduating  from the University  of Pennsylvania  with a\nmajor in computer  science,  and a degree  in business  from Wharton,  Kevin started  his career  as a data\nscientist  at Facebook,  where he worked  on reducing  bad actors and harmful  content  on the Facebook\nGroups’  platform.  After a year, Wall Street came calling.  Kevin currently  works as a data scientist  at\na hedge fund in New York.\nOn the side, Kevin combined  his passion  for data science  and helping  people,  which led him to found\nDataSciencePrep.com,  become a course creator on DataCamp,  and coach dozens of people in their\ndata science  careers.\nviii"
  },
  {
    "page_number": 11,
    "content": "Ace the Data  Science  Interview  results  from Kevin’s  and my experience  working  in Silicon  Valley  and\nWall Street,  the insights  we’ve garnered  from networking  with recruiters  and data science  managers,\nour personal  experience  coaching  hundreds  of data scientists  to land their dream  role, and our shared\nfrustration  with the career  and interview  guidance  that’s been available  to Data Scientists  — that is,\nuntil now!\nWhat Exactly  Do We Cover?\nWe start with a “behind  the scenes”  look at how recruiters  and hiring managers  at top companies\nevaluate  your resumes  and portfolios  so you can see what it takes to stand out from the rest. After\nreviewing  hundreds  of resumes,  we've seen technical  folks make the same mistakes  over and over\nagain. But not you, after you follow  the advice  in Chapter  1.\nIn Chapter  2, we show you how to make kick-ass  portfolio  projects.  These projects  will leap off the\nresume,  and will make any person reading  your application  want to interview  you. A well-crafted\nportfolio  project  will also help you ace the behavioral  interview.\nBut how do we get folks to read your application  in the first place?\nIn Chapter  3, we teach you how to cold-email  your way to your dream  job in data. We give you a new\nway of finding  a job so that you don’t have to keep applying  online and getting  ghosted.  By getting\nto the top of the recruiter’s  and hiring  manager’s  email inbox,  you’ll get noticed,  start the networking\nprocess  early, and often get an inside scoop into the role.\nNext comes Chapter  4: Ace the Behavioral  Interview.  While there’s no one right answer  to “tell me\nabout yourself”  or “do you have any questions  for us,” there are plenty of wrong ways to approach\nthese conversations.  Learn how to avoid these mistakes,  craft a better personal  story, and tailor your\nanswers,  so that the interviewer  is left thinking  you’re  born for the role.\nFinally, we’re ready for the trickiest  part of the data science  job hunt, and the meat of our book:\nacing the technical  data science interview.  Chapters  5-10 give you an overview  of the common\ntechnical  subjects  asked during data science  interviews.  We detail what to brush up on and what to\nskip, for topics within Probability,  Statistics,  Machine  Learning,  SQL & Database  Design,  Coding,\nand Product  Sense. In Chapter  11 — the boss chapter  --- we cover how to approach  open-ended  case\nquestions,  which blend multiple  topics into one big problem.\nEach of these technical  chapters  also tests your knowledge  with real interview  questions  asked by\ntech behemoths  like Facebook,  Google, Amazon,  and Microsoft,  mid-sized  tech companies  like\nStripe, Robinhood  and Palantir,  and Wall Street’s  biggest  banks and funds like Goldman  Sachs, Two\nSigma, and Citadel.  With problems  organized  into easy, medium,  and hard difficulty  levels, there ts\nsomething  to learn for everyone  from the data science  neophyte  all the way to a Kaggle  champion.  If\nyou get stuck, there’s nothing  to fear, as each problem  has a fully worked-out  solution  too.\nAdditional  Resources  to Accompany  the Book\nAlongside  reading this book, you are 94.6% encouraged  to join our Instagram community  at\ninstagram.com/acedatascienceinterviews  for additional  career tips, interview  problems,  memes, and\nthe chance to see our glowing  faces from time to time when we flex on the gram."
  },
  {
    "page_number": 12,
    "content": "Also, make sure you’ve  subscribed  to Nick’s  monthly  career  advice  email newsletter:  nicksingh.com/signup\nIt’s just one email a month with the latest tips, resources,  and guides to help you excel in your\ntechnical  career.\nAnd speaking  of email, if you have suggestions,  find any mistakes,  have success  stories to share, or\njust want to say hello, send us an email: hello@acethedatascienceinterview.com  or feel free to connect\nwith us on social media.\nNick Singh\nnicksingh.com  — where I blog my long-form  essays and career  guides\nlinkedin.com/in/Nipun-Singh  —- where I share career  advice  daily (please  send a connection  request\nwith a message  that you've  got the book. I’m close to the 30k connection  limit so don’t want to miss\nyour connection  request!)\ninstagram.com/DJLilSingh  — for a glimpse  into my hobbies  (DJing  and being Drake’s  #1 stan)\ntwitter.com/NipunF  Singh  — for tweets on careers  and tech startups\nKevin Huo\nlinkedin.com/in/Kevin-Huo\ninstagram.com/K  whuo"
  },
  {
    "page_number": 13,
    "content": "4 Resume  Principles\nto Live by for Data Scientists\nCHAPTER  1\nBefore  you kick off the job hunt, get your resume  in order. Time and effort spent here can\npay rich dividends  later on in the process.  No one is going  to grant  you an interview  if your\nresume  doesnt  scream  success  and competence.  So here are four principles  your resume\nshould  live by, along with miscellaneous  hacks to level up. We even included  our actual\nresumes  from our senior  year of college  to show you how, in real life, we practiced  what we\npreach  to land our dream  jobs at Facebook.\nPrinciple  #1: The Sole Purpose  of Your Resume  Is to Land\nan Interview\nNo resume  results in an immediate job offer; that isn’t its role. What your resume  must do 1s convince\nits recipient  to take a closer look at you. During the interview  process,  your data science chops and\npeople skills will carry you toward an offer. Your resume merely opens the door to the interview\nprocess.  In practice,  that means keeping  your resume short! One page if you have under a decade of\nexperience,  and two pages if more. Save whatever  else you want to say for your in-person  interview\nwhen you’ll be given ample time to get into the weeds and impress  the interviewer  with your breadth\nof knowledge  and experience.\nSince the main aim of your resume is to land an interview,  the best way to do that is by highlighting  a\nfew of your best achievements.  It’s crucial that these highlights  are as easy and as obvious  as possible\nto find so that the person reading  your resume  decides to grant you an interview.  Recruiters  are busy\nAce the Data Science  Interview ]"
  },
  {
    "page_number": 14,
    "content": "CHAPTER  1: 4 RESUME  PRINCIPLES  TO LIVE BY FOR DATA SCIENTISTS\npeople who often only have 10 seconds or less to review your resume and decide whether  to give\nyou an interview.  Keeping  the resume short and removing  fluff is key to making sure your highlights\nshine through  in the short timespan  when  a recruiter  is evaluating  your resume.\nOne way to shorten the resume  and keep it focused  on the highlights  is by omitting  non-relevant  jobs.\nGoogle  doesn’t care that you were a lifeguard  three summers  ago. The exception  to this rule is if you\nhave zero job experience.  If that’s the case, do include the job on your resume to prove that you've\nheld a position  that required  a modicum  of “adulting.”\nAnother  way to make sure your resume lands you an interview  is by tailoring  the resume to the\nspecific  job and company.  You want the recruiter  reading  your resume  to think you are a perfect  fit for\nthe position  you’re gunning  for. For example,  say you worked as a social media assistant  part-time\nthree years ago promoting  your aunt’s restaurant.  This experience  isn’t relevant  for most data science\nroles and can be left off your resume. But if you’re applying  to Facebook  and interested  in ads, it’s\nworth including.  If you’re applying  to be a data analyst  on a marketing  analytics  team, it can help to\nleave in the social media marketing  job.\nAnother  resume customization  example:  when I (Nick) applied  to government  contractors  with my\nforeign-sounding  legal name (Nipun Singh), I put “U.S. Citizen”  at the top of my resume.  This way,\nthe recruiter  knew | had the proper background  to work on sensitive  projects,  and could be eligible\nlater for a top secret security  clearance.\nPrinciple  #2: Build Your Resume  to Impress  the Recruiter\nThe person you most need to impress  with your resume  is a nontechnical  recruiter.  The senior data\nscientist  or hiring manager  at the company  you want to work for is NOT your target audience  —\nthey will have a chance  to review your experience  in-depth  during your on-site interview.  As such,\nspell out technical  acronyms  if they aren’t obvious.  Give  a little background;  don’t just assume  the\nrecruiter  will understand  what you did and why it’s impressive.  For example,  a research  project  called\n“Continuous  Deep Q-Learning  with Model-Based  Acceleration”  doesn’t  make sense to most people.\nBut a “Flappy  Bird Bot Using Machine  Learning”  is more memorable  and intriguing  to the average\nnontechnical  recruiter.\nDo you know what else recruiters  love (along with hiring managers  and execs)?\nNumbers.  BIG numbers!\nDon’t be afraid to report usage or view counts for your projects.  Talking  about user metrics  shows\nthat you drove a project to completion,  and got it in front of real people  who were impacted  by your\nwork. Even if the project is technically  straightforward,  real user or view counts go a long way in\nhelping  a recruiter  understand  you made something  of value.\nFor example,  in college,  I (Nick) made RapStock.io.  The website  didn’t have a sleek UI, and the\nunderlying  code wasn't  complex,  but at its peak, it had 2,000 Monthly  Active  Users. This expenence\nopened  many doors and gave me a great story to tell because  recruiters  realized  I'd actually  shipped\nsomething  of value before.\nAn even better way to impress  the recruiter  is if you can quantify  your impact in business  terms:\nwrite out the specific  dollars  earned,  or dollars  saved,  due to your work. Most recruiters  would  prefer\nto read about the analysis  you did that led to $20,000  in savings  for a business  rather than that small\nlittle side project  where you solved  P vs. NP.\n2 Ace the Data Science  Interview | 4 Resume  Principles  to Live by for Data Scientists"
  },
  {
    "page_number": 15,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nWhy? Because  it’s hard to explain  the P vs. NP problem  and why it matters  on a resume,  unless you\ndescribe  it with secondary  results like, “Won a Nobel Prize.”  But $20,000  cash is $20,000  cash; no\nexplanation  needed.  And because  most data science  job seekers  are applying  to businesses,  talking\nabout real value you’ve  generated  in the past gives businesses  confidence  that you'll do the same\nat their company.  The truth is that, ultimately,  what the recruiter  is looking  for isn’t necessarily  an\nexpert data scientist  per se, but someone  who will move the business  and product  forward  — who\njust so happens  to do it using data science!\nPrinciple  #3: Only Include  Things  That Make  You Look Good\nYour resume  should make you shine, so don’t be humble  or play it cool. If you deserve  credit for\nsomething,  put it on your resume.  But never lie or overstate  the truth. It’s easy for a recruiter  or other\ncompany  employee  to chat with you about projects  and quickly  determine  if you did it all or if it was\na three-person  group project  you’re  passing  off as your own. And technical  interviewers  love to ask\nprobing  questions  about projects,  so don’t overstate  your contributions.\nAnother  way to look good is to not volunteer  negative  information.  Sounds  obvious  enough,  but I’ve\nseen people  make this mistake  often. For example,  you don’t have to list your GPA! Only write down\nyour GPA if it helps. A 3.2 at MIT might be okay to list, but a 3.2 at a local lesser-known  college\nmight not be worth listing if you are applying  to Google,  especially  if Google  doesn’t  usually  recruit\nat your college.  Why? Because  Google  being Google  might be expecting  you to be at the top of your\nclass with a 4.0 coming  from a non-target  university.  As a result, a 3.2 might look bad.\nAvoid Neutral  Information\nA mistake  more common  than volunteering  negative  information  1s adding neutral details to your\nresume.  Drowning  out your accomplishments  with irrelevant  information  detracts  from your resume\nin much the same way as volunteering  negative  information.  Remember:  the average  recruiter  will\nonly spend around 10 seconds  skimming  your resume,  so you can’t afford for them to be distracted\nand miss your strongest  points.\nOne big source of neutral information  is the summary  or objective  section at the top. Your aim must\nbe to look exceptional,  not typical or neutral. How boring and undifferentiated  is: “hard-working,\nresults-oriented  analytics  professional  looking  for a Data Science  job starting  fall 2022.” Worse, this\nsection is usually  right at the top of a resume,  taking up valuable  real estate. Get rid of it completely!\nYou Probably  Don’t Need a Skills or Technologies  Section\nAnother  section of the resume packed with neutral details is the skills and technologies  section.\nTraditional  resume advice says to jampack  keywords  in here. We disagree.  You can eliminate  this\nsection entirely or shorten it to two lines max (if you do choose to include it). There are several\nreasons  why we advocate  shortening  or removing  the skills and technologies  section.\nFirst off, we need to address why traditional  resume advice advocates  for including  this section. The\nreasoning  is that to please the application  tracking  system (ATS), which has an algorithm  that flags a\nrecruiter  that your application  is relevant  to the job description  you applied for, you need to stuff your\nresume with keywords.  We don’t agree with this advice. As we detail in Chapter  3, applying  online 1S\nan ineffective  strategy,  so pleasing  the ATS isn’t paramount.  Also, anything  you list on your resume\nis fair game for the interviewer  to ask you about. Filling this secuon with tools you aren’t familiar\nwith to please the ATS algorithm  can easily backfire  come interview  time.\nAce the Data Science  Interview 3"
  },
  {
    "page_number": 16,
    "content": "CHAPTER  1: 4 RESUME  PRINCIPLES  TO LIVE BY FOR DATA SCIENTISTS\nAnother  reason to get rid of the skills section: remember  “Show and Tell” in grade school?  Well, it’s\nstill way better to show rather than just to tell! Include the technologies  inline with your past work\nexperience  or portfolio  projects. Listing the tech stack this way contextualizes  the work you did\nand shows an interviewer  what you’re able to achieve  with different  tools. Plus, in explaining  your\nprojects  and past work experiences,  you’!l have enough  keywords  covered  to appease  the ATS which\ntraditional  career  advice  is overly  focused  on.\nThe last reason to ditch a big skills and technologies  section is that you are expected  to learn new\nlanguages  and frameworks  quickly at most companies.  The specific tools you already know are\nhelpful but not crucial to landing  a job. You are expected  to be an expert at data science  in general,\nnot in specific  tools. Plus, at large tech companies  like Facebook  and Google,  the tools and systems\nare often proprietary.  Thus, it doesn’t matter much about the specific  tools you know — it’s about\nwhat you’ve  actually  accomplished  with those tools in the past.\nMistakes  College  Students  and New Grads  Often Make\nListing too many volunteer  experiences  and club involvements  from high school and college is a\nfrequent  mistake  we see college  students  and new grads make on their resume.  They still think it’s\nlike college  applications,  where listing your involvement  in varsity soccer, piano lessons,  and the\nNational  Honor Society means something.  It’s great that you are a civically  engaged,  respectable\nhuman involved  with your community,  but competitive  tech companies  and Wall Street firms are\nselecting  you for your ability to tum data into insight,  and not much other than that. Attending  ACM\nmeetings  or going to the data science  club is practically  worthless  as far as resume  material  goes.\nUnless  your involvement  was significant,  don’t list it.\nAnother  source  of potentially  irrelevant  details:  things from a long time ago, like your SAT score, or\nwhich high school you attended.  One caveat  that traditional  career  services  folks don’t tell you to do:\nleave in details from high school if they are exceptional.\nGot more than a 2350 (or 1450) on your SAT? Leave it in!\nDid well in USAMO,  ISEF, or Putnam?  Leave it in!\nAre you a NCAA  athlete  or attended  college  on a full-ride  merit scholarship?  Leave it in!\nSame goes 1f you attended  a prestigious  high school like Phillips  Exeter,  Harker,  or Thomas  Jefferson\nHigh School  for Science  & Technology  (go Colonials!).\nWe've found that at clite tech companies  and Wall Street firms, the interviewers  went to these same\nschools  and won these same competitions.  It may be okay to keep on your resume  even if it’s from a\nlong while ago, provided  it doesn’t  take too much space.\nAnd if none of  this applies  to you, don’t worry -- there’s  so much more to you and your resume  than\nthe brand names  you’ ve listed.\nPrinciple  #4: Break  Formatting  and Convention  to Your Favor\nWe believe traditional  resume advice is too rigid, with a one-size-fits-al]  approach.  If breaking\nconventional  writing  rules makes  your resume  more readable,  then do it!\nTypically,  bold font 1s reserved  for section  headings.  But feel free to bold any words or phrases  that\nmake your resume  easier to understand  so the interviewer  can decide  — in 10 seconds  or less! —— to\ninterview  you.\n4 Ace the Data Science  Interview  | 4 Resume  Principles  to Live by for Data Scientists"
  },
  {
    "page_number": 17,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nFor example,  back in college,  I (Nick)  made the size of the companies  I worked  at bigger  than other\ntext. This way, while scanning  my resume,  a recruiter  could quickly  see and say, “Microsoft  Intern.\nCheck. Google Intern. Check. Okay, let’s interview  this kid.” I also bolded the user metrics for\nRapStock.io  on my resume.  | wanted  to quickly  call out this information  because  it was sandwiched\nbetween  additional  details  about my projects.\nAnother  resume  convention  you can break to your favor is section  order. There is no hard and fast\nrule on ordering  your education,  work experience,  projects,  and skills sections.  Just remember:  in\nEnglish,  we scan from top to bottom.  So list your best things at the top of the resume.\nKeeping  what’s most important  up top is an important  piece of advice to remember,  since it may\nconflict  with advice  from many college  career  advisors  who suggest  listing  your university  at the top.\nFor example,  if you currently  attend a small unknown  university,  but interned  at Uber this summer,\ndon’t be afraid to list your work experience  with Uber at the top, and move education  to the bottom.\nWent to MIT but have no relevant  industry  internships  yet? Then it’s fine to lead with your education,\nand not the work experience  section.\nAnother  resume  rule on ordering  you can break:  items  within  a section  don’t  have to be in chronological\norder! For example,  | had a friend who interned  at Google  one summer,  then interned  part-time  at a\nsmall local startup  later that fall. It’s okay to keep Google  at the top of the resume,  ahead  of the small\nlocal startup,  even though the startup experience  was more recent work experience.  List first what\nmakes  you look the best.\nAnother  convention  you can safely break is keeping  standard  margins  and spacing.  Change  margins\nto your favor, to give yourself  more space and breathing  room. You can also use different  font sizes\nand spacing  to emphasize  other parts of your resume.  Just don’t use margin  changes  to double the\ncontent  on your one-page  resume.  If you do, you're likely drowning  out your best content,  which is\na big no-no (as mentioned  in Principle  #3).\nOh, and speaking  of breaking  the resume  rules: you can ignore  any of the tips I’ve listed earlier  in this\nchapter  if it helps you tell a better story on your resume. Earlier, 1 mentioned  that listing irrelevant\njobs — like being a waiter — won’t help you land a data science  gig. But go ahead and list it if,\nfor example,  you were a waiter who then built a project that analyzed  the data behind restaurant\nfood waste. Same way, there’s nothing  wrong with listing the waiter position  if you're applying  to\nDoorDash  or Uber Eats. If listing something  helps tell the story of you, leave it in.\nSodon’tlisten  to folks who tell you that linking  to your SoundCloud  from your  resume  is unprofessional.\nSuppose  you’ve made a hackathon  project around music, or are applying  to a company  like Spotify.\nIn that case, it’s perfectly  fine to list the SoundCloud  link since it shows a recruiter  that you followed\nyour passions  and created projects  to further your interests.  And by the way, if your mixtape  is fire,\nplease email us the link to hello@acethedatascienceinterview.com  and we’ ll give it a listen.\nMiscellaneous  Resume  Hacks\nMake Your Name and Email Prominent\nSelf-explanatory.  Near your name, add a professional  email address  too. Also, do not use Yahoo mail\nor Hotmail  addresses.  Silicon Valley tech folks are especially  judgy about this.\nNever Include  Your Mailing  Address\nCompanies  are biased toward hiring local candidates  because  they don’t need to pay relocation  fees.\nAnd recruiters  are compensated  based on the number of candidates  they can close. So, put yourself\nAce the Data Science  Interview 5"
  },
  {
    "page_number": 18,
    "content": "CHAPTER  1 : 4 RESUME  PRINCIPLES  TO LIVE BY FOR DATA SCIENTISTS\nin a recruiter’s  chair. Let’s say you’re a Silicon Valley-based  company  recruiter  with two identically-\nskilled candidates,  but one lives in the Bay Area and the other lives in NYC. Which of the two are\nyou more likely to close? The NYC candidate  who needs to decide to move to SF and uproot her\nfamily before accepting  your offer or the local person who can take your offer and start next week?\nDon’t List Your Phone Number  Unless  It’s Local\nBecause  of the spam robocall epidemic,  anyone who calls you will email you first to ask for your\nphone number  and set up a time. So, there’s no need to list it. Remember:  you have 10 seconds to\nrivet someone  reading your resume. Don’t waste a second of their time by presenting  nonessential\ninformation.  Plus, if your phone number  is international,  it’ll hurt even more, as often there’s a bias\nto hire local candidates.  And yes, hiring managers  and recruiters  do notice when your number  is from\na far-flung  area code.\nInclude  Your GitHub\nYour GitHub  link doesn’t  need to be super prominent.  It’s okay if your GitHub  is a bit messy. Merely\nhaving a GitHub listed is a sign that you have done work out in the open and that you’re aware of\nversion  control.  And remember:  since the first interview  gatekeeper  is a nontechnical  recruiter,  he or\nshe most likely won’t be able to tell a messy GitHub  from an okay one anyway.\nHyperlink  to Outside  Work Where  Possible\nInclude links to your data analysis  blogs or hackathon  projects  whenever  possible.  Linking  to the\noutside  world validates  to the recruiter  that you made something  and published  it to the real world.\nEven if the recruiter  never clicks on a link, just seeing the blue hyperlink  will give them a sense that\nthere’s  something  more there.\nSave Your Resumes  as PDFs with Good Names\nPDFs maintain  formatting  better and lead to better viewing  experiences  on mobile.\nAnd be sure to save your resumes  as “First Name Last Name Company  Name Resume.”\nWhen people  name it “Resume.pdf,”  it implies  you’re  applying  to many  jobs willy-nilly.  You should\nbe customizing  your resumes  for specific companies  and roles anyway,  so saving them with the\ncompany  name included  will help you be more organized.\nResume  Hacks  for IRL\nThe first hack for sharing  a resume  in real life is to print your resume  on heavier  paper. Doing this\nwill make the resume  stand out, and it will feel more professional.  Next, bring your resume  to every\nplace where you'll be meeting  hiring managers  and recruiters:  job fairs, coffee chats, conferences,\nmeetups,  and, of course,  onsite interviews.\nYou might be skeptical  of the value of doing  this, especially  since your LinkedIn  and personal  website\nmight have all the same information,  but here’s the main reason:  you don’t want to have to pull these\nup on your phone if someone  is curious.  In many contexts,  it just is way easier to hand them a piece\nof paper.\nAnd there’s a subtle reason for doing this too. Even if the person you are trying to network  with\ndoesn't  read your resume  in the moment,  they’t! likely hang onto it and read it later while wailing  for\ntheir Uber or when they're  stuck at the airport. It's a physical  memento  you gave them, and they'll\nbe more likely to remember  you and respond  to your email follow-ups  later (covered  in Chapter  3).\n6 Ace the Data Science  Interview | 4 Resume  Principles  to Live by for Data Scientists"
  },
  {
    "page_number": 19,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nAnd lastly, since you’re  carrying  your resume  everywhere,  get a folio to carry it. Crumpled  resumes\nlook unprofessional.  If you are in college  or are a new grad, and went to a top school,  get a leather\npadfolio  with the school logo front and center  for the subtle flex.\nNick’s  Facebook  Resume  (Senior  Year, Fall Semester)\nNick (Nipun)  Singh\nns2se@virginia.edu,  nipunsingh.com,  github.com/NipunSingh\nExperience:\n¢ Reduced latency from 45 seconds to 80 milliseconds  for a new monitoring  dashboard  for\npayments  team\n¢ Did the above by developing  an effcient  ASP.NET  Web API in C# which leveraged  caching  and\npre-processing  of payment  data queried from Cosmos  via Scope (Microsoft  internal versions  of\nHadoop  File System  and Hive)\n¢ Worked  on a NLP algorithm  for a contract  with the Office of Naval Research\n* Improved  Fl measure  of algorithm  70% compared  to the original  geo-location  algorithm  used by\nNorthrup  Grumman,  by designing  new algorithm  in Scala which used Stanford  NLP package  to\ngeo-locate  events in news\n* Grew site to 2,000 Monthly  Active Users, and received 150,000  page views\n¢ Developed  using Python (Django), d3.js, JQuery, Bootstrap, PostgreSQL,  and deployed to\nHeroku\n* Game similar to fantasy football but players bet on the real world popularity  and commercial\nsuccess of rappers -— the rappers' performance  1s based on metrics scraped from Spotify and\nBillboards\n» “Great to see that folks stick around” - Alexis Ohanian, Founder  of Reddit, commenting  on our\nretention  metrics\nAce the Data Science  Interview 7"
  },
  {
    "page_number": 20,
    "content": "¢ Won ‘Best Use Of Amazon  Web Services’  award at the 2,000 person Hack-a-thon\n¢ Created  a website  which takes in a photo and then automatically  captions  it.\n* Used IBM Watson  API and Image recognition  for tagging. Scraped  7,000 jokes and quotes with\nPython. Used and tuned AWS Cloud Search to search the database  of captions. Backend  built\nwith Dyango.\n¢ Languages:  Python  (Django),  Java, Scala, R\n¢ Other: PostgreSQL,  Spark, Google  Cloud Platform,  AWS, Heroku\ncompetitions\nKevin’s  Facebook  Resume  (Senior  Year, Fall Semester)\nKevin Huo\nEDUCATION\nUniversity  of Pennsylvania  - Philadelphia,  PA Graduating:  May 2017\nThe Wharton  School:  BS in Economics  with concentrations  in Statistics  & Finance\nSchool  of Engineering  and Applied  Sciences:  BSE in Computer  Science\nGPA: 3.65/4.00\nHonors:  Dean’s  List (2013-2014),  PennApps  Health  Hack Award  (2014)\nStatistics  Coursework:  Modern  Data Mining,  Statistical  Inference,  Stochastic  Process,  Probability,\nApplied  Probability  Modeling\n8 Ace the Data Science  Interview | 4 Resume  Principles  to Live by for Data Scientists"
  },
  {
    "page_number": 21,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nComputer  Science  Coursework:  Algorithms,  Data Structures,  Automata  and Complexity,\nDatabases,  Intro to Data Science,  Software  Engineering,  Machine  Learning\nFinance  Coursework:  Investment  Management,  Derivatives,  Monetary  Economics\nThomas  Jefferson  High School  for Science  and Technology  - Alexandria,  VA Graduated:  June 2013\nGPA: 4.44/4.00  (Weighted),  SAT: 2350 (Math-800,  Writing-800,  Reading-750)\nHonors:  National  Merit Finalist,  National  AP Scholar,  American  Invitational  Math Exam (AIME)\nQualifier\nWharton  Undergraduate  Data Analytics  Club (Team Leader) January  2016-January  2017\n* Participated  in speaker  series, tech talks, and data hackathons  as a general  member\n* Project  leader  for consulting  group performing  analyses\nFacebook  (Data Science  Intern) June 2016-August  2016\n* Analyzed  fraud within  Atlas by looking  at edge cases among  existing  systems\n* Built cost view of fraud for advertisers  and presented  recommendations  to relevant  teams\n° Technologies  used: SQL, R, Python\nBloomberg  LP (Software  Engineering  Intern) May 2015-August  2015\n* Developed  a contributor  analysis  tool for the Interest  Rate Volatility  Team\n¢ Constructed  various  statistical  metrics  to gauge contributors\n¢ Built UI component  using JavaScript  and in-house  technologies\n¢ Wrote various  Python  scripts  to monitor  metrics\n¢ Technologies  used: Python,  JavaScript\nZetta Mobile  (Software  Engineering  Intern) June-August  2014\n¢ Wrote Python  scripts  to compile  recorded  data from logs of mobile  advertisements\n¢ Used  R to look for useful trends and patterns  in the compiled  data\n¢ Built scripts  to automate  the data analysis  of click-through  rate\n¢ Scripts  are being used in beta-testing  for future automated  data analyses  for the company  to use\n¢ Technologies  used: Python\nComputer  Science  Teaching  Assistant January  2014-December  2016\n¢ Held weekly  recitation  and office hours and responsible  for grading  of homework,  tests, and quizzes\n¢ Discrete  Math (Spring  & Fall 2014), Data Structures  & Algorithms  (Spring/Fall  2015 & 2016)\n) : wo\n¢ Proficient:  Python,  R, SQL, Java, Familiar:  JavaScript,  HTML/CSS,  Basic: OCaml,  Hadoop,  Linux\nAce the Data Science  Interview 9"
  },
  {
    "page_number": 22,
    "content": ""
  },
  {
    "page_number": 23,
    "content": "How to Make Kick-Ass\nPortfolio  Projects\nCHAPTER  2\nUnanimously,  data science hiring managers  have told us that not having portfolio\nprojects was a big red flag on a candidate's  application.  This holds true especially\nfor college students or people new to the industry, who have more to prove. From\nmentoring  many data scientists,  we've found that having kick-ass portfolio  projects\nwas one of the best ways to stand out in the job hunt. And from our own experience,\nwe know that creating  portfolio  projects  is a great way to apply classroom  knowledge\nto real-world  problems  in order to get some practical  experience  under your belt.\nWhichever  way you Slice it, creating  portfolio  projects  is a smart  move. In this chapter,  you'll\nlearn 5 tips to level-up  your data science  and machine  learning  projects  so that recruiters\nand hiring managers  are jumping  at the chance to interview  you. We teach you how to\ncreate,  position,  and market  your data science  project. When done right, these  projects  will\ngive you something  engaging  to discuss  during  your behavioral  interviews.  Plus, they'll\nhelp make sure your cold emails  get answered  (Chapter  3).\nThe High-Level  Philosophy\nAs we discussed  to death in Chapter |, the recruiter  is the person we need to impress  because  they\nare the interview  gatekeeper.  A recruiter  won't dive deep into your Jupyter  Notebook,  look at line\n#94, see the clever model you chose, and then offer you an interview.  That’s not how recruiting  (or\npeople!)  work.\nAce the Data Science  Interview 11"
  },
  {
    "page_number": 24,
    "content": "CHAPTER  2: HOW TO MAKE KICK-ASS  PORTFOLIO  PROJECTS\nThe majority  of recruiters  just read the project description  for 10 seconds in the cold email you send\nthem or when reviewing  your resume. Maybe — if you're lucky —— they click a link to look at a\ngraphic or demo of the project. At this point, usually in under 30 seconds,  they think to themselves,\n“This is neat and relevant  to the job description  at hand,” and decide to give you an interview.\nThus, we're optimizing  our data science portfolio projects to impress the decision-maker  in this\nprocess — the busy recruiter. We’re optimizing  for projects that are easily explainable  via email.\nWe're optimizing  for ideas that are “tweetable”:  ones whose essence can be conveyed  in 140\ncharacters  or less. By having  this focus from day one when you kick off the portfolio  project,  you will\nskyrocket  your chances  of ending up with “kick-ass”  portfolio  project that gets recruiters  hooked.\nDon’t worry if you think that focusing  on the recruiter  will cheapen  your portfolio  project’s  technical\nmerits. Believe  us: the technical  hiring manager  and senior data scientists  interviewing  you will also\nappreciate  how neatly packaged  and easily understandable  your project is. And following  our tips\nwon't stop you from making the project technically  impressive;  an interesting  and understandable\nproject does not need to come at the expense  of demonstrating  strong technical  data science  skills.\nTip #1: Pick a Project  Idea That Makes  for an Interesting  Story\nRecruiters  and hiring managers  are human. Human  beings love to hear and think in terms of stories.\nYou can read the book that’s quickly become a Silicon Valley favorite,  Sapiens:  A Brief  History\nof Humankind,  by Yuval Harari, to understand  how fundamental  storytelling  1s to our success  as a\nspecies. In the book, Harari argues that it’s through  the shared stories we tell each other that Homo\nsapiens  are able to cooperate  on a global scale. We are evolutionarily  hardwired  to listen, remember,\nand tell stories.  So do yourself  a favor and pick ideas to work on which help you tell a powerful  story.\nA powertul  story comes  from making  sure there is a buildup,  then some conflict,  and a nice, satisfying\nresolution  to said conflict.  To apply the elements  of a story to a portfolio  project,  make sure your\nwork has some introductory  exploratory  data analysis  that builds up context  around what you are\ninaking  and why. Then pose a hypothesis,  which is akin to a conflict.  Finally,  share the verdict  of\nyour posed hypothesis  to resolve  the conflict  you posed earlier. By structuring  your portfolio  like a\nstory, itll be easier to talk more eloquently  about your project  in an interview.  Plus, the interviewer\n1s hardwired  to be more interested  — and therefore  more likely to remember  you and your project  —\nwhen you tell it in a format that we’re hardwired  to love.\nSo, how do you discover  projects  that will translate  into captivating  stories?\nLooking  at trending  stories in the news is a great starting point because  they are popular  topics\nthat are easy to storytell  around. For example,  in the fall of 2020, the biggest  news stories were the\nCOVID-19  pandemic  and the 2020 U.S. presidential  election.  Interesting  projects  on these topics\ncould be to look at vaccination  rates by zip code tor other diseases,  and see how they correlate  to\ndemographic  factors in order to understand  healthcare  inequities  and complications  with vaccine\nrollout plans. For the 2020 U.S. presidential  election,  an interesting  project would be to see what\ndemographic  factors  correlate  highest  for a county  flipping  from Donald  Trump  in 2016 to Joe Biden\nIn 2020, and then predicting  which counties  are the most flippable  for future elections.\nIf you ever get stuck on these newsworthy  topics, data journalism  teams at major  outlets  like the New\nyork Times and FiveThirtvEight  have already  made a whole host of visualizations  related to these\nissues. These can serve as inspiration  or as a jumping-off  point for more granular  analysis.\nAnother  easy source of ideas with good story potential  is to think about problems  you personally\nface. You'll have a great story to tell where you're positioned  as the hero problem-solver  if you can\nconvey how annoying  a problem  was to you and that you needed to solve it for yourself  and other\n12 Ace the Data Science  Interview  | How to Make Kick-Ass  Portfolio  Projects"
  },
  {
    "page_number": 25,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nsufferers.  I’ve seen friends  at hackathons  tackle  projects  on mental  health  (something  they personally\nstruggled  with), resulting  in a very powerful  and moving  narrative  to accompany  the technical  demo.\nTip #2: Pick a Project  Idea That Visualizes  Well\nA picture  is worth a thousand  words.  And a GIF is worth a thousand  pictures.  So go with a portfolio\nproject idea that visualizes  well to stand out to recruiters.  Ideally, make a cool GIF, image, or\ninteractive  tool that summarizes  your results.\nI (Nick)  saw the power  of a catchy  visualization  firsthand  at the last company  I worked  at, SafeGraph,\nwhen we launched  a new geospatial  dataset.  When  we just wrote a blog post and put it on SafeGraph’s\nTwitter, we wouldn’t  get much engagement.  But when we included  a GIF of the new dataset\nvisualized,  we’d get way more attention.\nThis phenomenon  wasn’t just isolated to social media — the power of catchy photos and GIFs\neven extended  to cold email. When we’d send sales emails  with a GIF embedded  at the top, we got\nmuch higher  engagement  than when we’d send boring  emails  that only contained  text to announce  a\nproduct.  These marketing  lessons  apply to your data science  portfolio  projects  as well, as you should\nbe emailing  your work to hiring managers  and recruiters  (covered  in detail in Chapter  3). You might\nbe thinking.  “Why are we wasting  time on this and not focusing  on the complicated  technical  skills\nthat a portfolio  project  should  demo?”\nWe want to remind  you: your ability to convey  results succinctly  and accurately  is a very real skill.\nExplaining  your work and presenting  it well is a great signal to companies,  because  real-world\ndata science  means convincing  stakeholders  and decision  makers  to go down  a certain path. Fancy\nmodels are great, but not unless you can easily explain to higher ups their results and business\nimpact. A compelling  visual is one of the easiest  ways to accomplish  that goal in the business  world.\nDemonstrating  this ability  through  a portfolio  project  gives any interviewer  confidence  you'll be able\nto excel at this aspect  of data science  when actually  on the job.\nTip #3: Make Your Project  About  Your Passion\nMaking  your portfolio  project  about your passion  is a cheat code for a whole slew of reasons.  Passion\nis contagious.  If you’re having fun talking about your passion project, chances are those same\ngood vibes will catch on with the interviewer.  Plus, when you work on something  you’re naturally\npassionate  about, it becomes  much easier and more comfortable  for you to talk about the work during\nan otherwise  nerve-wracking  interview.  This effortless  communication  will help you come across as\na more articulate  communicator  — a highly desirable  attribute  for any hire. Making your project\nabout your passion,  and then communicating  this passion,  also leads to a halo effect, where you come\nacross as passionate  for related  things, like the field of data science,  the job at hand, and the company.\nThis passion and enthusiasm  halo effect is especially  crucial to create for more junior data science\ncandidates.  Early-career  data scientists require more hand holding and resources invested by a\ncompany  compared  to experienced  hires. From talking to hiring managers,  we found that they chose\nto invest in more junior candidates  when the candidate  displayed  high amounts  of enthusiasm  and\npassion.  This enthusiasm  and passion is a great signal that the junior candidate  will be motivated  to\nlearn quickly  and close the skill gap fast. Thus, by signaling  passion  by working  on passion  projects,\nyou help make companies  want to invest in you over a more senior candidate  who might have more\ntechnical  skills but lacks the same interest  in the field.\nWhat does this advice mean in practice?  If you love basketball,  then use datasets from the NBA in\nyour portfolio  projects.  Passionate  about music? Classify  songs into genres based on their lyrics.\nAce the Data Science  Interview 13"
  },
  {
    "page_number": 26,
    "content": "CHAPTER  2: HOW TO MAKE KICK-ASS  PORTFOLIO  PROJECTS\nBinge Netflix shows? Take the IMDB movies dataset and make your own movie recommender\nalgorithm.\nFor example,  I (Nick) — a passionate  hip hop music fan and DJ that’s always  on the hunt for upcoming\nartists and new music — made RapStock.io,  a platform  to bet on upcoming  rappers. When talking\nabout the project to recruiters,  it was effortless  for me to come across as passionate  about data science\nand pricing algorithms  because  the underlying  passion for hip hop music was shining  through.\nAnother  benefit of working  on a project related to your passion: it’s less of a chore to get the damn\nproject over the finish line when work becomes  play. And getting the project done is paramount  to\nyour success,  as we later detail in tip #5.\nTip #4: Work with Interesting  Datasets\nDon’t work with datasets  that people have worked  with in their school work, such as the classic Ins Plant\ndataset  or the Kaggle  passenger  survival  classification  project  using the Titanic  dataset  — they’re  overdone.\nWorse,  working on these datasets  likely means  you are not working  on something  you are passionate  about,\nwhich goes against  the advice in the previous  tip. | stand corrected,  though,  if you’re a weirdo  whose true\npassion  is classifying  flowers  into their respective  species  based on petal and sepal lengths.\nAnother  reason to not work with these standard  datasets  is because  the recruiter  and hiring manager\nwill have seen this project done multiple  times already.  This situation  occurs frequently  if you are\na college student or in a bootcamp,  and are trying to pitch a required  class project  as your portfolio\nproject.  You can imagine  how lame that comes  across to recruiters  during  university  recruiting  events\nto see the same project  over and over again from everyone  who took the same class.\nKevin  heard a recruiter  complain  about exactly  this after she saw one too many Convolutional  Neural  Nets\ntrained  with Tensortlow  for handwniting  recognition  based on the MNIST  Digit Recognition  Dataset.  So\nstay away trom this classic  dataset,  along with avoiding  stock ticker  data (unless  you are gunning  for Wall\nStreet  jobs) and the Twitter  firehose  data (unless  you truly have a fresh take on analyzing  tweets).\nTo drive home how you can seek out interesting  datasets  that tell a good story and relate to your\npassion,  let’s work with a concrete  example.  Suppose  you love space and dream of  working  as a data\nscientist  al NASA.  How would you find exciting  datasets  to help you break into your dream  job?\nOur first step would be to go on Kaggle  and find NASA’s  Asteroid  Classification  challenge.  Or we can\nanalyze  the Kepler Space Observatory  Exoplanet  Search dataset.  If we wanted  to start more simply\nand only knew Excel, we could look at the CSV of all 357 astronauts  and their backgrounds  and make\na few cool graphics  about what their most common  undergrad  majors in college  were. So much data\nIs out there just one Google  Search  away you have no excuse  to be working  on something  boring!\nThe upside of using a website  like Kaggle  to get datasets  is that it’s well-formatted,  clean, and often\nthere are starter notebooks  exploring  the data. The downside  is that others may also be looking  at it,\nhurting  your project's  uniqueness.  There are also some lost learning  opportunities,  since collecting\nand cleaning  data is a big part of a data scientist’s  work. However,  if you find something  you really\nlove on Kaggle,  it’s not a big problem.  Go for it, and maybe later find a different,  complementary\ndataset  to add another  dimension  to your project.\nOne way to tackle interesting  datasets  that are unique is to scrape the data yourself.  Packages  like\nBeautifulSoup  and Scrapy  in Python  can help, or Rvest for R users. Plus it’s an excellent  way to practice\nyour programming  skills and also show how scrappy  you are. And since collecting  and cleaning  data is\nsuch a large part of a real data scientist’s  workflow,  scraping  your own dataset  and cleaning  it up shows\na hiring manager  you're familiar  with the whole life cycle of a data science  project.\n14 Ace the Data Science  Interview  | How to Make Kick-Ass  Portfolio  Projects"
  },
  {
    "page_number": 27,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nTip #5: Done > Perfect.  Prove  You Are Done.\nAs long as your work is approximately  correct,  the actual technical  details  don’t matter  as much for\ngetting  an interview.  Again,  as mentioned  above,  a recruiter  will not dig into your project  and notice\nthat you didn’t remove  some outliers  from the data. However,  a recruiter  can quickly  determine  how\ncomplete  a project  is! So make sure you go the extra mile in “wrapping  up a project.”  See if you can\n“productionize”  the project.\nTurn the data science  analysis  into a product.  For example,  if your project was training  a classifier  to\npredict  age from  a picture  of a face, go the extra step and stand up a web app that allows  anyone  to upload\na photo and predict  their own age. As part two to the project,  use a neural net to transform  the person’s\nface to a different  age, similar  to FaceApp.  Putting  in this extra work, and then cold-emailing  the project\nto hiring managers,  could be your ticket into companies  like Snapchat,  Instagram,  and TikTok.\nIf your project was less productizable  and more exploratory,  see if you can make an interactive\nvisualization  that helps you tell a story and share your results. For example,  let’s say you did an\nexploratory  data analysis  on the relationship  between  median  neighborhood  income  and quality of\nschool  district.  To wrap this project  up, try to make and host an interactive  map visualization  so that\nfolks can explore  and visualize  the data for themselves.  I like D3.js for interactive  charts and Leaflet.\njs for interactive  maps. rCharts  is also pretty cool for R users. By creating  a visualization,  and then\nsending  this completed  interactive  map to hiring managers  at Zillow  or Opendoor,  you'll be able to\nstand out from other candidates.\nLastly, your portfolio  project  isn’t done until it’s public, so make sure you publicly  share your code\non GitHub.  You can also use Google  Collab  to host and share your interactive  data analysis  notebook.\nEven if no one sees the code or runs the notebook  (which is likely!),  just having  a link to it sends a\nsignal that you are proud enough  of your work to publish  it openly. It also shows that you actually  did\nwhat you said you did and didn’t  just fabricate  something  to pad the resume.\nTip #6: Demonstrate  Business  Value\nThe best portfolio  projects are able to demonstrate  business value. Try to make it concrete,  not\ntheoretical.  This advice is crucial for PhDs breaking  into industry,  especially  if the academic  is trying\nto break into smaller  companies  or startups.  When you are applying  to businesses,  talk in business\nterms. Show how your technical  skills can drive business  value. Try to make sure your project has a\ncrisp business  recommendation  or fascinating  takeaway.\nIf you can’t point to exact metrics  like dollars  earned or time saved by creating  the project,  you can instead\nput down usage numbers  as a proxy for the amount  of value you created  for people. Plus, mentioning  view\ncounts or downloads  or active users helps demonstrate  to a business  that you drove a project  to completion.\nIt’s okay to skip out on demonstrating  business  value IF you work with interesting  enough data and can\ntell a good story. An example  project we find interesting,  creative  and fun, but technically  sumple and not\nobviously  a driver  of real business  value: A Highly Scientific  Analysis  of Chinese  Restaurant  Names. Send\nthat project to recruiters  at Yelp or DoorDash  and watch the interviews  come pouring  in.\nDouble Down on One Portfolio  Project  to Breeze  Through  Your\nBehavioral  Interview\nIf the tips above seem like a lot of work, that’s because  they are. But the good news is you don’t need\nto do this for every project. Spending  all your energy on having a single killer project 1s worth it, as\nlong as you end up with a lot to show for it, like beautiful  graphics,  a working  demo, and some usage\nstatistics  or metrics  on business  value created.\nAce the Data Science  Interview 15"
  },
  {
    "page_number": 28,
    "content": "CHAPTER  2: HOW TO MAKE KICK-ASS  PORTFOLIO  PROJECTS\nThe other reason it’s okay to focus your time and energy on creating a single kick-ass project 1s\nbecause this can help carry you through a behavioral  interview. Typical behavioral interview\nquestions  start with “Tell me about a time...” or “Tell me about a project where...”  By coming back\nto the same project, you don’t have to waste valuable  time setting context or background.  Instead\nyou can dive straight into answering  the behavioral  interview  question.  Plus, focusing  your ume and\nenergy into one project means you’ll be able to tackle more challenging  problems  and apply more\nadvanced  techniques.  This is good because  common  behavioral  interview  questions  include,  “What\nwas the hardest  data-related  problem  you tackled?”\nAn additional  benefit to going deep into one project is that you’re much more likely to create real\nbusiness  value or gather users and views for your project.  Trying  to market  and promote  multiple  side\nprojects  is a recipe for disaster,  because  showing  traction  for a single project is hard enough  for most\npeople. Sticking  to one project makes it much more likely you'll discover  some method  or angle to\ngenerate  value and publicity.\nAnother  reason why a well-crafted  portfolio  project allows you to breeze through  your behavioral\ninterview  is because one common question is, “Why this company?”  or “Why this industry?”\nHopefully,  you worked on something  you are passionate  about that is related to the company  or\nindustry  you are interviewing  with. Now your project  is able to “show, not tell” your interest.\nOne example  of using a project  to “show,  not tell” was when I (Nick) applied  to Facebook’s  Growth\nteam. They asked a common  question:  “Why Facebook’s  Growth  team?”\nI was able to tell the story of creating  consumer  products  and being a DJ, which led me to create\na music-tech  startup called RapStock.io.  From RapStock.io  I found my love of growing  consumer\ndemand  through  engineering.  This project sparked  my interest  in combining  Software  Engineering,\nData & Experimentation  Design,  and creating  consumer  products. This trajectory  mapped  exactly\nto what Facebook's  Growth  team did all day, so Id like to think [ gave the perfect  answer  backed  up\nby an authentic  story.\nYou might be thinking,  “Nick, you got lucky working  on a consumer  tech startup with a focus on\ngrowth  that lined up beautifully  with what Facebook’s  Growth  team does.” But dear reader,  here’s a\nlittle secret:\n© When | talked  to fintech  companies,  [ told them about the stock market  and commodity  pricing\naspect  of my game.\n© When I talked to data companies,  | went deeper into the algorithm  that assigned  prices to\nrappers  from Spotify  data.\n* When I talked  to marketing  and ad-tech  companies,  | talked  about how this tech project  piqued\nmy interest  in the world  of advertising  after a failed  Google  ads campaign.\n© When T interviewed  with startups,  I talked  about  how I, too, was a startup  founder  in the past, and\nwanted  to move  fast and ship things  quickly  rather  than suffer  through  big company  hureaucracy.\nWith just one project,  I was able to “show,  not tell” my direct interest  in a variety  of companies  and\ntypes of work, while showcasing  my technical  expertise  and personality  at the same time. A kick-ass\nportfolio  project,  along with the more detailed  behavioral  interview  tips we present  in Chapter  4, will\nallow you to do the same.\n16 Ace the Data Science  Interview | How to Make Kick-Ass  Portfolio  Projects"
  },
  {
    "page_number": 29,
    "content": "Cold Email Your Way to Your\nDream  Job in Data\nCHAPTER  3\nYou've crafted  the perfect  resume  and made a kick-ass  portfolio  project, which means it’s\ntime to apply to an open data science  position.  Eagerly,  you go to an online  job portal\nand submit  your resume,  and maybe  even a cover letter. And then it’s crickets.  Not even an\nautomated  rejection  letter.\nIf you've  applied  online and then been effectively  ghosted,  you're not alone. Kevin and |\nhave been in the same situation  plenty  of times. We are all too familiar  with the black hole\neffect of online  job applications,  where it almost  feels like you're  tossing  your resume  into\nthe void.\nSo how do you reliably  land interviews,  especially  if you have no connections  or referrals?\nTwo words: Cold. Emails.\nWhile in college, Snapchat  and Cloudflare  interviewed  me (Nick) when I had no connections\nor referrals  at those companies.  I got these interviews  by writing an email, out of the blue, to\nthe company’s  recruiters.  This process is known as cold emailing  (in contrast to getting a warm\nintroduction  to a recruiter).  Even my previous  job at data startup SafeGraph  is the result of a cold\nemail that I sent to the CEO. We firmly believe  this tactic can be a game changer  on the data science\njob hunt.\nWe don’t want to over-promise,  though. The best written cold email won't help if you're pursuing\njobs you aren’t a good fit for, like a new grad applying  to be a VP of Data Science.  Plus. you need to\nAce the Data Science  Interview 17"
  },
  {
    "page_number": 30,
    "content": "CHAPTER  3: COLD EMAIL YOUR WAY TO YOUR DREAM JOB IN DATA\nhave a strong resume (Chapter 1) and strong portfolio  projects (Chapter  2). But if you’ve got your\nducks in a row, yet struggle  to land the first interview,  this chapter  will be a game changer.\nWho are we even cold emailing?\nBefore we talk about the content  of the cold email, let’s cover who we’re reaching  out to in the first place.\nAt smaller companies  with less than 50 employees,  emailing  the CEO or CTO works very well. At\nmid-range  companies  (from between  50 and 250 people),  see if there is a technical  recruiter  to email;\notherwise  just a normal recruiter  should do. Another  option is emailing  the hiring manager  for the\nteam you want to join.\nFor larger companies,  finding the right person can be trickier. If you are looking for internships  or\nare a new grad, many of the larger companies  (1,000+  employees)  have a person titled “University\nRecruiter”  or “Campus  Recruiter.”  Reaching  out to these recruiters  is how I (Nick) had the most luck\nwhen cold emailing  in college.\nAt very large companies  like FANG,  there should  also be dedicated  recruiters  only working  with data\nscientists.  To find these recruiters,  go to the company’s  LinkedIn  page and hit “employees.”  Then,\nfilter the search results by title and search for “Data Recruiter.”  When doing this at Google,  I found\nsix relevant  data science  recruiters  to reach out to.\nAnother  option is to just filter the search by “recruiter.”  You'll get hundreds  of results  that you can sift\nthrough  manually.  Doing so at Google  uncovered  an “ML recruiter,”  “PhD (Data Science)  Recruiter,”\nand a “Lead Recruiter,  Databases  & Data Analytics  (GCP),”  all in just a few minutes.\nAnother  good source of people to email at a company  is alumni from your school who work there.\nEven if they work in a non-data  science  role, they may be able to refer you or know the right person\nto connect  you with. To find these people,  search your university  on LinkedIn  and click ‘“‘alumni.”\nFrom there, you can filter the alumni  profiles  based on what companies  they work at or what titles\nthey hold. I resort to this tactic if my first few cold emails to hiring managers  and recruiters  go\nunanswered.\nHow do we find their email address?\nNow that we know who we want to email, how do we find their email address?\nYou can use a tree tool like Clearbit  Connect  or Hunter.io  to look this up. If you can’t find the person\nyou want with an email-lookup  tool, you can always  guess. At smaller  companies,  or when dealing\nwith the founders,  a good guess is firstname@ucompanydomain.com.  For example,  Jeff Bezos’s  real\nemail 1s jeff@amazon.com.\nAt mid-sized  companies,  firstname.lastname@companydomain.com  may work well, along with\nusing their first initial and last name with no spaces.  When you’re in doubt, you can use Hunter.io  or\nClearbit  Connect  to see the format  that others at the company  use, and then you can make an educated\nguess. Then put your best guess email address  in the “to” section of the email, and cc a few more\nemail guesses  in the hopes that one of the emails  will be on target.\nTo shortcut  all this work, you can also use MassApply  (massapply.com),  which has hundreds  of\ntechnical  jobs with the recruiter  contact  information  already  available  inside the platform.  It allows\nyou to send customized  cold emails in a single click to recruiters,  as well as to track your job\napplications.  We're huge fans, but are a bit biased,  because  Nick’s brother  founded  MassApply!\n18 Ace the Data Science  Interview  | Cold Email Your Way to Your Dream  Job in Data"
  },
  {
    "page_number": 31,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nCan't Find an Email?  Every  address  you guessed  bounced?\nEvery address  you guessed  bounced?  Send them a LinkedIn  InMail  or Twitter  DM instead!\nWhile email should  still be your first choice  (busy people  like recruiters  and CEOs are in their email\ninbox all day — not on LinkedIn  or Twitter),  you have nothing  to lose when reaching  out on other\nplatforms!  The tips we soon cover on writing effective  cold emails apply to other types of cold\nmessages  as well.\n8 Tips for Writing  Effective  Cold Emails\nNow that we know who to email and how to get their email, what do we actually  say in the email?\nHere are 8 quick tips for writing  effective  emails.\nTip #1: Keep the Email Short\nRecruiters  are busy people, as we covered  in depth in Chapter 1 on resumes.  Just like with your\nresume,  they don’t have  a lot of time to read your email. You’ve  got 10 seconds  to impress  them with\nyour email so that they respond  to it rather  than ignore it. So keep your email short.\nThe data backs email brevity.  HubSpot  analyzed  40 million  emails and found the ideal length of a\ncold sales email is between  50 and 125 words to maximize  response  rates. I’ve personally  had the\nbest luck at around 100 words. It’s all about maintaining  a high signal-to-noise  ratio. You don’t need\nto include  phrases  like “I hope you are doing well today!”  or “I hope this email finds you well.” At\nbest, it’s extraneous  and at worst, insincere.\nTip #2: Mention  an Accomplishment  or Two\nOur cold email is just like a sales email. In one paragraph,  you are trying to sell yourself  to the\nrecruiter  as someone  worthy  of an interview.  This is sales — don’t be shy.\nHighlight  a relevant  accomplishment  or internship  experience  that makes you worthy  of a response.\nName-drop  that hackathon  you won. Hyperlink  to your favorite  project  or an app on the app store with\na few thousand  downloads  and mention  that usage number.  If you went to an impressive  engineering\nschool,  lean into that.\nHowever,  you don’t need to link to too many things or copy-paste  the entire resume. That would\nend up breaking  Principle  #1: Keep It Short. Instead, attach your resume to the initial email so the\nrecruiter  can get more background  if needed.\nTip #3: Add Urgency  and Establish  a Timeline\nMy favorite  tip: if you already  have a return internship  offer with a different  company  or a competing\njob offer extended  to you, mention  that. It puts pressure  on a recruiter  to respond  promptly  and might\neven fast-track  you to an onsite interview.  This tactic works especially  well if it’s an offer with a\nwell-known  company.\nEven if the deadline is very far from now, so there is no true urgency, name-dropping  the other\ncompany  is helpful as social proof. If other companies  desire you, then a recruiter  is more likely to\nfeel you are valuable  and have #fomo. This leads to them responding  to you.\nYou don’t even need the offer in hand to make this tactic work! Just having an onsite interview\nscheduled  with a top company  helps other companies  realize you've got something  worthwhile  and\nthat there is a specific  timeline  to adhere to.\nAce the Data Science  Interview 19"
  },
  {
    "page_number": 32,
    "content": "CHAPTER  3: COLD EMAIL YOUR WAY TO YOUR DREAM JOB IN DATA\nFormer  Microsoft  Intern With Upcoming  Deadline  Interested @ &\ninUber  ATG _ inbox x\nNick Singh <hello@nicksingh.com> 9:18 AM(Ominutesago)  yo & :\nto recruiter  #\nHello Recruiter  Name,\n| have an upcoming  onsite-interview  with Microsoft's  Azure ML team next month, but wanted to also interview  with\nUber because  seif-driving  cars is where | believe Computer  Vision will help improve  the world the most in the next\ndecade.\nHeiping  the world through CV became  my passion  after seeing the impact of the last project | made, which used CV\nto find_and  categonze  skin diseases.\nFrom reading  the ATG engineering  blogs. | know Uber is the best place for a passionate  computer  vision engineer\nto make an impact, and am eager to start the interview  process  before | go too far down the process  with Microsoft.\nI've attached  my resume.\nThanks,\nNick Singh\nOne warning:  be careful  not to make it seem like the company  you are talking  to is the backup  option.\nTo do this, make sure you convey  enthusiasm  for the company  and mission.  Below is an example  of\nthat.\nTip #4: Relate Personally  to the Recruiter  or Company\nYes, this is a cold email, but you don’t have to be so cold! The person at the other end of the email\nis still a human,  and you can make  a real-life  connection  even if you haven’t  met before. It’s well\nworth it to do your research.  For example,  see if you have a mutual  connection  with the recruiter.  Use\nLinkedIn  to see if you have any commonalities  like education  or cities you’ve  both lived in. Even\ntwo minutes  of sleuthing  on the internet  looking  for a commonality  can pay huge dividends  when it\ncomes  to response  rates.\nTip #5: Have a Specific  Ask\nBe up front with what you want. A vague email hoping to “set up a time to chat” or “learn more\nabout the interview  process”  is too meek and indirect.  The recruiter  knows  that between  your friends,\nGoogle  Search,  Quora,  and Glassdoor,  you can find any information  you need about a company  and\nthe interview  process.  They undoubtedly  know you are angling  for a job or internship  but are too shy\nto ask directly.\nSo why not be be bold -- after all, fortune  favors the bold —- and always  include  a specific  ask:\n“I'd like to interview  for a Data Science  Internship  for Summer  2021.”\n“Td like to start the interview  process  for the Senior  Data Scientist  position  at your company.”\nTip #6: Have a Strong  Email Subject  Line\nAn email is only read if is opened.  Without  a strong subject  line to lure the recipient  into actually\nopening  the email, the email is wasted.  Thus, it’s worth spending  time crafting  a strong  email subject\nline. To make the subject line click-worthy,  it’s key to include  your most noteworthy  and relevant\n20 Ace the Data Science  Interview  | Cold Email Your Way to Your Dream  Job in Data"
  },
  {
    "page_number": 33,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\ndetails. It’s okay if the subject  line is keyword  driven and a “big flex,” as the teens say these days.\nBorrow  from BuzzFeed  clickbait  titles — they actually  work! I wouldn’t  go so far as to say “21 Weird\nFacts That'll  Leave  You DYING  to Hire This Data Scientist,”  but you get the gist.\nWhat I (Nick)  used, for example:  “Former  Google  & Microsoft  Intern Interested  in FT @ X”\nThis subject  line works  because  I lead immediately  with my background,  which  is click-worthy  since\nGoogle  and Microsoft  are well-known  companies,  and I have my specific  ask (for full-time  software\njobs) included  in the subject  line. Some other subject  line examples  that are short and to the point if\nyou can’t rely on internship  experience  at name-brand  companies:\n“Computer  Vision Ph.D. Interested  In Waymo”’\n“Princeton  Math Major  Interested  in Quant  @ Goldman  Sachs”’\n“Kaggle  Champion  Interested  in Airbnb  DS”\n“UMich  Junior  & Past GE Intern  Seeking  Ford  Data Science  FT”\n[f I (Nick) found a recruiter  from my alma mater (UVA), I'd be sure to include  that in the subject\nline to show that it’s personalized.  For reaching  out to UVA alumni, I’d thrown in a “Wahoowa”\n(similarly,  a “Go Bears”  or “Roll Tide” if you went to Berkeley  or Alabama,  respectively).  Including\nthe name of the recruiter  should  also increase  the click-through  rate.\nExample:  “Dan | FinTech  Hackathon  Winner And Wahoo  Interested  in Robinhood”\nAnother  hack: including  “Re:” in the subject line to make it look like they’ve  already  engaged  in\nconversation  with you.\nTip #7: Follow  Up 3 Times\nA perfectly  written  email sent only once may not work. You should  follow  up at least three times. You\ncan reply directly  to the thread,  so that the context  from the first email still remains  there.\nDon’t worry about feeling  too pushy — it’s standard  in sales to reach out 3+ times. I know first hand\nnot to give up too early: some of the cold emails that turned into interviews  only got responses  after\nthe third email. Send the first follow-up  after 3-4 days, and send the second  follow-up  4—5 days later.\nDon’t think putting  in a 2-week  delay will make you come across as more polite.\nA free Gmail plugin like Boomerang,  which will flag when an email hasn’t been responded  to in\nsome time, can help to keep yourself  accountable.  There is also automatic  email scheduling  within\nMassApply  so that you can follow up three times.\nIf after a few emails you don’t get a response,  reach out to another  recruiter  at the same company.\nIt’s okay to reach out to multiple  people at a company  that you want to work for. Trust me, it’s not a\nweird thing to do. In enterprise  sales lingo, reaching  out to multiple  people  at your target company  is\ncalled being “multi-threaded  into an account,”  and it’s a time-tested  tactic.\nTip #8: Send the Email at the Right Time\nWe’ ve all been guilty of getting an email, reading  it, and waiting  till later to respond  to it. And then\n“later” never comes. That’s why Principle  #7 — following  up three times -— works so damn well.\nBut sending  an email out at the right time can save you from having to bump up emails. To maximize\nyour reply rate, send the email when you think the reader is most likely to be free and in the mood to\nrespond.  That means no weekend  emails. No emails on holidays  or days people typically  might take\na long weekend.  Figure out the time zone for the recruiter,  and be sure to not send it after business\nhours.\nAce the Data Science  Interview 21"
  },
  {
    "page_number": 34,
    "content": "CHAPTER  3: COLD EMAIL YOUR WAY TO YOUR DREAM  JOB IN DATA\nI had the best luck emailing  Silicon  Valley recruiters  at ~11:00  A.M. or 2 P.M. P.S.T. The psychology\nbehind this is we’ve all felt ourselves  counting  down the minutes  to lunch, aimlessly  refreshing  your\nemail and Slack to pass the time. That’s a great time to catch someone.  Same with the after lunch lull.\nThe best days I found to send emails were Tuesday through Thursday.  I avoided Mondays  since\nthat’s the day many people have 1:1s or team meetings  or have work they are catching  up on from\nthe weekend.  On Fridays,  many people might be on PTO, or even if they are in office, have some\nother kind of event like happy hour in the afternoon  (or they’ve  already  mentally  checked  out before\nthe weekend).\n3 Successful  Cold Email Examples\nHere are some real cold emails I’ve sent in the job hunt. The text is exactly  the same as what I sent,\nbut I just re-created  it to protect  the recipient’s  name and emai! address.\nThese emails aren’t perfect  by any means,  but generally  follow the eight tips I’ve laid out above. Just\nremember:  even sending  a cold email that’s bad puts you in the top decile of  job seekers.  Most people\nwill never make an effort to personally  write an email to someone  they don’t know, and then have the\ntenacity  to follow up a few times. It’s precisely  why cold email works so well!\nThe 4 Sequence  Email Drip to Periscope  Data\nIntro Email:\n<>nyIN Ex-Google  & Microsoft  Intern Interested  in\nWorking  FT at Periscope  Data\nNick Singh <hello@nicksingh.com> Tue, Sep 8, 2020,2:43PM yer €&\nto recruiter  w\nHi X,\nFound your email on Hacker  news. I'm a former  Software  Engineering  Intern @ Google's  Nest\nLabs and Microsoft  who will be graduating  from college  May’17.\nI'm interested  in working  full time at Periscope  Data because  of my interest  in data engineering\n(spent  the summer  on the Data Infrastructure  team @ Nest) and my interest  in turning  data into\ninsights  (built dashboards  to do just that the past two summers).\nHow can  | start the interview  process?\nBest,\n22 Ace the Data Science  Interview  | Cold Email Your Way to Your Dream  Job in Data"
  },
  {
    "page_number": 35,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nMy Two Follow-Ups\nNick Singh 9:32 AM(iminuteago)  yy €& :\nto recruiter  ~\nJust wanted  to follow up with you about  full-time  roles at Periscope  data. | believe  my interest  in dataenone  along with past experience  building  dashboards  and visualization  tools, makes  me a\ngood fit.\nNick Singh 9:33 AM(1 minuteago) ye €& :\nto recruiter  v\nHi X, Wanted  to circle back on this. What do next steps look like?\nThe Hail Mary:\nI send this email when | have on-site interviews  near the target company  planned,  or when | have\noffer deadlines  approaching.  This email is often sent weeks  after the initial outreach.  It works  because\nit adds an element  of urgency  to the recruiter,  and it gives social proof that other companies  have\nvetted me enough  to bring me on-site.\nNick Singh <hello@nicksingh.com> Tue, Sep 8, 2020,2:47PM ye €&\nto recruiter  ~\nHi X,\nJust wanted  to follow up with you regarding  opportunities  with Periscope  Data. | will be in the\nbay area doing interviews  with Facebook  and Uber next week. Would love a chance  to doa\nphone interview  with Periscope  Data this week to assess  technical  fit. If we are a good match,\nI'd be happy to swing by the office the following  week for technical  interviews  while | am\nalready  in town.\nThanks,\nNick Singh\nAce the Data Science  Interview 23"
  },
  {
    "page_number": 36,
    "content": "CHAPTER  3: COLD EMAIL YOUR WAY TO YOUR DREAM JOB IN DATA\nMore Examples  of Real Cold Emails  I’ve Sent\nCold Email to Airbnb  in 2016\nFormer  Google  Intern from UVA Interested  in =\nAironb Inbox x\nNick Singh <hello@nicksingh.com> 0:42 AM(Ominutesago)  yy —&\nto recruiter  ~\nHello X,\nWe met briefly  at the UVA in SF mixer this past summer.\n| just wanted  to reach out to you about new grad Software  Engineering  positions  @ Airbnb. My\nfriend, Y, interned  at Airbnb  on the Infrastructure  team and really loved their experience.\nThis past summer,  | was on the Data Infrastructure  team at Google's  Nest Labs. From talking  to\nY, | think | can be a good fit for similar  teams  at Airbnb.  Let me know what the next steps are.\nThanks,\nNick Singh\nCold Email to Reddit in 2015\nFormer  Microsoft  Intern @ Avid Redditor  Interested @®\nin SWE Internship Inbox x\nNick Singh <hello@nicksingh.com> 9:47 AM(Ominutes  ago) ye €&\nto recruiter  v\nHello X,\n| saw your post on Hacker  News and wanted  to reach out regarding  why |'m a good fit to be a\nSoftware  Engineering  intern at Reddit  for Summer  2016.\n| interned  at Microsoft  this past summer  on the Payments  Team where | helped  the team turn data\ninto insight  to diagnose  payment  issues  faster.\nIn my free time (when I'm not on Reddit)  | built RapStock.io  which | grew to 2000 users. 1400 out of\nthe 2000 users came from Reddit  when we went viral so | have a soft spot for the community  and\nproduct.\nLet me know what next steps | should  take.\n24 Ace the Data Science  Interview  | Cold Email Your Way to Your Dream  Job in Data"
  },
  {
    "page_number": 37,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nCold Email to SafeGraph  in 2018 (how | got my last job!)\nBelow is the screenshot  of the exact email I sent in summer  of 2018 to SafeGraph  CEO, Auren\nHoffman.  | decided  to email the CEO directly,  in addition  to my AngelList  application,  due to my\npoor track record  of hearing  back from online  job portals.  Within  24 hours of sending  this email, I had\nan interview  booked  with Auren and ended up working  at SafeGraph  for close to two years. That’s\nthe power  of cold email!\nBy utilizing these cold email tips and taking inspiration  from these cold-email  examples,  in\nconjunction  with a strong  resume  and kick-ass  portfolio  projects,  you’re  well on your way to landing\nmore data science  interviews.  Now comes  the next challenge  of the data science  job hunt: acing the\nbehavioral  interview.\nNipun Singh <ns2se@virginla.edu> © Sun, Jul15,2018,2:29AM  Yr @&\nto auren, auren.hoffman  ~\nAuren,\ni'm super interested  in the CoS role at Safe Graph. | applied  on Angel list but figured  I'd also shoot you an email.\nI'm a good fit for the role because\ne I'mcurrently  a Software  Engineer  on Facebook's  Growth  team. |'m data-driven  to the max - all day either coding or\ncutting  data to understand  the impact  of what ! coded and the AJB tests | ran.\ne I'ma hustler. | ran a startup  in college,  which | grew to 2,000 MAU. | ran a DJ business  in highschool,  which taught\nme how to be a people  person and also sell. | helped run the Venture  Capital  club (Virginia  Venture  Fund) and\nEntrepreneurship  Group (HackCville)  at my college.\n° I studied  Systems  Engineering  in college,  which is super similar to Industrial  Engineering  / OR (your major!). | also\nstudied  Computer  Science.  I've taking classes  on ML, Computer  Vision, Stochastic  Processes,  Databases.  I'd be\nable to understand  the technical  details of SafeGraph  and the space we operate  in very quickly.\n've attached  my resume. I'd love to call or meet up in person to talk more about why I'm excited about SafeGraph.\nThanks,\nNipun Singh\nwww.nipunsingh.com\nAce the Data Science  Interview 25"
  },
  {
    "page_number": 38,
    "content": ""
  },
  {
    "page_number": 39,
    "content": "Ace the Behavioral  Interview\nCHAPTER  4\nNow that you've  finally  built a kick-ass  resume, compiled  an impressive  project  portfolio\nand intrigued  the HR department  at your dream company  enough to call you in for an\ninterview  based  on your Strategically  written  emails,  vou ‘re ready to ace the technical  data\nscience  interview  questions  and land the job. But there ’ one more  piece to the puzzle  whose\nimportance  is usually  underestimated:  the behavioral  interview.  While its true that 90%\nof the reason  candidates  pass interviews  for the most coveted  big tech and finance jobs is\nbecause  of their technical  skills — their ability  to code on the spot, write SQL queries,  and\nanswer  conceptual  statistics  questions  — neglecting  the other 10%, which stems  from the\nbehavioral  interview,  can be a huge mistake.\nBehavioral  Interviews  Aren’t  Fluffy  B.S.\nYou may not agree that the behavioral  questions  are important.  You might think this behavioral\ninterview  stuff is fluffy bullshit,  and that you can simply wing it. Sure, in some companies,  as long\nas you aren’t an asshole in the interview  and have the technical  skills, you'll be hired. But some\ncompanies  take this very seriously.  Amazon,  for example,  interviews  every single candidate  on the\ncompany  leadership  principles  like “customer  obsession”  and “invent  and simplify”  in their bar raiser\ninterviews.  Uber also includes  a bar-raiser  round which focuses  on behavioral  questions  about culture\nfit, previous  work experience,  and past projects. If you want to work there, you've got to take the\nbehavioral  interviews  seriously.\nEven small companies  have their unique twist on this. Consider  this: my (Nick’s)  former employer,\nSafeGraph,  displayed  the company  values on a poster hung in every room in the building.  Even the\nAce the Data Science  Interview 27"
  },
  {
    "page_number": 40,
    "content": "CHAPTER  4: ACE THE BEHAVIORAL  INTERVIEW\nbathroom.  While you’re pissing,  the company  values are in your face. No joke. /ts that paramount.  So\nimagine if you came to an interview  with me and didn’t share any stories that exhibited  SafeGraph’s\ncompany  values. You’d have a pretty piss-poor  chance of passing  the interview!\nWhen Do Behavioral  Interviews  Happen?\nYou might be wondering,  “I had  four interviews  for a position,  and not one of them was called a\nbehavioral  interview.”\nThe behavioral  interview  is an integral part of any interview  and consists of questions  that help\nthe interviewer  assess candidates  based on actual experiences  in prior jobs or situations.  Even the\n“friendly  chat” time at the start of the interview  can essentially  be a behavioral  interview.\nSo while you might not have an explicit  calendar  invite for “Behavioral  Interview,”  don’t be fooled:\nbehavioral  interviews  occur all the damn time. That casual, icebreaker  of a question,  “So, tell me\nabout yourself...”  — that’s an interview  question!  For every job, and at practically  every round,\nthere will be a behavioral  component,  whether  it’s explicit or not. Behavioral  interview  questions\ncan happen:\n¢ With  a recruiter  before getting  to technical  rounds. In which case you might not even get to the\ntechnical  interview  rounds...\n¢ During  your technical  interviews,  where the first 5-10 minutes  are usually  carved  out for a casual\nchat about your past projects  and your interest  1n the company.\n¢ During  lunch, to understand  how you behave  outside  of the interview  setting.\n¢ At the end of the on-site interview;  they know you can do the work, but are you someone  they\nwant to personally  work with? You'll meet with your future boss, and maybe even their boss,\nwhere they'll both try to sell you on the company,  but also see if you'd be a good culture  fit.\nThe reality is, vou are constantly  being assessed!  That’s why, on the basis of frequency  alone,\npreparing  for and practicing  answers  to these questions  is well worth the effort.\nAce the Behavioral  Interview  to Beat the Odds\nAcing the behavioral  interview  can be the X-factor  —- the thing that separates  you out from the\nhorde of other applicants.  You don’t want to be lying in bed at night, wide awake,  thinking,  “Damn\nit. | forgot to tell them about that time | caught that data analysis  mistake  and saved the company\n$50,000!\"  A little prep work for your interview  can mean the difference  between  a strikeout  and a\nhome run.\nFocusing  on behavioral  interviews  is especially  important  if you’re new to the data science  game.\nWhen a company  makes an investment  in junior talent, they are looking  at 3-6 months  of training\nbetore that junior data person becomes  truly productive.  You probably  won't give the best answers\non technical  questions,  and there will always be more senior candidates  in the pipeline,  but the\ncoachability.  enthusiasm,  and eagerness  to learn that you show in the behavioral  interview  could be\nwhat convinces  a company  to take a chance  and invest in you.\n3 Things  Behavioral  Interviews  Test For\nYou know that behavioral  interviews  are important,  that they happen  all the time, and that the stakes\nare high especially  tor junior talent. Now you might be wondering,  “What are interviewers  even\n28 Ace the Data Science  Interview | Ace the Behavioral  Interview"
  },
  {
    "page_number": 41,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nlooking  for?” Behavioral  questions  have to do with...well...behavior.  There are three basic kinds of\nthings an employer  tests for:\n¢ Soft skills; How well do you communicate?  So much of effective  data science  is dealing  with\nStakeholders  —- would you be able to articulate  your proposals  to them, or sell your ideas\nconvincingly  enough  to get buy-in  from them? How well do you work with others?  Data science\nis a team sport, after all! How do you deal with setbacks  and failures?  Do you get defensive,  or\nexhibit  a growth  mindset?\n¢ — Position  fit: How interested  are you in the job and team you’re  gunning  for? What motivates  you\nabout the position  — only the paycheck  or passion  as well?\n¢ Culture  fit: How well do you fit the team and company’s  culture?  Can you get behind the\ncompany’s  mission  and values?  Basically  the corporate  version  of a “vibe check”!\nEssentially,  while technical  interviews  are about whether  you can do the job, behavioral  interviews\nare about whether  you want to do the job, and if you are someone  others will want to work with.\nFortunately,  you can have the charisma  of Sheldon  Cooper  from Big Bang Theory and still pass\nbehavioral  interviews  — if you prepare  for the most common  behavioral  questions  asked in data\nscience  interviews.\nTell Me About  Yourself:  The #1 Behavioral  Interview\nQuestion\n“Tell me about yourself”  may seem like a simple icebreaker  to ease tension  and get the interview\nrolling, but it’s actually  the #1 most asked behavioral  interview  question!  If you are not properly\nprepared  with your answer,  you can stumble  through  it blithely,  telling  your life history  and all sorts\nof irrelevant  details that are not what they want to know about you. First impressions  matter, and a\nwell-thought-out  answer  can impress  the hell out of your interviewer  and put you in the running  from\nthe get-go.\nSo, how do you prepare  an awesome  answer  to this seemingly  innocuous  question?\n¢ Limit your answer  to a minute or two; don’t ramble!  As such, start your story al a strategically\nrelevant  point (which  is often college  for most early-career  folks).\n¢ Relate your story to the position  and company  at hand. See if you can weave your pitch with key\nterms from the job description  and company  values. Speak their language!\n¢ Mention  a big accomplishment  or two; even though they've seen your resume, don’t let them\nforget about your biggest  selling  point!\n¢ Rehearse.  You know this question  will be asked at the start of every interview.\nYour answer  should include  these three key points:\n1) Who you are\n2) How you came to be where you are today (sprinkle  in your achievements  here)\n3) What you’re into/looking  for now (hint hint. it’s basically  this role + this company)\nTo make this more concrete,  here’s the “about me” pitch we authors used on the job hunt.\nAce the Data Science  Interview 29"
  },
  {
    "page_number": 42,
    "content": "CHAPTER  4: ACE THE BEHAVIORAL  INTERVIEW\nKevin’s  Wall Street \"About  Me” Pitch\nHi, I'm Kevin, currently a data scientist at Facebook. I graduated  from Penn in 2017,\nstudving  computer  science, statistics,  and  finance.  At Facebook  I focused  on analytics  within\nthe groups team, making sure Facebook  Groups is free of spam and hate speech. Before\nFacebook,  I briefly interned  at a hedge  fund, working  on looking  at alternative  data sets, like\nclickstream  data and satellite  imagery, to analyze  stocks. Having  worked  in both big tech and\nWall Street, I've come to realize I'm more passionate  about applying  data science  in financial\nmarkets  because  of the fast-paced  nature and high stakes environment.  | was drawn to your\nfund in particular  due to the small team, high autonomy,  and chance to be part of a more\ngreenfield  data science  effort.\nNick’s Google  Nest Data Infrastructure  Internship  “About  Me” Pitch\nHi! I'm Nick, and I’m currently  a 3rd year student  at the University  of Virginia!  I love the\nintersection  between  sofiware  and data, which is why I’m studying  Systems  Engineering  and\nComputer  Science  at UVA. Its also why two summers  ago, | interned  as a Data Scientist  at\na defense contractor,  and last summer  I interned  on the Payments  team at Microsoft  doing\nback-end  work.\nI’m super excited  to potentially  work on the Data Infrastructure  team at Nest Labs since\nit’s the perfect blend of my past data and SWE experience.  Plus, Nests intelligent  home\nautomation  products  rely on great data and machine  learning, and I want to work ata\ncompany  where data is at the forefront.  Lastly, I love how you all are a smaller,  faster-paced\ndivision  within Google. Having  made a startup in the past, which I growth  hacked  to 2,000\nusers in justa few months, | love the “move  fast” attitude  of  smaller  companies.  I think that\nNest being an autonomous  company  within Google  strikes  the perfect  blend  between  startup\nand big tech company,  and it’s why I’m so excited  by this team and company.\nWhy did you choose  Data Science?\nHere's another  question  you might be asked, which 1s closely related to your personal  pitch: “Why\ndid you choose  Data Science?”  Likely  your answer  to “tell me about yourself”  contains  some element\nof how you got into the field, but you may be asked to harp on this point more, especially  if you’re\nan industry  switcher,  or come from an untraditional  background.\nIf your path isn't the most straightforward,  don’t be nervous  -— capitalize  on this opportunity  to show\nyou are a go-getter  who decided  to make a career change,  a fast learner who has accomplished  so\nmuch in a short time, and how your passion  for the field is genuine!  This 1s also a great opportunity  to\ntalk about how your skills from prior jobs and industry  experience  naturally  led you into data science.\nRemember,  data science  1s much more than modeling - even if you weren’t  throwing  XGBoost  at\nrandom  datasets  in your last job, there must have been some relevant  data science-adjacent  skills you\nacquired.  And deep down, intemalize  that your newness  to the field isn’t a weakness,  but a strength\n—~ you've  probably  got extra subject  matter  expertise  and a fresh perspective!\nTell Me About  a Time: The #1 Most Common  Pattern  for Questions\nOnce you have your opening  pitch prepared,  along with the story of how you got into the field, it’s\ntime to focus on the other questions  most likely to be asked.\nThe #1 question  after “tell me about yourself”  is: “Tell me about a situation  where” something\nhappened.  Note that this question  can be phrased  in various  ways: “Give me an example  of when you\n30 Ace the Data Science  Interview | Ace the Behavioral  Interview"
  },
  {
    "page_number": 43,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nX” and “Describe  a situation  when you Y.” This is your time to share war stories from past jobs. If\nyou lack work experience,  this is the time for your independent  portfolio  projects  to shine.\nMost Common  “Tell Me About  a Time”  Questions\nSome of the most commonly  asked “‘tell me about a time” questions  are:\nTell me about  a time...\n* you dealt with a setback  — how did you handle  it?\n* you had to deal with a particularly  difficult  co-worker  —- how did you manage  it?\n* you made  a decision  that wasn’t  popular  — how did you go about implementing  it?\n* you accomplished  something  in your career that made you very proud  — why was that moment\nmeaningful  to you?\n* you missed  a big deadline  — how did you handle  it?\n“Tell Me About  a Time”  for Data Scientists\nWhile  the above popular  questions  are fair game, you might also be asked a twist on these questions\nso that they’re  better  geared  towards  data scientists,  analysts,  and machine  learning  engineers.  Below\nare some more data-driven  behavioral  interview  questions.\nTell me about  a time...\n¢ when data helped  drive a business  decision.\n* where  the results of vour analysis  were much different  than what  you would  have expected.  Why\nwas that? What did you do?\n¢ when  you had to make a decision  BUT the data you needed  wasn tt available.\n¢ you had an interesting  hypothesis  — how did  you validate  it?\n¢ when  you disagreed  with a PM or engineer.\nNow that we’ve got the laundry  list of situational  questions  out of the way, how do you answer  these\nquestions  well, on the spot?\nA superSTAR  Answer\nThe trick to answering  the behavioral  questions  we listed earlier  on the spot is...well...to  NOT answer\nthem on the spot! A lot of preparation  needs to go into this so you can give effortless  off-the-cuff\nanswers  Come interview  time. Your first step in preparing  flawless  answers  is to prepare  stories that\naddress  the questions  we mentioned  earlier. But don’t prepare factual  answers.\nPrepare  stories.\n“But I’m no storyteller,  I’m a data scientist!  How am I supposed  to “weave  a fascinating\ntale” about  something  as mundane  as my work history?”\nLuckily,  there is a simple formula  you can use as a framework  to structure  your story. It’s easy to\nremember,  too. Just remember  that a great story will make you a STAR, so you have to use the STAR\nformula:\ne Situation  —- Describe  a specific challenge  (problem  or opportunity)  you or your team, your\ncompany,  or your customers  encountered.\ne Task — Describe  the goal you needed  to accomplish  (the project  or task).\nAce the Data Science  Interview 31"
  },
  {
    "page_number": 44,
    "content": "CHAPTER  4: ACE THE BEHAVIORAL  INTERVIEW\n¢ Action — Describe  your role in the project or task using first person (not what your team did,\nbut what you did).\n¢ Result  — Describe  what happened  as a result of your actions.  What did you learn or accomplish?\nKeep in mind that not all outcomes  need be positive;  if things didn’t go your way, explain what\nlesson you learned (for example,  “I learned about the importance  of transparency  and clear\ncommunication”).  Showing  that you can handle failure and learn from it is a great trait!\nWrite your stories out using the STAR formula.  Where  possible,  weave into your narrative  key phrases\nfrom the job description  and the company  culture or values page, so that you hit the position  fit and\nculture  fit elements  of the interview.\nAmazon  Data Scientist  Interview  Example\nFor a concrete  example  of STAR, assume | (Nick) am interviewing  to be a data scientist on the\nAWS Product  Analytics  team. According  to the job opening,  the role entails influencing  the long-\nterm roadmap of the AWS EC2 Product Team. The job description  also mentions looking for\nsomeone  with a startup  mentality,  since “AWS is a high-growth,  fast-moving  division.”  Finally,  their\npreferred  qualifications  include “demonstrated  ability to balance  technical  and business  needs” and\n“independently  drive issues to resolution  while communicating  insights  to nontechnical  audiences.”\nNow that we’ve set up the role I’m interviewing  for, imagine  the Amazon  bar-raiser  hits me with the\nquestion:  “Tell me about a time you were not satisfied  with the status quo.”\nMy answer:\ne¢ Situation:  [| challenged  the status quo back when I worked on Facebook’s  Growth Team,\nspecifically  on the New User Experience  division.  Our main goal was to improve  new user\nretention  rates. In 2018, there was a company-wide  push for Facebook  Stories based on the\nsuccess  of Instagram  stories  and the fear of Snapchat  gaining  even more market  share. The status\nquo was lo prioriize  features  that would promote  the adoption  of Facebook  Stories,  but I had a\nstrong hunch this wasn’t  good for new users.\n¢ Task: My goal was to understand  how new users used Facebook  Stories,  and whether  the feature\nhelped  or hurt new user retention  rates.\n¢ Action: For 3 weeks, I sliced and diced data to better understand  whether  Facebook  Stories\nhelped  or hurt new users. In the process,  | found multiple  bugs and user experience  gaps related\nto Stories  for new users, which led to decreased  retention  rates for new users. I fixed the smaller\nbugs, and presented  the bigger  data-driven  insights  into the user experience  problems  with the\nwider Facebook  Stories  team as well as the New Person  Experience  team.\n¢ Result: Fixing the bugs resulted  in new user retention  rates increasing  by X%, and Y% more\nusage in Facebook  Stories. More importantly,  by questioning  the status quo that Facebook\nStories  was good for everyone,  I made the Facebook  Stories  team more conscious  of gaps in the\nproduct  as it related  to new users. This affected  the Facebook  Stories  product  roadmap,  and led\nthem to prioritize  user onboarding  features  for their next quarter.\nThis is an etfective  answer  because  it emphasizes  how my data-driven  work impacted  the product\nroadmap  --- essentially  what this Amazon  product  analytics  job is all about. It also demonstrates  my\npassion  for new users, which  Jives with Amazon’s  company  value of customer  obsession.\nRemember,  though, a winning  answer to a behavioral  interview  question  is about more than just\nwords. Project  the confidence  ofa college  sophomore  who thinks majoring  in business  means  they'll\n32 Ace the Data Science  Interview | Ace the Behavioral  Interview"
  },
  {
    "page_number": 45,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nbe a CEO one day. Embody  BDE — big data energy.  To dial in your delivery,  practice  telling  your\nStories  out loud. Do this in front of a mirror  — it’lI force you to pay attention  to your nonverbal  skills,\nwhich  are also very important  in an interview.  Use a timer, and without  rushing,  ensure  your answers\nare under  two minutes  long.\nHow to Ace Project  Walk-Through  Questions\nRather  than asking  you about  a situation,  project  walk-through  questions  let you talk about a project\nin detail. These questions  often have follow-ups  where they ask for more details —— and they may\neven be a jumping-off  point to ask more general  technical  questions.\nIn addition  to checking  your communication  skills, like the more traditional  behavioral  questions,\nthese questions  are also testing  to see if you’ve  actually  done what you say you did. The bullet points\non your resume  don’t always  tell the whole story —- maybe  the work is less (or more!)  impressive\nthan you made it sound. In fact, with the length limitations  on a resume’s  job description,  there’s\nprobably  a LOT more to the story than the resume  reveals.\nIn project  walk-throughs,  you might specifically  be asked questions  such as:\n* How did  you collect  and clean the data?  Did  you run into any issues when interpreting  the data?\n* How did  you decide  what models  and techniques  to use? What did you eventually  try?\n* How did you evaluate  the success  of  your  projects?  Was there a baseline  to compare  against?\nWhat metrics  did  you use to quantify  the project’s  impact?\n° Did  you deploy  the final solution?  What challenges  did you face launching  your work?\n¢ What tough technical  problems  did you  face — and how did you overcome  them?\n* How did you work with stakeholders  and teammates  to ensure the project  was successful?  If\nthere were any conflicts,  how did  you resolve  them?\n¢ Ifyou  did the project  again, what would  you do differently?\nIf the above questions  look familiar, that’s because you can consider  the project walk-through\nquestions  as the inverse  of “tell me about a time” questions.  Said another  way, given a project,  the\ninterviewer  asks  a lot of the same “tell me about a time” questions,  except the “time” is all about a\ncertain  project.  The same concepts  for applying  STAR still apply!\n“Do you have any questions  for us?”\nPretty much every interview  includes  a segment  where you get to ask questions.  “I don’t really have\nany” 1s NOT the right answer.  So, what should  you ask when the interviewer  says, “Do you have any\nquestions  for us?”\nThere is a right way to answer  this! Don’t waste this time asking random  questions  like how much\ntime you get off or how much the job pays! Traditional  advice says, “This is the time to interview\nthe company.”  We disagree!  Be strategic  about what questions  you ask. Have the mindset  “until |\nhave the offer in hand, I need to keep showing  why I’m a good fit” --- you aren't interviewing  them,\nyou’re selling  yourse/f!  As such, prepare  at least three smart, interesting  questions  per interviewer.\nDont  pass on this! You can ask about salary once you’ ve got the job 1n the bag. At that point, you are\nin a far better position  to discuss  compensation.\nAs we mention  later in Chapter 10: Product Sense, this is the time to leverage  the company  and\nproduct  research  you did. You'll gain much more by asking questions  that convey your interest In\nAce the Data Science  Interview 33"
  },
  {
    "page_number": 46,
    "content": "CHAPTER  4: ACE THE BEHAVIORAL  INTERVIEW\nthe company  and what they do. From your readings  and product  analysis,  surely you must be curious\nabout some internal detail. design decision,  or what’s coming next for some product. This point in\nthe interview  is your opportunity  not only to have your intellectual  curiosity  fulfilled,  but to impress\nthem with your research  and genuine  interest in the company  and its business.\nAnother  idea is to check out details about your interviewer  on LinkedIn.  It’s not uncommon  to know\nwho you’ll be interviewing  with. Asking a personal  question is a sure way to get the interviewer\ntalking about themself.  And people love to do that! If you can tailor questions  to their background  or\nprojects  they’ve  worked  on, great! If not, you can ask these sure-fire  conversation  starters:\n¢ How did you come into this role or company?\n¢ What's the most interesting  project  you've  worked  on?\n¢ What do vou think is the most exciting  opportunity  for the company  or product?\n¢ In your opinion,  what are the top three challenges  facing  the business?\n¢ What do you think is the hardest  part of this role?\n¢ How do you see the company  values in action during  your day-to-day  work?\nGoing against the grain from traditional  career advice.  we think asking questions  about the role isn't\nthe most beneficial  use of this opportunity.  Sure, you're not going to get into trouble  for asking  about\nthe growth trajectory  for the role at hand, or what success looks like for the position.  It’s just that\nyou'll have ample time, and it’s a better use of your time to ask these questions  affer you have the\njob. While youre in the interview  mode, again, it’s important  to either reinforce  your interest  in the\ncompany,  their mission  and values,  or at least have the interviewer  talk about themself.  We believe\ndiscussing  nuances  about the role isn’t the most productive  step to take without  an offer at hand.\nPost-Interview  Etiquette\nWhew!  Your interview  its finally  over!\nNo, its not!\nSend a follow-up  thank you note via email a few hours after your interview  to keep your name and\nabilities  fresh in their mind. Plus it shows them your interest in the position  is deep and sincere.\nIdeally,  you'll mention  a few of the specific  things you connected  with them over during  the interview\nin your email/note.  This will help jog their memory  as to which interviewee  you were and hopefully\nbring that connection  to mind when they see your name again.\nThe Best Is Yet to Come!\nNow that we've  walked  you through  our process  for landing  more data science  interviews  and passing\nthe behavioral  interview  screen. you're a/most ready for the meat of the book: acing the technical\ndata science  interview.\nBefore we get to the 201 interview  problems,  we have a quick favor to ask from you. Yes. vou. If\nyou're  enjoying  this book, share a photo of  your copy of 4ce the Data Science  Interview  on LinkedIn\nand tag us (Nick Singh and Kevin Huo). Feel tree to add a quick sentence  or two on what's  resonated\nwith you so far. We'll both connect  with you as well as like and comment  on the post. You'll gel more\nLinkedIn  profile  views, followers,  and brownie  points from us this way!\n34 Ace the Data Science  Interview  | Ace the Behavioral  Interview"
  },
  {
    "page_number": 47,
    "content": "Probability\nCHAPTER  5\nOne of the most crucial skills a data scientist  needs to have is the ability to think\nprobabilistically.  Although  probability  is a broad  field  and ranges  from theoretical  concepts\nsuch as measure theory to more practical  applications  involving  various probability\ndistributions,  a strong  foundation  in the core concepts  ofprobability  is essential.\nLn interviews,  probability  s foundational  concepts  are heavily  tested,  particularly  conditional\nprobability  and basic applications  involving  PDFs of various  probability  distributions.  In\nthe finance industry, interview  questions  on probability,  including  expected  values and\nbetting  decisions,  are especially  common.  More in-depth  problems  that build off of these\nfoundational  probability  topics are common  in statistics  interview  problems,  which we\ncover in the next chapter.  For now, we'll start with the basics  of  probability.\nBasics\nConditional  Probability\nWe are often interested  in knowing  the probability  of an event A given that an event B has occurred.\nFor example,  what 1s the probability  of a patient having a particular  disease,  given that the patient\ntested positive  for the disease?  This is known as the conditional  probability  of A given B and is often\nfound in the following  form based on Bayes’  rule:\nP(B|A)P(A)\nP(B)\nUnder Bayes’ rule, P(A) is known as the prior. P(B A) as the likelihood,  and P(A|B) as the posterior.P(A|B)=\nAce the Data Science  Interview 35"
  },
  {
    "page_number": 48,
    "content": "CHAPTER  5: PROBABILITY\nIf this conditional  probability  is presented  simply as P(A)—that  is, if P(A|B) = P(A)}—then  A and\nB are independent,  since knowing  about B tells us nothing about the probability  of A having also\noccurred.  Similarly,  it is possible  for A and B to be conditionally  independent  given the occurrence  of\nanother  event C: P(A A B|C) = P(A|COP(BIC).\nThe statement  above says that, given that C has occurred,  knowing  that B has also occurred  tells us\nnothing  about the probability  of A having  occurred.\nIf other information  is available  and you are asked to calculate  a probability,  you should always\nconsider using Bayes’ rule. It is an incredibly  common interview topic, so understanding  its\nunderlying  concepts  and real-life applications  involving  it will be extremely  helpful. For example,\nin medical  testing for rare diseases,  Bayes’ rule is especially  important,  since it is may be misleading\nto simply diagnose  someone  as having a disease—even  if the test for the disease is considered  “very\naccurate”—without  knowing  the test’s base rate for accuracy.\nBayes’ rule also plays a crucial part in machine  learning,  where, frequently,  the goal is to identify  the\nbest conditional  distribution  for a variable  given the data that is available.  In an interview,  hints will\noften be given that you need to consider  Bayes’ rule. One such strong hint is an interviewer's  wording\nin directions  to find the probability  of some event having occurred  “given that” another  event has\nalready  occurred.\nLaw of Total Probability\nAssume we have several disjoint events within B having occurred;  we can then break down the\nprobability  of an event 4 having  also occurred  thanks to the law of total probability,  which 1s stated\nas follows:  P(A) = P(A|B,)P(B,)  + ...+ P(A|B )P(B ).\nThe equation  above provides  a handy way to think about partitioning  events. If we want to model\nthe probability  of an event A happening,  it can be decomposed  into the weighted  sum of conditional\nprobabilities  based on each possible  scenario  having  occurred.  When asked to assess a probability\ninvolving  a “tree of outcomes”  upon which  the probability  depends,  be sure to remember  this concept.\nOne common  example  is the probability  that a customer  makes a purchase,  conditional  on which\ncustomer  segment  that customer  falls within.\nCounting\nThe concept  of counting  typically  shows  up in one form or another  in most interviews.  Some  questions\nmay directly  ask about counting  (e.g., “How many ways can five people  sit around  a lunch table?”),\nwhile others may ask a similar  question,  but as a probability  (e.g., “What  is the likelihood  that I draw\nfour cards of the same suit?’’),\nTwo forms of  counting  elements  are generally  relevant.  If the order of selection  of the n items being\ncounted  k at a time matters,  then the method  for counting  possible  permutations  is employed:\nn!\n(n—k)!\nIn contrast,  if order of selection  does not matter, then the technique  to count possible  number  of\ncombinations  is relevant:\nn n!\nk}) k'n-k)!me(n-1l)*..*#n-k+1jy=\n36 Ace the Data Science  Interview | Probability"
  },
  {
    "page_number": 49,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nKnowing  these concepts  is necessary  in order to assess various  probabilities  that involve  counting\nprocedures.  Therefore,  remember  to determine  when selection  does versus  does not matter.\nFor some real-life  applications  of both, consider  making  up passwords  (where  order of characters\nmatters)  versus  choosing  restaurants  nearby  ona map (where  order  does not matter,  only the options).\nLastly, both permutations  and combinations  are frequently  encountered  in combinatorial  and graph\ntheory-related  questions.\nRandom  Variables\nRandom  variables  are a core topic within probability,  and interviewers  generally  verify that you\nunderstand  the principles  underlying  them and have a basic ability to manipulate  them. While it is\nnot necessary  to memorize  all mechanics  associated  with them or specific  use cases, knowing  the\nconcepts  and their applications  is highly  recommended.\nA random  vartable  is a quantity  with an associated  probability  distribution.  It can be either discrete\n(1.e., have a countable  range)  or continuous  (have an uncountable  range).  The probability  distribution\nassociated  with a discrete  random  variable  is a probability  mass function  (PMEF),  and that associated\nwith a continuous  random  variable  is a probability  density  function  (PDF). Both can be represented\nby the following  function  of x: f, (x)\nIn the discrete  case, X can take on particular  values with a particular  probability,  whereas,  in the\ncontinuous  case, the probability  of a particular  value of x is not measurable;  instead,  a ‘probability\nmass” per unit per length around  x can be measured  (imagine  the small interval  of x and x + 8).\nProbabilities  of both discrete  and continuous  random  variables  must be non-negative  and must sum\n(in the discrete  case) or integrate  (in the continuous  case) to 1:\nDiscrete: } fy (x) = 1, Continuous:  { fy (x)dx =1\nxEX\nThe cumulative  distribution  function  (CDF) 1s often used in practice  rather than a variable’s  PMF or\nPDF and is defined  as follows  in both cases: F’, (x) = p(X < x)\nFor a discrete random variable, the CDF is given by a sum: F, (x)= > pk); whereas, for a\ncontinuous  random  variable,  the CDF is given by an integral: ksx\nFy(x)= |  p(y)dy\nThus, the CDF, which is non-negative  and monotonically  increasing,  can be obtained  by taking the\nsums of PMFs for discrete  random  variables,  and the integral of PDFs for continuous  random  variables.\nKnowing  the basics of PDFs and CDFs is very useful for deriving  properties  of random  variables,  so\nunderstanding  them is important.  Whenever  asked about evaluating  a random  variable,  it 1s essential\nto identify  both the appropriate  PDF and CDF at hand.\nJoint, Marginal,  and Conditional  Probability  Distributions\nRandom  variables  are often analyzed  with respect  to other random  variables,  giving rise to joint PMFs\nfor discrete random variables  and joint PDFs for continuous  random variables.  In the continuous\ncase, for the random variables  X and Y varying over a two-dimensional  space, the integration  of the\njoint PDF yields the following:\n[J fev (x, y)dxdy =1\nAce the Data Science  Interview 37"
  },
  {
    "page_number": 50,
    "content": "CHAPTER  5 : PROBABILITY\nThis is useful, since it allows for the calculation  of probabilities  of events involving  X and Y.\nFrom a joint PDF, a marginal PDF can be derived. Here, we derive the marginal PDF for X by\nintegrating  out the Y term:\nhy (x)= {_ hey (x, y)dy\nSimilarly,  we can find a joint CDF where  F, ,. (x, y) = P(X sx, Y<y) is equivalent  to the following:\nFyy(ma)=  | fo fey v)dvdu\nIt is also possible  to condition  PDFs and CDFs on other variables. For example,  for random  variables\nX and Y, which are assumed  to be jointly distributed,  we have the following  conditional  probability:\nhy (x) = [ft (y) fey (x | y)dy\nwhere X is conditioned  on Y. This is an extension  of Bayes’ rule and works in both the discrete  and\ncontinuous  case, although  in the former,  summation  replaces  integration.\nGenerally,  these topics are asked only in very technical  rounds,  although  a basic understanding  helps\nwith respect  to general  derivations  of properties.  When asked about more than one random  variable,\nmake it a point to think in terms of joint distributions.\nProbability  Distributions\nThere are many probability  distributions,  and interviewers  generally  do not test whether  you have\nmemorized  specific  properties  on each (although  it is helpful  to know the basics),  but, rather,  to see if\nyou can properly  apply them to specific  situations.  For example,  a basic use case would  be to assess\nthe probability  that a certain event occurs when using a particular  distribution,  in which case you\nwould directly  utilize the distribution’s  PDF. Below are some overviews  of the distributions  most\ncommonly  included  in interviews.\nDiscrete  Probability  Distributions\nThe binomial  distribution  gives the probability  of k number  of successes  in n independent  trials,\nwhere each trial has probability  p of success.  Its PMF is\nnal (1-p)\"\"\nand its mean and variance  are: p = np, o? = np(1 — p).Pix=h)=(\nThe most common  applications  for a binomial  distribution  are coin flips (the number  of heads in 7\nflips), user signups,  and any situation  involving  counting  some number  of successful  events where\nthe outcome  of each event is binary.\nThe Poisson  distribution  gives the probability  of the number  of events  occurring  within a particular\nfixed interval where the known, constant rate of each event’s occurrence  is A. The Poisson\ndistribution’s  PMF ts\n-AxkerP(X =k) = ——\n| k!\nand its mean and variance  are: 1) = A, o? =A.\n38 Ace the Data Science  Interview | Probability"
  },
  {
    "page_number": 51,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nThe most common  applications  for a Poisson  distribution  are in assessing  counts over a continuous\ninterval,  such as the number  of visits to a website  in a certain  period  of time or the number  of defects\nin a square foot of fabric. Thus, instead  of coin flips with probability  p of a head as a use case of the\nbinomial  distribution,  applications  on the Poisson  will involve  a process X occurring  at a rate A.\nContinuous  Probability  Distributions\nThe uniform  distribution  assumes  a constant  probability  of an X falling  between  values  on the interval\na to b. Its PDF is\n1\n(x) = ——\n. f b-a\nand its mean and variance  are:\na+b 2 (b _ a)\nLu =——-,  0° = ————_\n2 12\nThe most common  applications  for a uniform  distribution  are in sampling  (random  number  generation,\nfor example)  and hypothesis  testing  cases.\nThe exponential  distribution  gives the probability  of the interval  length between  events  of a Poisson\nprocess  having  a set rate parameter  of i. Its PDF is f(x) = Ae * and its mean and variance  are:\nual gal\nrn ?\nThe most common  applications  for an exponential  distribution  are in wait times, such as the time\nuntil a customer  makes  a purchase  or the time until a default  in credit occurs.  One of the distribution’s\nmost useful properties,  and one that makes for natural  questions,  is the property  of memorylessness\nthe distribution.\nThe normal  distribution  distributes  probability  according  to the well-known  bell curve over a range\nof X°s. Given a particular  mean and variance,  its PDF 1s\nf(x) = tb exp- (=u)2n0 20°\nand its mean and variance  are given by: p = p, 6? = 0”\nMany applications  involve the normal distribution,  largely due to (a) its natural fit to many real-life\noccurrences,  and (b) the Central Limit Theorem  (CLT). Therefore,  it is very important  to remember\nthe normal  distribution’s  PDF.\nMarkov  Chains\nA Markov  chain is a process in which there is a finite set of states, and the probability  of being in a\nparticular  state is only dependent  on the previous  state. Stated another way, the Markov property  1s\nsuch that, given the current  state, the past and future states it will occupy  are conditionally  independent.\nThe probability  of transitioning  from state  i to state j at any given time is given by a transition  matrix,\ndenoted  by P:\nPy, Py,\nPn} Pmn\nAce the Data Science  Interview 39"
  },
  {
    "page_number": 52,
    "content": "CHAPTER  5: PROBABILITY\nVarious characterizations  are used to describe  states. A recurrent  state is one whereby,  if entering  that\nstate, one will always transition  back into that state eventually.  In contrast,  a transient  state 1s one In\nwhich, if entered,  there is a positive  probability  that upon leaving,  one will never enter that state again.\nA stationary  distribution  for a Markov  chain satisfies  the following  characteristic:  n =P, where P is\na transition  matrix, and remains fixed following  any transitions  using P. Thus, P contains  the long-\nrun proportions  of the time that a process will spend in any particular  state over lime.\nUsual questions  asked on this topic involve setting up various problems as Markov chains and\nanswering  basic properties  concerning  Markov chain behavior.  For example,  you might be asked to\nmodel the states of users (new, active, or churned)  for a product  using a transition  matrix and then be\nasked questions  about the chain’s long-term  behavior.  It is generally  a good idea to think of Markov\nchains when multiple  states are to be modeled  (with transitions  between  them) or when questioned\nconcerning  the long-term  behavior  of some system.\nProbability  Interview  Questions\nEasy\n5.1. Google:  Two teams play a series of games (best of 7 — whoever  wins 4 games first) in which\neach team has a 50% chance of winning  any given round (no draws allowed).  What is the\nprobability  that the series goes to 7 games?\n5.2. JP Morgan:  Say you roll a die three times. What is the probability  of getting  two sixes in a row?\n5.3. Uber: You roll three dice, one after another. What is the probability  that you obtain three\nnumbers  in a strictly  increasing  order?\n5.4. Zenefits:  Assume  you have a deck of 100 cards with values ranging  from 1 to 100, and that you\ndraw two cards at random  without  replacement.  What ts the probability  that the number  of one\ncard 1s precisely  double  that of the other?\n5.5. JP Morgan:  Imagine  you are in a 3D space. From (0,0,0) to (3,3,3),  how many paths are there\nif you can move only up, right, and forward?\n5.6. Amazon:  One 1n a thousand  people  have a particular  disease,  and the test for the disease  1s 98%\ncorrect in testing for the disease.  On the other hand, the test has a 1% error rate if the person\nbeing tested does not have the disease.  If someone  tests positive,  what are the odds they have\nthe disease’?\n5.7. Facebook:  Assume  two coins, one fair (having  one side heads and one side tails) and the other\nunfair (having  both sides tails). You pick one at random,  flip it five times, and observe  that it\ncomes up as tails all five times. What is the probability  that you are flipping  the unfair  coin?\n5.8. Goldman  Sachs: Players  A and B are playing  a game where they take turns flipping  a biased\ncoin, with p probability  of landing  on heads (and winning).  Player  A starts the game, and then\nthe players  pass the coin back and forth until one person flips heads and wins. What is the\nprobability  that A wins?\n5.9. Microsoft:  Three friends  in Seattle  each told you it is rainy, and each person  has a 1/3 probability\nof lying. What is the probability  that Seattle is rainy, assuming  that the likelihood  of rain on\nany given day is 0.25?\n5.10. Bloomberg:  You draw a circle and choose  two chords at random.  What is the probability  that\nthose chords  will intersect?\n40 Ace the Data Science  Interview | Probability"
  },
  {
    "page_number": 53,
    "content": "5.11.\n5.12.\n5.13.\n5.14.\n5.15.\n5.16.\n5.17.\n5.18.ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nMorgan  Stanley:  You and your friend are playing  a game. The two of you will continue  to toss\na coin until the sequence  HH or TH shows up. If HH shows up first, you win. If TH shows up\nfirst, your friend wins. What is the probability  of you winning?\nJP Morgan:  Say you are playing  a game where you roll a 6-sided  die up to two times and can\nchoose  to stop following  the first roll if you wish. You will receive  a dollar  amount  equal to the\nfinal amount  rolled. How much are you willing  to pay to play this game?\nFacebook:  Facebook  has a content  team that labels pieces of content  on the platform  as either\nspam or not spam. 90% of them are diligent  raters  and will mark 20% of the content  as spam and\n80% as non-spam.  The remaining  10% are not diligent  raters and will mark 0% of the content\nas spam and 100% as non-spam.  Assume  the pieces of content  are labeled  independently  of\none another,  for every rater. Given that a rater has labeled  four pieces  of content  as good, what\nis the probability  that this rater is a diligent  rater?\nD.E. Shaw: A couple  has two children.  You discover  that one of their children  is a boy. What\nis the probability  that the second  child is also a boy?\nJP Morgan:  A desk has eight drawers.  There is a probability  of 1/2 that someone  placed  a letter\nin one of the desk’s eight drawers  and a probability  of 1/2 that this person  did not place  a letter\nin any of the desk’s  eight drawers.  You open the first 7 drawers  and find that they are all empty.\nWhat is the probability  that the 8th drawer  has a letter in it?\nOptiver:  Two players  are playing  in a tennis match, and are at deuce (that is, they will play\nback and forth until one person has scored two more points than the other). The first player\nhas a 60% chance  of winning  every point, and the second  player  has a 40% chance  of  winning\nevery point. What 1s the probability  that the first player  wins the match?\nFacebook:  Say you have a deck of 50 cards made up of cards in 5 different  colors, with 10\ncards of each color, numbered  | through 10. What is the probability  that two cards you pick al\nrandom  do not have the same color and are also not the same number?\nSIG: Suppose  you have ten fair dice. If you randomly  throw these dice simultaneously,  what 1s\nthe probability  that the sum of all the top faces is divisible  by 6?\nMedium\n5.19.\n5.20.\n5.21.\n5.22.\n5.23.Morgan  Stanley:  A and B play the following  game: a number  k from 1-6 is chosen,  and A and\nB will toss a die until the first person throws a die showing  side k, after which that person is\nawarded  $100 and the game is over. How much is A willing  to pay to play first in this game?\nAirbnb:  You are given an unfair coin having  an unknown  bias towards  heads or tails. How can\nyou generate  fair odds using this coin?\nSIG: Suppose  you are given a white cube that is broken into 3 x 3 x 3 — 27 pieces. However,\nbefore the cube was broken, all 6 of its faces were painted green. You randomly  pick a small\ncube and see that 5 faces are white. What is the probability  that the bottom face 1s also white?\nGoldman  Sachs: Assume  you take a stick of length | and you break it uniformly  at random  into\nthree parts. What is the probability  that the three pieces can be used to form a triangle?\nLyft: What is the probability  that, in a random sequence  of H’s and T’s, HHT shows up before\nHTT?\nAce the Data Science  Interview 4l"
  },
  {
    "page_number": 54,
    "content": "5.24.\n5.25.\n5.26.\n5.27.\n5.28.CHAPTER  5 : PROBABILITY\nUber: A fair coin is tossed twice, and you are asked to decide whether  it is more likely that two\nheads showed  up given that either (a) at least one toss was heads, or (b) the second toss was a\nhead. Does your answer  change if you are told that the coin is unfair?\nFacebook:  Three ants are sitting at the corners of an equilateral  triangle. Each ant randomly\npicks a direction  and begins moving  along an edge of the triangle.  What is the probability  that\nnone of the ants meet? What would your answer be if there are, instead, k ants sitting on all k\ncorners  of an equilateral  polygon?\nRobinhood:  A biased coin, with probability  p of landing on heads, is tossed n times. Wnite a\nrecurrence  relation  for the probability  that the total number  of heads after n tosses is even.\nCitadel:  Alice and Bob are playing  a game together.  They play a series of rounds until one of\nthem wins two more rounds than the other. Alice wins a round with probability  p. What is the\nprobability  that Bob wins the overall  series?\nGoogle: Say you have three draws  of a uniformly  distributed  random  variable  between  (0, 2).\nWhat is the probability  that the median  of the three is greater  than 1.5?\nHard\n5.29.\n5.30.\n5.31.\n5.32.\n5.34.\n5.35.D.E. Shaw: Say you have 150 friends, and 3 of them have phone numbers  that have the last\nfour digits with some permutation  of the digits 0, 1, 4, and 9. What’s the probability  of this\noccuring?\nSpotify:  A fair die is rolled n times. What is the probability  that the largest number  rolled is r,\nfor each  r in 1,...,6?\nGoldman  Sachs: Say you have  a jar initially  containing  a single amoeba  in it. Once every\nminute,  the amoeba  has a | in 4 chance  of doing one of four things: (1) dying out, (2) doing\nnothing,  (3) splitting into two amoebas,  or (4) splitting  into three amoebas.  What is the\nprobability  that the jar will eventually  contain  no living amoeba?\nLyft: A fair coin is tossed n times. Given that there were k heads in the n tosses, what is the\nprobability  that the first toss was heads?\n. Quora: You have N1.1.d.  draws of numbers  following  a normal  distribution  with parameters\nand o. What is the probability  that & of those draws are larger  than some value Y?\nAkuna Capital:  You pick three random  points on a unit circle and form a triangle  from them.\nWhat is the probability  that the triangle  includes  the center  of the unit circle?\nCitadel:  You have r red balls and w white balls in a bag. You continue  to draw balls from the\nbag until the bag only contains  balls of one color. What is the probability  that you run out of\nwhite balls first?\nProbability  Interview  Solutions\nSolution  #5.1\nFor the series to go to 7 games,  each team must have won exactly  three times for the first 6 games,\nan occurrence  having  probability\n(\") 20. 5ot .\n42Ace the Data Science  Interview  | Probability"
  },
  {
    "page_number": 55,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nwhere the numerator  is the number  of ways of splitting  up 3 games won by either side, and the\ndenominator  is the total number  of possible  outcomes  of 6 games.\nSolution  #5.2\nNote that there are only two ways for 6s to be consecutive:  either the pair happens  on rolls 1 and 2 or\n2 and 3, or else all three are 6s. In the first case, the probability  is given by\n6)\\6/) 216\n6 216\n10, 1 Ue\n216 216 216and, for all three, the probability  is\nThe desired  probability  is given by:\nSolution  #5.3\nFirst, note that the three rolls must all yield different  numbers;  otherwise,  no strictly increasing\norder is possible.  The probability  that the three numbers  will be different  is given by the following\nreasoning.  The first number  can be any value from | through  6, the second  number  has a 5/6 chance\nof not being the same number  as the first, and the third number  has a 4/6 chance  of not being the prior\ntwo numbers.  Thus,\nConditioned  on there being three different  numbers,  there 1s exactly  one particular  sequence  that will\nbe in a strictly  increasing  order, and this sequence  occurs with probability  1/3! = 1/6.\n5Therefore,  the desired probability  1s given by: = * - = a4\nSolution  #5.4\n100\nNote that there are a total of 9 = 4950\nways to choose  two cards at random  from the 100. There are exactly  50 pairs that satisfy  the condition:\n(1, 2),..., (50, 100). Therefore,  the desired  probability  1s:\n_50_ =O0.01\n4950\nSolution  #5.5\nNote that getting to (3, 3, 3) requires  9 moves. Using these 9 moves, it must be the case that there are\nexactly three moves in each of the three directions  (up, right, and forward).  There are therefore  9!\nways to order the 9 moves in any given direction.  We must divide by 3! for each direction  to avoid\novercounting,  since each up move is indistinguishable.  Therefore,  the number  of paths 1s:\n9!——— = 1680\n313!3!\nAce the Data Science  Interview 43"
  },
  {
    "page_number": 56,
    "content": "CHAPTER  5 : PROBABILITY\nSolution  #5.6\nLet A denote the event that someone  has the disease, and B denote the event that this person tests\npositive  for the disease.  Then we want: P(A|B)\nP(B\\A)P(A)\nP(B)By applying  Bayes’ theorem,  we obtain: P(A|B)=\nFrom the problem  description,  we know that P(B|A)  = 0.98, P(A) = 0.001\nLet A’ denote the event that someone  does not have the disease.  Then, we know that P( BIA’) = 0.01.\nFor the denominator,  we have:\nP(B) = P(BIA)P(A)  + P(BIA’)P(A’)  = 0.98(0.001)  + 0.01(0.999)\nTherefore,  after combining  terms, we have the following:\n0.98 *0.001= = 8.93%\n0.98(0.001)  + 0.01(0.999)P(A|B)\nSolution  #5.7\nWe can use Bayes’ theorem  here. Let U denote the case where we are flipping  the unfair coin and F\ndenote the case where we are flipping  a fair coin. Since the coin is chosen randomly,  we know that\nP(U) = P(F) = 0.5. Let 5T denote the event of flipping  5 tails in a row. Then, we are interested  in\nsolving  for P(UI57),  i.e., the probability  that we are flipping  the unfair coin, given that we obtained\n5 tails in a row.\nWe know P(57|U)  = 1, since, by definition,  the unfair coin always  results in tails. Additionally,  we\nknow that P(ST|F)  = 1/2%5 — 1/32 by definition  of a fair coin. By Bayes’  theorem,  we have:\nP(5T |[U)* P(U) 0.5P(U\\5T) = -(W157)  = ys Pi) a Plo F)* PUP) 0520591390.97\nTherefore,  the probability  we picked  the unfair  coin is about 97%.\nSolution  #5.8\nLet P(A) be the probability  that A wins. Then, we know the following  to be true:\n1. IfA flips heads initially, A wins with probability  1.\n2. If A flips tails initially,  and then B flips a tail, then it is as if neither  flip had occurred,  and so A\nwins with probability  P(A).\nCombining  the two outcomes,  we have: P(A) = p + (1 — p)*P(A),  and simplifying  this yields\nP(A) = p + P(A) — 2pP(A)  + p*P{A)  so that p?P(A)  — 2pP(A)  + p =0\nand hence: P(A) = —\n—p\nSolution  #5.9\nLet R denote the event that it ts raining,  and Y be a “yes” response  when you ask a friend if it is\nraining.  Then, from Bayes’  theorem,  we have the following:\n(YYY | R)P(R)P(R|YYY)=_ |\nP(YYY)\n44 Ace the Data Science  Interview | Probability"
  },
  {
    "page_number": 57,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nwhere  the numerator  is given by:\nP (yy 1R)P(R)=(2) (*)- 2\nLet A’ denote  the event of no rain; then the denominator  is given by the following:\nP(YYY) = P(YYY|R)P(R)  + P(YYYIR)P(R’)  = (2) ( | ‘ (2) (2)\no/ \\A4 3/ \\4\nwhich, when simplified,  yields: P(YYY)  = on\n2\nCombining  terms, we obtain the desired  probability:  P(R|YYY)  = t= ~\n108\nSolution  #5.10\nBy definition,  a chord is a line segment  where the two endpoints  lie on the circle. Therefore,  two\narbitrary  chords can always  be represented  by any four points chosen  on the circle. If you choose  to\nrepresent  the first chord by two of the four points,  then you have:\n()\nchoices  of choosing  the two points  to represent  chord | (and, hence the other two will represent  chord\n2). However,  note that in this counting,  we are duplicating  the count of each chord twice, since a\nchord with endpoints  p! and p2 is the same as a chord with endpoints  p2 and p1. That is, chord  AB is\nthe same as BA, (likewise  with CD and DC). Therefore,  the proper  number  of valid chords 1s:\nAmong  these three configurations,  only one of the chords  will intersect;  hence,  the desired  probability\niS:\n_1\nPe 3\nSolution  #5.11\nAlthough  there is a formal way to apply Markov  chains to this problem,  there is a simple trick that\nsimplifies  the problem greatly. Note that, if T is ever flipped, you cannot then reach HH before\nyour friend reaches TH, since the first heads thereafter  will result in them winning.  Therefore,  the\nprobability  of you winning  is limited to just flipping  an HH initially,  which we know 1s given by the\nfollowing  probability:\nP(HH)=+*+=+22 4\nTherefore,  you have a 1/4 chance of winning,  whereas  your friend has a 3/4 chance.\nSolution  #5.12\nThe price you would be willing to pay is equal to the expectation  of the final amount. Note that, for\nthe first roll, the expectation  is\n“. i 0 21— = — = 3.526 6...\nAce the Data Science  Interview 45"
  },
  {
    "page_number": 58,
    "content": "CHAPTER  5: PROBABILITY\nTherefore.  there are two events on which you need to condition.  The first is on getting a 1, 2. or 3 on\nthe first roll, in which case you would roll again (since a new roll would have an expectation  of 3.5,\nand so, overall, you have an expectation  of 3.5. The second is on if you roll a 4. 5, or 6 on the first\nroll, in which case you would keep that roll and end the game, and the overall expectation  would be\n5, the average  of 4, 5, and 6. Therefore,  the expected  payoff of the overall game 1s\n1 g54+%5  = 4.25\n2 2\nTherefore,  you would be willing  to pay up to $4.25 to play.\nSolution  #5.13\nLet D denote the case where  a rater is diligent,  and E the case where a rater is non-diligent.  Further,\nlet 4N denote the case where four pieces of content are labeled as non-spam.  We want to solve for\nP(DI4N),  and can use Bayes’  theorem  as follows  to do so:\nP(4N | D)* P(D)\nP(4N|D)*  P(D)+ P(AN|E)*  P(E)\nWe are given that P(D) = 0.9, P(E) = 0.1. Also, we know that P(4ND)  — 0.8 * 0.8 * 0.8 * 0.8 due\nto the independence  of each of the 4 labels assigned  by a diligent rater. Similarly,  we know thatP(D|4N)=\nP(4NE)  = 1, since a non-diligent  rater always labels content as non-spam.  Substituting  into the\nequation  above yields the following:\nP(4N|D)*  P(D) 0.8° *0.9\nme ree - = =0.7P(4N|D)*P(D)+P(4N|E)*P(E)  0.8°* 0.941 *0.1 i”\nTherefore,  the probability  that the rater ts diligent  1s 79%.\nSolution  #5.14\nThis is a tricky problem,  because  your mind probably  jumps to the answer  of 1:2 because  knowing\nthe gender of one child shouldn't  atfect the gender of the other. However,  the phrase “the second\nchild ts also a boy” imphes  that we want to know the probability  that both children  are boys given\nthat one Is a boy. Let B represent  a boy and G represent  a girl. We then have the following  total\nsample  space representing  the possible  genders  of 2 children:  BB, BG, GB, GG.\nHowever,  since one child was said to be a boy, then valid sample  space ts reduced  to the following:\nBB, BG, GB.\nSince all of these options  are equally  likely, the answer  is simply 1/3.\nSolution  #5.15\nLet A denote the event that there is a letter in the Sth drawer,  and B denote the event that the first 7\ndrawers  are all empty.\nThe probability  of B occurring  can be found by conditioning  on whether  a letter was put in the\ndrawers  or not if so. then cach drawer is equally likely to contain a letter, and if net. then none\ncontain  the letter. Therefore,  we have the following:\npipr=(F)(  2) (SJa- 3\nale) 2 16\n46 Ace the Data Science  interview | Probability"
  },
  {
    "page_number": 59,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nFor A and B to both occur, we also know that: P(A c. B) = (= )(4) = I\n2/\\8/ 16\nTherefore,  we have: P(A|B)=  P(ANB)  i\nP(B) 9\nSolution  #5.16\nWe can use a recursive  formulation.  Let p be the probability  that the first player wins. Assume  the\nscore is 0-0 (on a relative  basis).\nif the first player  wins a game (with probability  0.6), then two outcomes  are possible:  with probability\n0.6 the first player  wins, and with probability  0.4 the score is back to 0-0, with p being the probability\nof the first player  winning  overall.\nSimilarly,  if the first player loses a game (with probability  0.4), then with probability  0.6 the score is\nback to 0-0 (with p being the probability  of the first player  winning),  or, with probability  0.4, the first\nplayer  loses. Therefore,  we have: p = 0.6? + 2(0.6)(0.4)p\nSolving  this yields the following  for p: p ~ 0.692\nThe key idea to solving  this and similar  problems  is that, after two points. either the game is over,\nor we're back where we started. We don’t need to ever consider  the third, fourth. etc.. points in an\nindependent  way.\nSolution  #5.17\nThe first card will always  be a unique color and number,  so let’s consider  the second  card. Let A be\nthe event that the color of card 2 does not match that of card |, and let B be the event that the number\nof card 2 does not match that of card |. Then, we want to find the following:\nP(A rn B)\nNote that the two events  are mutually  exclusive:  two cards with the same colors cannot  have the same\nnumbers,  and vice versa. Hence,  P(A 7 B) = P(A)P(BIA)\nFor A to occur. there are 40 remaining  cards of a color different  from that of the first card drawn (and\n49 remaining  cards altogether).  Therefore,\n_ 40\n49\nFor B, we know that. of the 40 remaining  cards, 36 of them (9 in each color) do not have the same\nnumber  as that of card 1.P(A)\n36\nTherefore,  P(B| A) = —40\n40 36 36 . oqe : ( —- —  * ——_  = —-Thus, the desired  probability  is: P AB) 49 40 49\nSolution  #5.18\nConsider  the first nine dice. The sum of those nine dice will be either 0. 1. 2. 3. 4. or 3 modulo 6.\nReyardless  of that sum. exactly one value for the tenth die will make the sum of all 10 divisible  by 6.\nFor instance. if the sum of the first nine dice is | modulo 6, the sum of the first 10 will be divisible\nby 6 only when the tenth die shows a 5. Thus, the probability  is 1 6 for any number of dice. and.\ntherefore,  the answer  is simply 1/6.\nAce the Data Science  Interview 47"
  },
  {
    "page_number": 60,
    "content": "CHAPTER  5: PROBABILITY\nSolution  #5.19\nTo assess the amount  A is willing to pay, we need to calculate  the expected  probabilities  of winning\nfor each player, assuming  A goes first. Let the probability  of A winning  (if A goes first) be given by\nP(A), and the probability  of B winning  (if A goes first but doesn’t win on the first roll) be P(B’).\n1 5 ,\nThen we can use the following  recursive  formulation:  P(A)= 6 + a - P(B’))\nSince A wins immediately  with a 1/6 chance (the first roll is &), or with a 5/6 chance (assuming  the\nfirst roll is not a k), A wins if B does not win, with B now going first.\nHowever,  notice that, if A doesn’t roll side & immediately,  then P(B’) = P(A), since now the game 1s\nexactly  symmetric  with player  B going first.\n1 5 6\nTherefore,  the above can be modeled  as follows:  P(A) = 6 ten gp iA)\nSolving yields P(A) = 6/11, and P(B) = 1 — P(A) = 5/11. Since the payout is $100, then A should\nbe willing to pay an amount up to the difference  in expected values in going first, which 1s\n100 * (6/11 — 5/11) = 100/11,  or about $9.09.\nSolution  #5.20\nLet P(H) be the probability  of landing  on heads, and P(T) be the probability  of landing  tails for any\ngiven flip, where P(/1) + P(T) = 1. Note that it is impossible  to generate  fair odds using only one flip.\nIf we use two flips, however,  we have four outcomes:  HH, HT, TH, and TT. Of these four outcomes,\nnote that two (HT, TH) have equal probabilities  since P(H) * P(T) = P(T) * P(H). We can disregard\nHH and TT and need to complete  only two sets of flips, e.g., /4/7T  wouldn’t  be equivalent  to HT.\nTherefore,  it is possible  to generate  fair odds by flipping  the unfair  coin twice and assigning  heads to\nthe HT outcome  on the unfair coin, and tails to the TH outcome  on the unfair  coin.\nSolution  #5.21\nThe only possible  candidates  for the cube you selected  are the following:  either it is the inside center\npiece (in which case all faces are white)  or a middle  face (where  5 faces are white, and one face is green).\nThe former  can be placed in six different  ways, and the latter can only be placed in one particular\nway. Since all cubes are chosen  equally  randomly,  let A be the event that the bottom  face of the cube\npicked  1s white, and B be the event that the other five faces are white.\nNote that there is a 1/27 chance  that the piece is the center piece and a 6/27 chance  that the piece is\nthe middle  piece. Therefore,  the probability  of B happening  is given by the following:\nP(B)= 0+ 2 (4)\n27 27\\6\n1Then, using Bayes’ rule: P(A |B) = P(AQB)  _ 2 1\nP(B) vi tan * 6 2\nSolution  #5.22\nAssume  that the stick looks like the following,  with cut points at_X¥ and Y\nLet Af(shown  as ; above)  denote  the stick’s midpoint  at 0.5 of the stick’s l-unit length.  Note that, if XY\nand ¥ fall on the same side of the midpoint,  either on its left or its right, then no triangle  is possible,\n48 Ace the Data Science  Interview  | Probability"
  },
  {
    "page_number": 61,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nbecause,  in that case, the length of one of the pieces would be greater  than 1/2 (and thus we would\nhave two sides having  a total length strictly less than that of the longest side, making  forming  a\ntriangle  impossible).  The probability  that X and Y are on the same side (since the breaks  are assumed\nto be chosen  randomly)  is simply 1/2.\nNow, assume  that X and ¥ fall on different  sides of the midpoint.  If X is further  to the left in its half  than\nY is in its half, then no triangle  is possible  in that case, since then the part lying between  X and Y would\nhave a length strictly  greater  than 0.5 (for example,  X at 0.2 and Y at 0.75). This has a 1/2 chance  of\noccurring  by a simple  symmetry  argument,  but it is conditional  on X and Y being on different  sides of\nthe midpoint,  an outcome  which itself  has a 1/2 chance  of occurring.  Therefore,  this case occurs  with\nprobability  1/4. The two cases represent  all cases in which no valid triangle  can be formed;  thus, it\nfollows  that probability  of a valid triangle  being formed  equals 1 ~ 1/2 — 1/4 = 1/4.\nSolution  #5.23\nNote that both sequences  require  a heads first, and any sequence  of  just tails prior to that is irrelevant  to\neither showing  up. Once the first H appears,  there are three possibilities.  If the next flip is an H, HHT\nwill inevitably  appear  first, since the next 7 will complete  that sequence.  This has probability  1/2.\nIf the next flip is a 7, there are two possibilities.  If 77 appears,  then H7T appeared  first. This has\nprobability  1/4. Alternatively,  if TH appears,  we are back in the initial configuration  of having  gotten\nthe first H. Thus, we have:\ni,ta 2 eon aeSolving  yields p = 3\nSolution  #5.24\nLet A be the event that the first toss is a heads and B be the event that the second  toss is a heads. Then,\nfor the first case, we are assessing:  P(A > B\\A U B), whereas  for the second case we are assessing:\nP(A BIB)\n]1For the first case, we have: P(ANB|AUB)=  PAA pons  UB) = aa = e = ri =3\nA AnB) + 1And, for the second case, we have: P(A B| B) = Pt ats P(B) = es ) = 7 =5\nTherefore,  the second  case is more likely. For an unfair coin, the outcomes  are unchanged,  because  it will\nalways be true that P(A U B) > P(B), so the first case will always be less probable  than the second  case.\nSolution  #5.25\nNote that the ants are guaranteed  to collide unless they each move in the exact same direction.  This\nonly happens  when all the ants move clockwise  or all move counter-clockwise  (picture the triangle\nin 2D). Let P(N) denote the probability  of no collision, P(C) denote the case where all ants go\nclockwise,  and P(D) denote the case where all ants go counterclockwise.  Since every ant can choose\neither direction  with equal probability,  then we have:\nP(N) = P(C)+P(D)=  (+) (2) -}\nAce the Data Science  Interview 49"
  },
  {
    "page_number": 62,
    "content": "CHAPTER  5: PROBABILITY\nIf we extend this reasoning  to & ants, the logic is still the same, so we obtain the following:\nP(N) = P(C)+P(D)=(4]  (2) =s5\nSolution  #5.26\nLet A be the event that the total number  of heads after n tosses is even, B be the event that the first\ntoss was tails, and B’ be the event that the first toss was heads. By the law of total probability,  we have\nthe following:  P(A) = P(A|B)P(B)  + P(A|B’)|P(B’)\nThen, we can write the recurrence  relation  as follows:  P, = (1—- p)P__,+pQ  —- P,_,)\nSolution  #5.27\nNote that since Alice can win with probability  p, Bob, by definition,  can win with probability  1-p. Denote\n1-p as q for convenience.  Let B represent  the event that Bob wins i matches  for 1 = 0, 1, 2. Let B* denote\nthe event that Bob wins the entire series. We can use the law of conditional  probability  as follows:\nP(B*) = P(B* | B,) * P(B,) + P(B* | B,) * P(B,) + P(B* | B,) * P(B,)\nSince Bob wins each round with probability  1-p, we have: P(B,) = q’, P(B,) = 2pq, P(B,) = Pp”\nSubstituting  these values into the above expression  yields: P( B*) = 1 * q? + P(B*) * 2pq + 0 * p*\n2\nHence, the desired probability  is the following:  P(B *) = oe 5\n— 4Qp\nSolution  #5.28\nBecause  the median  of  three numbers  is the middle  number,  the median  is at least 1.5 if at most one\nof the 3 1s strictly less than 1.5 (since the other 2 must be strictly greater  than 1.5). Since each is\nuniformly  randomly  distributed,  then the probability  of any one of them being strictly less than 1.5\nis given by the following:\n1.5 063\n2 4\nTherefore,  the chance  that at most one is strictly  less than 1.5 is given by the sum of probabilities  for\nexactly  one being strictly  less than 1.5 and none being strictly  less than 1.5\nSolution  #5.29\nLet p be the probability  that a phone number  has the last 4 digits involving  only the above given\ndigits (0, 1, 4 and 9).\nWe know that the total number  of possible  last 4 digit combinations  is 10'— 10000, since there are\n10 digits (0-9). There are 4! ways to pick a 4-digit  permutation  of 0, 1, 4 and 9.\n: 4!Theretore,  we have: p = -—-— = 8o\n10000 1250\nNow, since you have 150 friends,  the probability  of there being exactly 3 with this combination  is\ngiven by:\n1503 |e\"-p)\" ~ 0.00535\n50 Ace the Data Science  Interview  | Probability"
  },
  {
    "page_number": 63,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nSolution  #5.30\nLet B be the event that all n rolls have a value less than or equal to r. Then we have:\nr”\nP(B.)=—( .) 6”\nsince all n rolls must have a value less than or equal to r. Let A be the event that the largest  number\nis r. We have: B= B_, UA. and, since the two events on the right-hand  side are disjoint,  we have\nthe following:  P(B_) = P(B__,)  + P(A)\nr” _ (r—1)\"Therefore,  the probability  of A is given by: P(A,)  =P (B, ) -P (B_, ) = 6\" 6\"\nSolution  #5.31\nLet p be the probability  that the amoeba(s)  die out. At any given time step, the probability  of dying\nout eventually  must still be p.\nFor case (1), we know the probability  of survival  is 0 (for one amoeba).\nFor case (2), we know the probability  of dying out is p.\nFor case (3), there are now two amoebas,  and both have a probability  p of dying.\nFor case (4), each of the three amoebas  has a probability  p of dying.\nPutting  all four together,  we note that the probability  of the population  dying out at t = 0 minutes\nmust be the same as the probability  of the population  dying out at ¢ = 1 minutes.  Therefore,  we have:\np=<(1+ p+ p+ p\")\nand solving this yields: p = VJ2-1\nSolution  #5.32\nn\nNote that there are b\nv| ways to choose k heads with the first coin being a T, and a total of\n(\") n-knr\nk\nhways to obtain k heads. So, the probability  of having  a tails first is given by: =\nk n\nand, therefore,  the probability  of obtaining  a heads first is given by the following:\nnck  _k\nn n\nSolution  #5.33\nLet the n draws be denoted  as X,, X,, ...X,\nWe know that, for any given draw i, we have the following:\n-u_y- Y-p(X, > ¥)=1- P(x, <¥)=1-P(*=#<*—#) 1-0 H)\nfe) fe) 0)\nAce the Data Science  Interview 51"
  },
  {
    "page_number": 64,
    "content": "Additionally,  the probability  that & of those draws are greater  than Y follows  a binomial  distribution\nwith the value above being the p parameter:\nnH)=1-| —+p10\n. “4° . . n k n-kThen, the desired probability  is given by: \" p'(1-p)\nSolution  #5.34\nNote that, without  loss of generality,  the first point can be located  at (1, 0). Using the polar coordinate\nsystem,  we have the two other points at angles:  0, o, respectively.\nNote that the second point can be placed on either half (top or bottom)  without  loss of generality.\nTherefore,  assume  that it is on the top half. Then,0<0<z\nIf the third point is also in the top half, then the resulting  triangle  will not contain  the center  of the unit\ncircle. It will also not contain  the center 1f the following  is the case (try drawing  this out): 62> 0+7\nTherefore,  for any given second  point, the probability  of the third point making  the resulting  triangle\ncontain  the center  of the unit circle is the following:\n_ 5\n» 27\nTherefore,  the overall probability  is given by the integrating  over possible  values of 0, where the\nconstant  in front is to take the average:\n1 1— dQ = —8 »? 4\nSolution  #5.35\nIn order to run out of white balls first, all the white balls must be drawn before the r-th red ball is\ndrawn. We can consider  the draws until w + r — 1 (we know the last ball must be red), and count how\nmany include  w white balls.\nThe first white ball has w + r— 1 options,  the second  white ball has w + r — 2 options,  etc., until the\ndrawing  of the w-th white ball: (w + r— 1)(w +r — 2)...(r),  which can be written  as a factorial:\n(w+r—1)!\n(r—1)!\nSimilarly,  there are r! ways to arrange  the drawing  of the remaining  r red balls. We know the total\nnuinber  of balls is r + w, so there are (r + w)! total arrangements.  Therefore,  the probability  is:\n(w+r-1)!\n(r-1)!\n(r+w)! werri r\nA more intuitive  way to approach  the problem  is to consider  just the last ball drawn.  The probability\nthat the ball is red is simply  the chance  of it being red when picking  randomly,  which is the following:\nr\nwt+r"
  },
  {
    "page_number": 65,
    "content": "Statistics\nCHAPTER  6\nStatistics  is a core component  of any data scientist's  toolkit. Since many commercial  layers\nof a data science  pipeline  are built  from statistical  foundations  (for example,  A/B testing),\nknowing  foundational  topics  of  statistics  is essential.\nInterviewers  love to test a candidate  s knowledge  about  the basics  of statistics,  starting  with\ntopics like the Central  Limit Theorem  and the Law of  Large  Numbers,  and then progressing\non to the concepts  underlving  hypothesis-testing,  particularly  p-values  and confidence\nintervals,  as well as Type | and Type II errors and their interpretations.  All of those topics\nplay an important  role in the statistical  underpinning  of A/B testing.  Additionally,  derivations\nand manipulations  involving  random variables  of various  probability  distributions  are\nalso common,  particularly  in finance  interviews.  Lastly, a common  topic in more technical\ninterviews  will involve  utilizing  MLE and/or  MAP.\nTopics  to Review  Before  Your Interview\nProperties  of Random  Variables\nFor any given random  variable  X, the following  properties  hold true (below  we assume  X is continuous,\nbut it also holds true for discrete  random  variables).\nThe expectation  (average  value, or mean) of a random variable  is given by the integral of the value\nof X with its probability  density  function  (PDF)  fx (x):\nnw B[X] = i xf, (xx)dx\nAce the Data Science  Interview 53"
  },
  {
    "page_number": 66,
    "content": "CHAPTER  6: STATISTICS\nand the variance  is given by:\nVar (X) = £|(x - E[X)'|=  £[x\"|-(E[X])\nThe variance is always non-negative,  and its square root is called the standard deviation,  which is\nheavily  used in statistics.\no= War()  = fe [x exif] = fe |-(elx)\nThe conditional  values of both the expectation  and variance  are as follows. For example,  consider  the\ncase for the conditional  expectation  of X, given that Y= y:\nE{X|Y=y|=  f fev (x | y)dx\nFor any given random  variables  X and Y, the covariance,  a linear measure  of relationship  between  the\ntwo variables,  is defined  by the following:\nCov(X,Y)=  E|(X — E[X])(Y¥  - E[Y])]=  E[XY]-  E[X]E[Y]\nand the normalization  of covariance,  represented  by the Greek letter p, is the correlation  between  X\nand Y:\nCov(X,Y)\n(XY) = |Var(X)Var(¥)\nAll of these properties  are commonly  tested in interviews,  so it helps to be able to understand  the\nmathematical  details behind  each and walk through  an example  for each.\nFor example,  if we assume  X follows  a Uniform  distribution  on the interval  [a, 5], then we have the\nfollowing:\n1\nb-a\nTherefore  the expectation  of X 1s:\nEIX]= [ xfg(x)dx = [ax -x?\n2(b—a)a+b\n2|”\nle\nAlthough  it is not necessary  to memorize  the derivations  for all the different  probability  distributions,\nyou should be comfortable  deriving  them as needed,  as it is a common  request in more technical\ninterviews.  To this end, you should make sure to understand  the formulas  given above and be able\nto apply them to some of the common  probability  distributions  like the exponential  or uniform\ndistribution.\nLaw of Large Numbers\nThe Law of Large Numbers  (LLN) states that if you sample a random variable  independently  a\nJarge number  of times, the measured  average  value should converge  to the random  variable’s  true\nexpectation.  Stated more formally,\n> XX, +...4XA =— 2 +, aSn— oo\nn\nThis is important  in studying  the longer-term  behavior  of random  variables  over time. As an example,\na coin might land on heads 5 times in a row, but over a much larger 7 we would  expect  the proportion\n54 Ace the Data Science  Interview  | Statistics"
  },
  {
    "page_number": 67,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nof heads  to be approximately  half of the total flips. Similarly,  a casino  might  experience  a loss on any\nindividual  game, but over the long run should  see a predictable  profit over time.\nCentral  Limit  Theorem\nThe Central Limit Theorem  (CLT) states that if you repeatedly  sample a random  variable  a large\nnumber  of times, the distribution  of the sample  mean will approach  a normal  distribution  regardless\nof the initial distribution  of the random  variable.\nRecall from the probability  chapter  that the normal  distribution  takes on the form:\n(x —\\e)\n20°\nwith the mean and standard  deviation  given by pu and o respectively.\nYu\no/Vn\nThe CLT provides  the basis for much of hypothesis  testing,  which is discussed  shortly.  At a very basic\nlevel, you can consider  the implications  of this theorem  on coin flipping:  the probability  of getting\nsome number  of heads flipped  over a large n should be approximately  that of a normal  distribution.\nWhenever  you’re  asked  to reason  about any particular  distribution  over a large sample  size, you should\nremember  to think of the CLT, regardless  of whether  it is Binomial,  Poisson,  or any other distribution.2X,+..4+X,  a Np\nn nThe CLT states that: X, = ; hence ~ N(0,1)\nHypothesis  Testing\nGeneral  Setup\nThe process of testing whether  or not a sample of data supports  a particular  hypothesis  is called\nhypothesis  testing. Generally,  hypotheses  concern particular  properties  of interest for a given\npopulation,  such as its parameters,  like j1 (for example,  the mean conversion  rate among  a set of users).\nThe steps in testing  a hypothesis  are as follows:\n|. State a null hypothesis  and an alternative  hypothesis.  Either the null hypothesis  will be rejected\n(in favor of the alternative  hypothesis),  or it will fail to be rejected  (although  failing to reject the\nnull hypothesis  does not necessarily  mean it is true, but rather that there is not sufficient  evidence\nto reject it).\n2. Use a particular  test statistic  of the null hypothesis  to calculate  the corresponding  p-value.\n3. Compare  the p-value  to a certain significance  level a.\nSince the null hypothesis  typically represents  a baseline (e.g., the marketing  campaign  did not\nincrease  conversion  rates, etc.), the goal is to reject the null hypothesis  with statistical  significance\nand hope that there is a significant  outcome.\nHypothesis  tests are either one- or two-tailed  tests. A one-tailed  test has the following  types of null\nand alternative  hypotheses:\nH, : =, versus H, : <p, or A, +> th,\nwhereas  a two-tailed  test has these types: H, : =p, versus H, : = py\nwhere H, is the null hypothesis  and #7, is the alternative  hypothesis,  and p is the parameter  of interest.\nAce the Data Science  Interview 55"
  },
  {
    "page_number": 68,
    "content": "CHAPTER  6 : STATISTICS\nUnderstanding  hypothesis  testing is the basis of A/B testing, a topic commonly  covered in tech\ncompanies’  interviews.  In A/B testing,  various  versions  of a feature  are shown to a sample  of different\nusers, and each variant is tested to determine  if there was an uplift in the core engagement  metrics.\nSay, for example,  that you are working for Uber Eats, which wants to determine  whether email\ncampaigns  will increase  its product's  conversion  rates. To conduct  an appropriate  hypothesis  test, you\nwould need two roughly equal groups (equal with respect to dimensions  like age, gender, location,\netc.). One group would receive the email campaigns  and the other group would not be exposed.  The\nnull hypothesis  in this case would be that the two groups exhibit  equal conversion  rates, and the hope\nis that the null hypothesis  would be rejected.\nTest Statistics\nA test statistic is a numerical  summary  designed  for the purpose  of determining  whether  the null\nhypothesis  or the alternative  hypothesis  should  be accepted  as correct. More specifically,  it assumes\nthat the parameter  of interest  follows  a particular  sampling  distribution  under the null hypothesis.\nFor example,  the number  of heads in a series of coin flips may be distributed  as a binomial  distribution,\nbut with a large enough sample size, the sampling  distribution  should be approximately  normally\ndistributed.  Hence, the sampling  distribution  for the total number  of heads in a large series of coin\nflips would  be considered  normally  distributed.\nSeveral  variations  in test statistics  and their distributions  include:\n1, Z-test: assumes  the test statistic  follows  a normal  distribution  under the null hypothesis\n2. t-test: uses a student’s  t-distribution  rather than a normal  distribution\n3. Chi-squared:  used to assess goodness  of fit, and to check whether  two categorical  variables  are\nindependent\nZ-Test\nGenerally  the Z-test is used when the sample  size is large (to invoke  the CLT) or when the population\nvariance  1s known,  and a (-test is used when the sample  size is small and when the population  variance\nis unknown.  The Z-test for a population  mean is formulated  as:\nX — fly\no/vn\nin the case where the population  variance  o? is known.—— ~ N(0,1)\nt-Test\nThe /-test 1s structured  similarly  to the Z-test, but uses the sample  variance  s? in place of population\nvariance.  The ¢-test is parametrized  by the degrees of freedom,  which refers to the number  of\nindependent  observations  in a dataset,  denoted  below  by n—1:\nX — [lot-te  7s/Vn nl\nwhere s° — —“-1-_. _\nn- |\n56 Ace the Data Science  Interview  | Statistics"
  },
  {
    "page_number": 69,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nAs stated earlier,  the t-distribution  is similar  to the normal  distribution  in appearance  but has larger\ntails (i.e., extreme  events  happen  with greater  frequency  than the modeled  distribution  would  predict),\na common  phenomenon,  particularly  in economics  and Earth sciences.\nChi-Squared  Test\nThe Chi-squared  test statistic  is used to assess goodness  of fit, and is calculated  as follows:\nx? = yO —By\ni i\nwhere O, is the observed  value of interest  and E, is its expected  value. A Chi-squared  test statistic\ntakes on a particular  number  of degrees  of freedom,  which is based on the number  of categories  in\nthe distribution.\nTo use the squared  test to check whether  two categorical  variables  are independent,  create a table of\ncounts  (called  a contingency  table),  with the values  of one variable  forming  the rows of the table and\nthe values of the other variable  forming  its columns,  and check for intersections.  It uses the same\nstyle of Chi-squared  test statistic  as given above.\nHypothesis  Testing  for Population  Proportions\nNote that, due to the CLT, the Z-test can be applied to random  variables  of any distribution.  For\nexample,  when estimating  the sample  proportion  of a population  having  a characteristic  of interest,\nwe can view the members  of the population  as Bernoulli  random  variables,  with those having the\ncharacteristic  represented  by “Is” and those lacking it represented  by ‘Os’. Viewing  the sample\nproportion  of interest  as the sum of these Bernoulli  random  variables  divided  by the total population\nsize, we can then compute  the sample  mean and variance  of the overall proportion,  about which we\ncan form the following  set of hypotheses:\nH,: P = Py versus H, : p + py\nP— Py\nPo (1 — Po )/n\nIn practice,  these test statistics  form the core of A/B testing. For instance,  consider  the previously\ndiscussed  case, in which we seek to measure  conversion  rates within groups  A and B, where  A is the\ncontrol group and B has the special treatment  (in this case, a marketing  campaign).  Adopting  the\nsame null hypothesis  as before, we can proceed  to use a Z-test to assess the difference  in empincal\npopulation  means (in this case, conversion  rates) and test its statistical  significance  at a predeterminedand the corresponding  test statistic  to conduct  a Z-test would be: 2 =\nlevel.\nWhen asked about A/B testing or related topics, you should always cite the relevant  test statistic  and\nthe cause of its validity  (usually  the CLT).\np-values  and Confidence  Intervals\nBoth p-values  and confidence  intervals  are commonly  covered topics during interviews.  Put simply,\na p-value is the probability  of observing  the value of the calculated  test statistic under the null\nhypothesis  assumptions.  Usually, the p-value is assessed relative to some predetermined  level of\nsignificance  (0.05 is often chosen).\nAce the Data Science  Interview 57"
  },
  {
    "page_number": 70,
    "content": "CHAPTER  6: STATISTICS\nIn conducting  a hypothesis  test, an a, or measure  of the acceptable  probability  of rejecting  a true null\nhypothesis,  is typically  chosen prior to conducting  the test. Then, a confidence  interval can also be\ncalculated  to assess the test statistic.  This is a range of values that, if a large sample were taken, would\ncontain the parameter  value of interest (1-a.)% of the time. For instance,  a 95% confidence  interval!\nwould contain the true value 95% of the time. If 0 is included  in the confidence  intervals,  then we\ncannot  reject the null hypothesis  (and vice versa).\nThe general form for a confidence  interval around the population  mean looks like the following,\nwhere the term is the critical value (for the standard  normal distribution):\n0bE 2a\nIn the prior example  with the A/B testing on conversion  rates, we see that the confidence  interval  for\na population  proportion  would be\nDp + zans[ PO?\nsince our estimate of the true proportion  will have the following  parameters  when estimated  as\napproximately  Gaussian:\nmp _ 2__mp(l—p)  _ p(l—p)\nwe Oe? ) —_\nn n* n\nAs long as the sampling  distribution  of a random variable  is known, the appropriate  p-values  and\nconfidence  intervals  can be assessed.\nKnowing  how to explain  p-values  and confidence  intervals,  in technical  and nontechnical  terms, is\nvery useful during  interviews,  so be sure to practice  these. If asked about the technical  details,  always\nremember  to make sure you correctly  identify  the mean and variance  at hand.\nType | and Il Errors\nThere are two errors that are frequently  assessed:  type I error, which is also known as a “false\npositive.”  and type II error, which is also known as a “false negative.”  Specifically,  a type I error is\nwhen one rejects  the null hypothesis  when it is correct,  and a type II error is when the null hypothesis\nis not reyected  when it is incorrect.\nUsually |-a is referred  to as the confidence  level, whereas  1-B is referred  to as the power. If you plot\nsample  size versus power, generally  you should see a larger sample size corresponding  to a larger\npower. It can be useful to look at power in order to gauge the sample  size needed for detecting  a\nsignificant  effect. Generally,  tests are set up in such a way as to have both |-a and 1-f relatively  high\n(say at 0.95 and 0.8, respectively).\nIn testing  multiple  hypotheses,  it is possible  that if you ran many experiments  -- even if a particular\noutcome  for one experiment  is very unlikely  —- you would see a statistically  significant  outcome  at\nleast once. So, for example,  if you set a = 0.05 and run 100 hypothesis  tests, then by pure chance  you\nwould expect 5 of the tests to be statistically  significant.  However,  a more desirable  outcome  is to\nhave the overall a of the 100 tests be 0.05, and this can be done by setting  the new a to a/#, where 7\nis the number  of hypothesis  tests (in this case, a/” = 0.05/100  = 0.0005).  This is known  as Bonferroni\ncorrection,  and using it helps make sure that the overall  rate of false positives  is controlled  within a\nmultiple  testing  framework.\n58 Ace the Data Science  Interview | Statistics"
  },
  {
    "page_number": 71,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nGenerally,  most interview  questions  concerning  Type J and II errors are qualitative  in nature — for\ninstance,  requesting  explanations  of terms or of how you would  go about assessing  errors/power  in\nan experimental  setup.\nMLE and MAP\nAny probability  distribution  has parameters,  so fitting  parameters  is an extremely  crucial  part of data\nanalysis.  There are two general  methods  for doing so. In maximum  likelihood  estimation  (MLE),  the\ngoal is to estimate  the most likely parameters  given  a likelihood  function:  6\nL(9) =f (x,...x |6).\nSince the values of X are assumed  to be i.i.d., then the likelihood  function  becomes  the following:\nL(@) =] f(x:10)\nThe natural  log of L(8) is then taken prior to calculating  the maximum;  since log is a monotonically\nincreasing  function, maximizing  the log-likelihood  log L(®) is equivalent  to maximizing  the\nlikelihood:wie = arg max L(O), where\nlog L(6) = 3 log f (x, 18)\nAnother  way of fitting parameters  is through maximum  a posteriori  estimation  (MAP), which\nassumes  a “prior  distribution:”\n0 ip = arg max g(0) flx,...x |B)\nwhere the similar  log-likelihood  1s again employed,  and g(8) is a density  function  of 0.\nBoth MLE and MAP are especially  relevant  in statistics  and machine  learning,  and knowing  these\nis recommended,  especially  for more technical  interviews.  For instance, a common  question  in such\ninterviews  is to derive the MLE for a particular  probability  distribution.  Thus, understanding  the\nabove steps, along with the details of the relevant  probability  distributions,  1s crucial.\n4O Real Statistics  Interview  Questions\nEasy\n6.1. Uber: Explain  the Central Limit Theorem.  Why it is useful?\n6.2. Facebook:  How would you explain a confidence  interval  to a non-technical  audience?\n6.3. Twitter:  What are some common  pitfalls  encountered  in A/B testing?\n6.4. Lyft: Explain  both covariance  and correlation  formulaically,  and compare  and contrast  them.\n6.5. Facebook:  Say you flip a coin 10 times and observe  only one heads. What would be your null\nhypothesis  and p-value  for testing whether  the coin is fair or not?\n6.6. Uber: Describe  hypothesis  testing and p-values  in layman’s  terms?\n6.7. Groupon:  Describe  what Type | and Type I] errors are, and the trade-offs  between  them.\n6.8. Microsoft:  Explain  the statistical  background  behind power.\n6.9. Facebook:  What is a Z-test and when would you use it versus  a t-test?\nAce the Data Science  Interview 59"
  },
  {
    "page_number": 72,
    "content": "6.10.CHAPTER  6: STATISTICS\nAmazon:  Say you are testing hundreds  of hypotheses,  each with t-test. What considerations\nwould you take into account  when doing this?\nMedium\n6.11.Google:  How would you derive a confidence  interval  for the probability  of flipping  heads from\na series of coin tosses?\n6.12. Two Sigma: What is the expected  number  of coin flips needed to get two consecutive  heads?\n6.13. Citadel:  What is the expected  number  of rolls needed  to see all six sides of a fair die?\n6.14. Akuna Capital: Say you’re rolling a fair six-sided  die. What is the expected  number  of rolls\nuntil you roll two consecutive  5s?\n6.15. D.E. Shaw: A coin was flipped 1,000 times, and 550 times it showed  heads. Do you think the\ncoin is biased?  Why or why not?\n6.16. Quora: You are drawing  from a normally  distributed  random  variable  X ~ N(0O, 1) once a day.\nWhat is the approximate  expected  number  of days until you get a value greater  than 2?\n6.17. Akuna Capital: Say you have two random  variables  X and Y, each with a standard  deviation.\nWhat is the variance  of aX + bY for constants  a and 5b?\n6.18. Google:  Say we have X ~ Uniform(0,  1) and ¥ ~ Uniform(0,  1) and the two are independent.\nWhat is the expected  value of the minimum  of X and Y?\n6.19. Morgan Stanley: Say you have an unfair coin which lands on heads 60% of the time. How\nmany coin flips are needed  to detect that the coin 1s unfair?\n6.20. Uber: Say you have ” numbers |...2, and you uniformly  sample from this distribution  with\nreplacement  » times. What is the expected  number  of distinct  values  you would  draw?\n6.21. Goldman  Sachs: There are 100 noodles in a bowl. At each step, you randomly  select two\nnoodle ends from the bowl and tie them together.  What is the expectation  on the number  of\nloops formed?\n6.22. Morgan  Stanley:  What is the expected  value of the max of two dice rolls?\n6.23, Lyft: Derive  the mean and variance  of the uniform  distribution  U(a, b).\n6.24, Citadel:  How many cards would you expect to draw from a standard  deck before seeing the\nfirst ace?\n6.25. Spotity: Say you draw n samples  from a uniform  distribution  U(a, 6). What are the MLE\nestimates  of a and b?\nHard\n6.26. Google: Assume you are drawing from an infinite set of i.i.d random variables  that are\nuniformly  distributed  from (0, 1). You keep drawing  as long as the sequence  you are getting  is\nmonotonically  increasing.  What is the expected  length of the sequence  you draw?\n6.27 Facebook:  There are two games  involving  dice that you can play. In the first game,  you roll two\ndice at once and receive  a dollar amount  equivalent  to the product  of the rolls. In the second\ngame, you roll one die and get the dollar  amount  equivalent  to the square  of that value. Which\nhas the higher  expected  value and why?\n60 Ace the Data Science  Interview  | Statistics"
  },
  {
    "page_number": 73,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\n6.28. Google:  What does it mean for an estimator  to be unbiased?  What about consistent?  Give\nexamples  of an unbiased  but not consistent  estimator,  and a biased  but consistent  estimator.\n6.29. Netflix:  What are MLE and MAP?  What is the difference  between  the two?\n6.30. Uber: Say you are given a random  Bernoulli  trial generator.  How would you generate  values\nfrom a standard  normal  distribution?\n6.31. Facebook:  Derive  the expectation  for a geometric  random  variable.\n6.32. Goldman  Sachs: Say we have a random  variable  X ~ D, where  D is an arbitrary  distribution.\nWhat is the distribution  F(X) where  F is the CDF of X?\n6.33. Morgan  Stanley:  Describe  what a moment  generating  function  (MGF) is. Derive  the MGF for\na normally  distributed  random  variable  X.\n6.34. Tesla: Say you have N independent  and identically  distributed  draws  of an exponential  random\nvariable.  What is the best estimator  for the parameter  A?\n6.35. Citadel:  Assume  that log X ~ N(0, 1). What is the expectation  of X?\n6.36. Google:  Say you have two distinct  subsets  of a dataset  for which you know their means and\nstandard  deviations.  How do you calculate  the blended  mean and standard  deviation  of the\ntotal dataset?  Can you extend  it to K subsets?\n6.37. Two Sigma: Say we have two random  variables  X and Y. What does it mean for _X and Y to be\nindependent?  What about uncorrelated?  Give an example  where X and Y are uncorrelated  but\nnot independent.\n6.38. Citadel:  Say we have X ~ Uniform(-—1,  1) and Y= X%2. What is the covariance  of X and Y?\n6.39. Lyft: How do you uniformly  sample  points at random  from  a circle with radius R?\n6.40. Two Sigma: Say you continually  sample  from some i.1.d. uniformly  distributed  (0, 1) random\nvariables  until the sum of the variables  exceeds  |. How many samples  do you expect to make?\n4O Real Statistics  Interview  Solutions\nSolution  #6.1\nThe Central Limit Theorem  (CLT) states that if any random variable, regardless  of distribution,\nis sampled a large enough number of times, the sample mean will be approximately  normally\ndistributed.  This allows for studying  of the properties  for any statistical  distribution  as long as there\nis a large enough  sample  size.\nThe mathematical  definition  of the CLT is as follows: for any given random variable X, as n\napproaches  infinity,\n2\nO\njl, -—\nhno~ N\nAt any company  with a lot of data, like Uber, this concept is core to the various experimentation\nplatforms  used in the product. For a real-world  example,  consider  testing whether  adding a new feature\nincreases rides booked in the Uber platform,  where each X is an individual  ride and is a Bernoulli\nrandom variable  (i.e., the rider books or does not book  a ride). Then, if the sample size is sufficiently\nlarge, we can assess the statistical  properties  of the total number  of bookings,  as well as the booking  rate\n(rides booked / rides opened on app). These statistical  properties  play a key role in hypothesis  testing,\nallowing  companies  like Uber to decide whether  or not to add new features in a data-driven  manner.\nAce the Data Science  Interview 61"
  },
  {
    "page_number": 74,
    "content": "CHAPTER  6: STATISTICS\nSolution  #6.2\nSuppose we want to estimate some parameters  of a population.  For example,  we might want to\nestimate  the average  height of males in the U.S. Given some data from a sample,  we can compute  a\nsample mean for what we think the value is, as well as a range of values around that mean. Following\nthe previous  example,  we could obtain the heights of 1,000 random males in the U.S. and compute\nthe average height, or the sample mean. This sample mean is a type of point estimate and, while\nuseful, will vary from sample to sample. Thus, we can’t tell anything  about the variation  in the data\naround this estimate,  which is why we need a range of values through  a confidence  interval.\nConfidence  intervals  are a range of values with a lower and an upper bound such that if you were to\nsample  the parameter  of interest  a large number  of times, the 95% confidence  interval  would contain\nthe true value of this parameter  95% of the time. We can construct  a confidence  interval using the\nsample standard  deviation  and sample mean. The level of confidence  is determined  by a margin of\nerror that is set beforehand.  The narrower  the confidence  interval,  the more precise  the estimate,  since\nthere is less uncertainty  associated  with the point estimate  of the mean.\nSolution  #6.3\nA/B testing has many possible  pitfalls  that depend  on the particular  experiment  and setup employed.\nOne common  drawback  is that groups may not be balanced,  possibly  resulting  in highly skewed\nresults. Note that balance is needed for all dimensions  of the groups — like user demographics  or\ndevice used ___- because,  otherwise,  the potentially  statistically  significant  results from the test may\nsimply be due to specific factors that were not controlled  for. Two types of errors are frequently\nassessed:  Type I error, which 1s also known as a “false positive,”  and Type II error, also known as\na “false negative.”  Specifically,  Type | error is rejecting  a null hypothesis  when that hypothesis  1s\ncorrect,  whereas  Type II error ts failing to reyect a null hypothesis  when its alternative  hypothesis  is\ncorrect.\nAnother common pitfall is not running an experiment  for long enough. Generally  speaking,\nexperiments  are run with a particular  power threshold  and significance  threshold;  however,  they\noften do not stop immediately  upon detecting  an effect. For an extreme  example,  assume  you’re at\neither Uber or Lyft and running  a test for two days, when the metric  of interest  (e.g., rides booked)  is\nsubject  to weekly  seasonality.\nLastly, dealing  with multiple  tests is important  because  there may be interactions  between  results of\ntests you are running  and so attributing  results  may be difficult.  In addition,  as the number  of variations\nyou run increases,  so does the sample  size needed.  In practice,  while it seems technically  feasible  to\ntest 1,000 variations  of a button when optimizing  for click-through  rate, variations  in tests are usually\nbased on some intuitive  hypothesis  concerning  core behavior.\nSolution  #6.4\nFor any given random  variables  X and Y, the covariance,  a linear measure  of relationship,  is defined\nby the following:  Cou(X, Y) = E[(X — E[X])(Y  - E[Y])] = E[XY] - ELXJE[Y]\nSpecifically,  covariance  indicates  the direction  of the linear relationship  between  X and Y and can\ntake on any potential  value from negative  infinity  to infinity.  The units of covariance  are based on the\nunits of X and Y, which may differ.\n62 Ace the Data Science  Interview | Statistics"
  },
  {
    "page_number": 75,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nThe correlation  between_X  and Y is the normalized  version  of covariance  that takes into account  the\nvariances  of X and Y:\nCov(X,Y)\nVVar(X) Var (Y)\nSince correlation  results from scaling covariance,  it is dimensionless  (unlike covariance)  and is\nalways  between  -1 and 1 (also unlike covariance).\nSolution  #6.5\nThe null hypothesis  is that the coin is fair, and the alternative  hypothesis  is that the coin is biased\ntowards  tails (note this is a one-sided  test):\nH,: p, = 9.5, H,: p,< 0.5\nNote that, since the sample  size here is 10, you cannot  apply the Central  Limit Theorem  (and so you\ncannot  approximate  a binomial  using a normal  distribution).o(X,Y)=\nThe p-value  here is the probability  of observing  the results  obtained  given that the null hypothesis  is\ntrue, 1.e., under the assumption  that the coin is fair. In total for 10 flips of a coin, there are 210 = 1024\npossible  outcomes,  and in only 10 of them are there 9 tails and one heads. Hence,  the exact probability\nof the given result is the p-value,  which is a — 0.0098. Therefore,  with a significance  level set, for\nexample,  at 0.05, we can reject the null hypothesis.\nSolution  #6.6\nThe process  of testing whether  data supports  particular  hypotheses  is called hypothesis  testing and\ninvolves  measuring  parameters  of a population’s  probability  distribution.  This process typically\nemploys  at least two groups  — one a control  that receives  no treatment,  and the other group(s),  which\ndo receive the treatment(s)  of interest. Examples  could be the height of two groups of people, the\nconversion  rates for particular  user flows in a product,  etc. Testing  also involves  two hypotheses  —\nthe null hypothesis,  which assumes  no significant  difference  between  the groups,  and the alternative\nhypothesis,  which assumes  a significant  difference  in the measured  parameter(s)  as a consequence  of\nthe treatment.\nA p-value  is the probability  of observing  the given test results under the null hypothesis  assumptions.\nThe lower this probability,  the higher the chance that the null hypothesis  should be rejected. If the\np-value is lower than the predetermined  significance  level a, generally  set at 0.05, then it indicates\nthat the null hypothesis  should be rejected  in favor of the alternative  hypothesis.  Otherwise,  the null\nhypothesis  cannot  be rejected,  and it cannot  be concluded  that the treatment  has any significant  effect.\nSolution  #6.7\nBoth errors are relevant in the context of hypothesis  testing. Type I error is when one rejects the\nnull hypothesis  when it is correct, and is known as a false positive.  Type II error is when the null\nhypothesis  is not rejected  when the alternative  hypothesis  is correct;  this is known as a false negative.\nIn layman’s  terms, a type I error is when we detect a difference,  when in reality there is no significant\ndifference  in an experiment.  Similarly,  a type I] error occurs when we fail to detect a difference,  when\nin reality there is a significant  difference  in an experiment.\nType | error is given by the level of significance  a, whereas the type II error is given by B. Usually,\n1-a is referred  to as the confidence  level, whereas 1-f is referred  to as the statistical  power of the test\nbeing conducted.  Note that, in any well-conducted  statistical  procedure,  we want to have both a and\nB be small. However,  based on the definition  of the two, it 1s impossible  to make both errors small\nAce the Data Science  Interview 63"
  },
  {
    "page_number": 76,
    "content": "CHAPTER  6: STATISTICS\nsimultaneously:  the larger a is, the smaller B is. Based on the experiment  and the relative  importance\nof false positives  and false negatives,  a data scientist  must decide what thresholds  to adopt for any\ngiven experiment.  Note that experiments  are set up so as to have both |-a and 1-6 relatively  high (say\nat .95, and .8, respectively).\nSolution  #6.8\nPower  is the probability  of rejecting  the null hypothesis  when, in fact, it is false. It is also the probability\nof avoiding  a Type II error. A Type II error occurs when the null hypothesis  is not rejected  when the\nalternative  hypothesis  is correct.  This is important  because  we want to detect significant  effects during\nexperiments.  That is, the higher  the statistical  power  of the test, the higher  the probability  of detecting  a\ngenuine  effect (i.e, accepting  the alternative  hypothesis  and rejecting  the null hypothesis).  A minimum\nsample size can be calculated  for any given level of power  — for example,  say a power level of 0.8.\nAn analysis  of the statistical  power of a test is usually performed  with respect to the test’s level of\nsignificance  (a) and effect size (ie., the magnitude  of the results).\nSolution  #6.9\nIn a Z-test, your test statistic  follows  a normal distribution  under the null hypothesis.  Alternatively,\nin a t-test, you employ  a student’s  t-distribution  rather than a normal distribution  as your sampling\ndistribution.\nConsidering  the population  mean, we can use either Z-test or t-test only 1f the mean is normally\ndistributed,  which is possible in two cases: the initial population  is normally  distributed,  or the\nsample  size is large enough  (# 2 30) that we can apply the Central  Limit Theorem.\nIf the condition  above 1s satisfied,  then we need to decide  which type of test 1s more appropriate  to\nuse. In general,  we use Z-tests  if the population  variation  is known,  and vice versa: we use /-test if the\npopulation  variation  1s unknown.\nAdditionally,  if the sample size is very large (n > 200), we can use the Z-test in any case, since for\nsuch large degrees  of freedom,  ¢-distribution  coincides  with z-distribution  up to thousands.\nConsidering  the population  proportion,  we can use a Z-test  (but not f-test)  where  np, 2 10 and n(1 —p,)2\n10, 1.e., when each of the number  of successes  and the number  of failures  is at least 10.\nSolution  #6.10\nThe primary  consideration  is that, as the number  of tests increases,  the chance that a stand-alone\np-value  for any of the t-tests is statistically  significant  becomes  very high due to chance  alone. As\nan example,  with 100 tests performed  and a significance  threshold  of a = 0.05, you would expect\nfive of the experiments  to be statistically  significant  due only to chance. That is, you have a very\nhigh probability  of observing  at least one significant  outcome.  Therefore,  the chance  of incorrectly\nrejecting  a null hypothesis  (i.e., committing  Type I error) increases.\nTo correct for this effect, we can use a method  called the Bonferroni  correction,  wherein  we set\nthe significance  threshold  to a/m, where m is the number  of tests being performed.  In the above\nscenario  with 100 tests, we can set the significance  threshold  to instead  be 0.05/100  = 0.0005.  While\nthis correction  helps to protect from Type I error, it is still prone to Type Hf error (ie., failing to\nreject the null hypothesis  when it should  be rejected).  In general,  the Bonferroni  correction  is mosily\nuseful when there is a smaller  number  of multiple  comparisons  of which a few are significant.  If  the\nnumber  of tests becomes  sufficiently  high such that many tests yield statistically  sigmificant  results,\nthe number  of Type II errors may also increase  significantly.\n64 Ace the Data Science  Interview | Statistics"
  },
  {
    "page_number": 77,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nSolution  #6.1]\nThe confidence  interval  (CI) for a population  proportion  is an interval  that includes  a true population\nproportion  with a certain  degree  of confidence  1 — a.\nFor the case of flipping  heads from  a series of coin tosses, the proportion  follows the binomial\ndistribution.  If the series size is large enough  (each of the number  of successes  and the number  of\nfailures  is at least 10), we can utilize the Central  Limit Theorem  and use the normal  approximation\nfor the binomial  distribution  as follows:\nwhere  p is the proportion  of heads tossed in series, and » is the series size. The Cl is centered  at the\nseries proportion,  and plus or minus a margin  of error:\npez (PA Db)\nn\nwhere z,,, is the appropriate  value from the standard  normal  distribution  for the desired  confidence\nlevel.\nFor example,  for the most commonly  used level of confidence,  95%, z, ,, = 1.96.\nSolution  #6.12\nLet X be the number  of coin flips needed  to obtain two consecutive  heads. We then want to solve for\nE[X]. Let H denote  a flip that results in heads, and T denote  a flip that results in tails. Note that ELX]\ncan be written  in terms of ELX/H] and E[X]T],  1.e., the expected  number  of flips needed,  conditioned\non a flip being either  heads or tails, respectively.\n, . i 1Conditioning  on the first flip, we have: E[X] = 5 + E{X| H]) + 5 + E{X| T))\nNote that E[X]T] = ELX] since if a tail is flipped,  we need to start over in getting  two heads in a row.\nTo solve for ELX|H],  we can condition  it further  on the next outcome:  either heads (HH) or tails (HT).\nTherefore,  we have: E[X|H] = (1 + EL_X|HA))  + (1 + E[X|HT))\nNote that if the result is HH, then ELX]HH]  = 0, since the outcome  has been achieved.  If a tail was\nflipped,  then ELX|HT]  = ELX], and we need to start over in attempting  to get two heads in a row. Thus:\n1 1E|X | H] =-(1 + 0) +501 + E[X}) =1 + GAIA]\nPlugging  this into the original  equation  yields:\n1 1 ]pix)=*i+1+5a(xi]+  204+ 21x)\nand after solving  we get: EX] = 6. Therefore,  we would expect 6 flips.\nSolution  #6.13\nLet k denote the number of distinct sides seen from rolls. The first roll will always result in a new\nside being seen. If you have seen k sides. where  & < 6, then the probability  of rolling an unseen value\nwill be (6 — &)/6, since there are 6 — k values you have not seen, and 6 possible  outcomes  of each roll.\nAce the Data Science  Interview 65"
  },
  {
    "page_number": 78,
    "content": "CHAPTER  6: STATISTICS\nNote that each roll is independent  of previous  rolls. Therefore,  for the second roll (k = 1), the time\nuntil a side not seen appears has a geometric  distribution  with p = 5/6, since there are five of the six\nsides left to be seen. Likewise,  after two sides (k = 2), the time taken is a geometric  distribution,  with\np= 4/6. This continues  until all sides have been seen.\nRecall that the mean for a geometric  distribution  is given by 1/p, and let X be the number of rolls\nneeded  to show all six sides. Then, we have the following:\n6 6 6 6 6 2.1E(x|=14+—4+-—4+-—4+-—+-=6)  — =14.7  rolls\nLx) 5 4 3 2 1 >\nSolution  #6.14\nSimilar  in methodology  to question  13, let XY be the number  of rolls until two consecutive  fives. Let\nY denote  the event that a five was just rolled.\nConditioning  on Y, we know that either we just rolled a five, so we only have one more five to roll,\nor we rolled some other number  and now need to start over after having  rolled once:\nBUX] = 2(1 + BIXI¥) + me + E[X)\nNote that we have the following:  E[X| Y] = (1) + (1 + E[X])\nPlugging  the results in yields an expected  value of 42 rolls: E LX] = 42\nSolution  #6.15\nBecause  the sample size of flips is large (1,000),  we can apply the Central Limit Theorem.  Since\neach individual  flip is a Bernoulli  random  variable,  we can assume  that p is the probability  of getting\nheads. We want to test whether  p is .5 (1.e., whether  it is a fair coin or not). The Central Limit\nTheorem  allows us to approximate  the total number  of heads seen as being normally  distributed.\nMore specifically,  the number  of heads seen out of 7 total rolls follows  a binomial  distribution  since\njt a sum of Bernoulli  random  variables.  If the coin is not biased  (p = .5), then the expected  number  of\nheads is as follows:  1 = np = 1000 * 0.5 = 500, and the variance  of the number  of heads is given by:\no”? = np(1 — p) = 1000 « 0.5 * 0.5 = 250, o = V250 = 16\nSince this mean and standard  deviation  specify the normal distribution,  we can calculate  the\ncorresponding  z-score  for 550 heads as follows:\n_ 550 — 500\n16\nThis means that, if the coin were fair, the event of seeing 550 heads should occur with a < 0.1%\nchance  under  normality  assumptions.  Therefore,  the coin is likely biased.\nSolution  #6.16\nSince X is normally  distributed.  we can employ  the cumulative  distribution  function  (CDF) of the\nnormal  distribution:  @(2) = P(X < 2) = P(X < + 20) = 0.9772= 3.16\nTherefore,  P(X > 2) = 1 - 0.977 = 0.023 for any given day. Since each day’s draws are independent,\nthe expected  time unul drawing  an X > 2 follows  a geometric  distribution,  with p = 0.023. Letting\nbe a random  variable  denoting  the number  of days, we have the following:\n66 Ace the Data Science  Interview  | Statistics"
  },
  {
    "page_number": 79,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\n1 1\nE(T| =1 = ~44d7] p .02272 ays\nSolution  #6.17\nLet the variances  for X and Y be denoted  by Var (X) and Var (Y).\nThen, recalling  that the variance  of a sum of variables  is expressed  as follows:\nVar(X  + Y) = Var(X)  + Var(Y)  + 2Cov(X,  Y)\nand that a constant  coefficient  of a random  variable  is assessed  as follows:  Var(aX)  = a’?Var(X)\nWe have Var(aX  + bY) = a?Var(X)  + b?Var(Y)  + 2abCou(X,  Y), which would  provide  the bounds\non the designated  variance;  the range will depend  on the covariance  between  X and Y.\nSolution  #6.18\nLet Z=min(X,  Y). Then we know the following:  P(Z < z) = P(min(X,  Y) < z)=1-P(X>z,  Y> z)\nFor a uniform  distribution,  the following  is true for a value of z between  0 and 1:\nP(X >z)=1-zand  P(Y>z)=1-2z\nSince X and ¥ are i.id., this yields:  P(Z < z) =1- P(X>z,  Y>z)=1-(1-2z)\nNow we have the cumulative  distribution  function  for z. We can get the probability  density  function\nby taking the derivative  of the CDF to obtain the following:  fz(z) = 2(1 — z). Then, solving  for the\nexpected  value by taking  the integral  yields the following:\n: 1 1) 1E\\Z]= { 2fz(z)dz  =2f z(1—z)dz=  22 -4)= =\n0 0 2 3) 38\nTherefore,  the expected  value for the minimum  of X and  Y is 1/3.\nSolution  #6.19\nSay we flip the unfair  coin 7 times. Each flip is a Bernoulli  trial with a success  probability  of p:\nXj, X,, ...%,, x, ~ Ber(p)\nWe can construct  a confidence  interval for p as follows,  using the Central Limit Theorem.  First,\nwe decide on our level of confidence.  If we select a 95% confidence  level, the necessary  z-score  1s\nz= 1.96. We then construct  a 95% confidence  interval  for p. If it does not include  0.5 as its lower\nbound,  then we can reject the null hypothesis  that the coin is fair.\nSince the trials are 1.i.d., we can compute  the sample  mean for p from a large number  of trials:\n~ 1d\nn 1-1\na)__ nn A 1- 1-We know the following  properties hold: E|p|= — = pand Var=(p)=  pt P) = pi ~ P)\n, 1—Therefore,  our 95% confidence  interval is given by the following: p+ 2 ed_ P}\nSince the true p = 0.6, plugging  that in and setting the lower bound of the interval  equal to 0.5 yields:\n0.6 —1.96, [0-64 = 0.6) _ 0.5\nfh\nSolving  for n yields 93 flips.\nAce the Data Science  Interview 67"
  },
  {
    "page_number": 80,
    "content": "CHAPTER  6: STATISTICS\nSolution  #6.20\nLet the following  be an indicator  random  variable:  X, = | if i is drawn in n turns\nWe would then want to find the following:  SJ EIX]\nt=]\nWe know that p(X, = 1) = 1 — p(X, = 0), so the probability  of a number  not being drawn (where each\ndraw is independent)  is the following:\nn\nn—-1\np(X, =0)=n\nn\nand by linearity  of expectation,  we then have:\nSolution  #6.21\nSay that we have 7 noodles.  At any given step, we will have one of two outcomes:  (1) we pick two\nends from the same noodle  (which  makes  a Joop), or (2) we pick two ends from different  noodles.  Let\nX denote a random  variable  representing  the number  of loops with ” noodles  remaining.Therefore,  we have: p(X, =1)=1-  \" -\nn\nn—1SF (x |=n8 1%] nf ~\n4: . . nN 1The probability  of case (1) happening  1s: 7, = ———\n(2) \"Bn\nwhere the denominator  represents  the number  of ends we can choose from the noodles,  and the\nnumerator  represents  the number  of cases where we choose  the same noodle.\n1 2n-2\n2n-1 2n-1\nThen, taking  case (1) and (2), we have the following  recursive  formulation  for the expectation  of the\nnumber  of loops formed:Therefore,  the probability  of case (2) happening  is: 1 -\n1 2n—2E|X,|= + E|X,, ,|\n2n-1 2n-1\nPlugging  in E[X,] = 1 and calculating  the first few terms, we can notice the following  pattern,  for\nwhich we can plug in n = 100 to obtain the answer:\n1 1XmJal+s  bee. + E ~3.32(100)-1\nSolution  #6.22\nSince we only have two dice, let the maximum  value between  the two be m. Let\nX,, X,, Y= max (X,, X,)\ndenote  the first roll, second  roll, and the max of the two. Then we want to find the following:\n6\nElY]=  ix P(Y =1)\nto]\nWe can condition  ¥ = #7 on three cases: (1) die one is the max roll; (2) die two is the max roll: or (3)\nthey are both the same.\n68 Ace the Data Science  Interview  | Statistics"
  },
  {
    "page_number": 81,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nFor cases (1) and (2) we have: P(X, =i, X, < i) = P(X, =i, X,<i)=  - x!\n“For case (3), where  both dice are the maximum:”\nPX, =X, =i) ==\nPutting everything  together  yields the following: E[Y]= »y, i (2 .f D4 1, -] _ 161\n= 6 6/ 36\nA simpler  way to visualize  this is to use a contingency  table, such as the one below:\n| 2 3 4 5 6\n(1, 1) (1, 2) (1, 3) (1, 4) (1, 5) (1, 6)\n(2, 1) (2, 2) (2, 3) (2, 4) (2, 5) (2, 6)\n(3, 1) (3, 2) (3, 3) (3, 4) (3, 5) (3, 6)\n(4, 1) (4, 2) (4, 3) (4, 4) (4, 5) (4, 6)\n(5, 1) (5, 2) (5, 3) (5, 4) (5, 5) (5, 6)\n(6, 1) (6, 2) (6, 3) (6, 4) (6, 5) (6, 6)\nThen the expectation  is simply  given by:\nElYJ=1x242x343x244x  145% 3% 46x tt OW 45\n6 6 6 6 6 6 36\nSolution  #6.23\nFor X ~U(a, b), we have the following: f, (x)= —\n—a\nTherefore,  we can calculate  the mean as:\nx? al a+bx= —_—\nb-a 2(a—b) 2\nSimilarly,  the variance  can be as expressed  as follows:  Var(X)  = E[X”] — E[X}?E{X] =f xfy (x)dx = f\nGiving  us:\n2 2 2\nEIX*]=  [xf (ede = [oes  _@ tabs\nTherefore:  Var  (X) =a’ +ab’ +b\" -(224)  _(b-a)\n3 2 12\nSolution  #6.24\nAlthough  one can enumerate  all the probabilities,  this can get a bit messy from an algebraic  standpoint,\nso obtaining  the following  intuitive  answer is more preferable.  Imagine we have aces Al, A2, A3,\nA4. We can then draw  a line in between  them to represent  an arbitrary  number  (including  0) of cards\nbetween  each ace, with a line before the first ace and after the last.\nJAIA2|A3|A4|\nThere are 52 — 4 = 48 non-ace cards in a deck. Each of these cards is equally likely to be in any of\nthe five lines. Therefore,  there should be 48/5 = 9.6 cards drawn prior to the first ace being drawn.\nHence, the expected  number of cards drawn until the first ace 1s seen 1s 9.6 + It = 10.6 cards\ncan’t forget to add 1, because  we need to include drawing  the ace card itself.\nAce the Data Science  Interview"
  },
  {
    "page_number": 82,
    "content": "CHAPTER  6: STATISTICS\nSolution  #6.25\nNote that for a uniform  distribution,  the probability  density 1s ba for any value on the interval [a,\nb]. The likelihood  function  is therefore  as follows:\nf (x, ...5%, \\a,0)=(-*-)\nTo obtain the MLE, we maximize  this likelihood  function,  which is clearly maximized  if 5 is the\nlargest of the samples  and  a is the smallest  of the samples.  Therefore,  we have the following:\n@=min(x,  ..., x,), = b max(x, ..., X,)\nSolution  #6.26\nAssume that we have an indicator  random variable:  X, = 1 if the sequence  is increasing  up to ith\nelement,  and otherwise  X, = 0.\nThen, we calculate  the expectation:  ELY, + X, + ...]. Consider  some arbitrary  7. In order to draw up\nto element i, the entire sequence  up to i must be monotonically  increasing,  which means that the\nfollowing  is true: X, <X,<...<X,.  Given that there are 1 possible  sequences  of the elements,  there is a\n1\ni!\nchance of X. being 1. Since each X is i.i.d., we then have: E[X, + X,+...J=1+  31 +,..=e-1\nSolution  #6.27\nOne method of solving this problem is the brute force method,  which consists  of computing  the\nexpected  values by listing all of the outcomes  and associated  probabilities  and payoffs. However,\nthere exists an easier way of solving  the problem.\nAssume  that the outcome  of the roll of a die 1s given by a random  variable  X (meaning  that it takes on\nthe values |...6 with equal probability).  Then, the question  is equivalent  to asking,  ““What is E[X] *\nE[LX] = E{X}? (.e., the expected  value of the product  of two separate  rolls), versus ELX?] (the expected\nvalue of the square  of a single roll)?”\nRecall that the variance  of a given random  variable  X is as follows:\nVar(X)  = E[(X — E[X])*] = E[X\"] — 2E[X]  EX] + E[X}? = E[X?] — E[X)?\nTypically,  this variance  term 1s exactly the difference  between  the two sets of die rolls —- the two\n“games”  — (the payoff  of the second  game minus the payoff  of the first game). Since the left-hand\nside is positive,  as expected  tor the value of a squared  number,  then the right-hand  side is also positive.\nTherefore,  1t must be the case that the second  game has a higher  expected  value than the first.\nSolution  #6.28\nIn both cases, we are dealing  with an estimator  of the true parameter  value. An estimator  is unbiased\nif the expectation  of the estimator  is the true underlying  parameter  value. An estimator  is consistent\nif, as the sample size increases,  the estimator’s  sampling  distribution  converges  towards  the true\nparameter  value.\nConsider  the following  random  variable  X, which is normally  distributed,  and 7 i.i.d. samples  used\nto calculate  a sample  mean:\nX, + Xy...4+X,X ~ N(u, 0’) and x =\nn\n70 Ace the Data Science  Interview  | Statistics"
  },
  {
    "page_number": 83,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nThe first sample is an example  of an unbiased  but not consistent  estimator.  It is unbiased  since\nE[x,] = u. However,  it is not consistent  since, as the sample  size increases,  the sampling  distribution\nof the first sample  does not become  more concentrated  with respect  to the true mean.\nAn example  of a biased but consistent  estimator  is the sample variance: S? = 2 (x, —X )’\nn1=1\nIt can be shown that E| S? |= role\nn\nThe formal  proof  of the above is called Bessel’s  correction,  but there is an intuitive  way to grasp the\npresence  of the term preceding  the variance.  If we uniformly  sample  two numbers  randomly  from the\nseries of numbers  | to n, we have an n/n? = 1/n chance  that the two equal the same number,  meaning\nthe sampled  squared  difference  of the numbers  will be zero. The sample variance  will therefore\nslightly  underestimate  the true variance.  However,  this bias goes to 0 as n approaches  infinity,  since\nthe term in front of the variance,  (n-1/n),  approaches  |. Therefore,  the estimator  is consistent.\nSolution  #6.29\nMLE stands for maximum  likelihood  estimation,  and MAP for maximum  a posteriori.  Both are ways\nof estimating  variables  in a probability  distribution  by producing  a single estimate  of that variable.\nAssume  that we have  a likelihood  function  P(X| 6). Given n i.1.d. samples,  the MLE ts as follows:\nMLE (8) = max P(X |8)= max] | P(x, 18)\nSince the product  of multiple  numbers  all valued  between  0 and | might be very small, maximizing\nthe log function  of the product  above is more convenient.  This is an equivalent  problem,  since the log\nfunction  is monotonically  increasing.  Since the log of a product  is equivalent  to the sum of logs, the\nMLE becomes  the following:\nMLElog(0) = max)  log P(x, | 6)\nRelying  on Bayes rule, MAP uses the posterior  P(8,X) being proportional  to the likelihood  multiplied\nby a prior P(8), i.e., P(X18)P(8).  The MAP for 6 is thus the following:\nMAP (6) = max P(X |®) = max | | P(>, 16) P(6)\nEmploying  the same math as used in calculating  the MLE, the MAP becomes:\nMAP,,,  (6) = max 2 log P(x, |8)+ log P (6)\nTherefore,  the only difference  between the MLE and MAP is the inclusion  of the prior in MAP;\notherwise,  the two are identical.  Moreover,  MLE can be seen as a special case of the MAP with a\nuniform  prior.\nSolution  #6.30\nAssume we have n Bernoulli  trials, each with a p probability  of success. Altogether,  they form a\nbinomial  distribution:  x,, x,, ..., X,, X ~ B(n, p) where x, = 1 means success and x, = 0 means failure.\nAssuming  i.i.d. trials, we can compute  the sample proportion  for p as follows:\n._ I<p=7 2%\nAce the Data Science  Interview 71"
  },
  {
    "page_number": 84,
    "content": "CHAPTER  6: STATISTICS\nWe know that if 7 is large enough, then the binomial  distribution  approximates  the following  normal\ndistribution:\nA I- b- vf pPU=P))\nn\nwhere n must be np 2 10, n(1 — p) 2 10\nTherefore,  the value p can be used as simulation  for a normal distribution.  The sample size n must\nonly be large enough to satisfy the conditions  above (at least n = 20 for p = .5), but it 1s recommended\nto use a significantly  larger n to get the better normal  approximation.\np-p\np(i-p)\nya ~~Finally,  to simulate  the standard  normal distribution,  we normalize  p: py =\n“we”\nn\nAt this point, we can derive the final formula  for our normal random  generator:  x =\np(l- p)\nneeThe previous  expression  can be simplified  to the following: x = ==+~————\nnp(1-  p)\nwhere x,, ..., x, is the Bernoulli  series we get from the given random  generator,  with probability  of\nSUCCESS  P.\nSolution  #6.31\nWe are seeking  the expected  value of geometric  random variable  X as follows:  E[X]|=  > Pfs (k)\nk=1\nThe expression  above contains  a summation  instead of an integral  since k is a discrete  rather than\ncontinuous  random  variable,  and we know the probability  mass function  of the geometric  probability\ndistribution  is given by the following:  f,(k) = (1 — p)*~' p\nTherefore,  we obtain the expected  value of X as follows: E[X]=  »y k(1-p)  p\nIr=)\nSince p is constant  with respect to k, we separate  out p as follows: E[X]|=  p> k(I ~p)TM\nR=]\nNote that the term inside the summation  is really the following:\nDei py =D aap) + RO\nke] ko] h=2\nThis simplifies  to the following:\nYap)  '=[- Joa  -p)ip+(1-  p) /pt... =+(14+(1-p)+(1-p)'  +...\np\n1 1 1\np1-(l-p)  p’\nPlugging  this back into the equation  for the expected  value of X yields the following:\n, l 1E|X|= p*—--=—pp\n72 Ace the Data Science  Interview | Statistics"
  },
  {
    "page_number": 85,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nSolution  #6.32\nWe can define  a new variable  Y = F(X), and, hence,  we want to find the CDF of y (where  y 1s between\n0 and | by definition  of a CDF): FY) = P(Y < y)\nSubstituting  in for Y yields the following:  Fy) = PUY)  < y)\nApplying  the inverse  CDF on both sides yields the following:\nBy) = PU (F(X))  < F)) = PX < Fy)\nNote that the last expression  is simply  the CDF for: P(X < F(y))  = FU (y)) = y\nTherefore,  we have: F\\(y) = y\nSince y falls between  0 and 1, Y’s distribution  is simply  a uniform  one from  0 to 1, i.e., U(0, 1).\nSolution  #6.33\nA moment  generating  function  is the following  function  for a given random  variable:\nM,(s) = E[e*)\nIf X is continuous  (as in the case of normal  distributions),  then the function  becomes  the following:\nMy(s)=|_  ef, (x)dx\nHence,  the moment  generating  function  ts a function  for a given value of s. It is useful for calculating\nmoments,  since taking derivatives  of the moment  generating  function  and evaluating  at s = 0 yields\nthe desired  moment.\n1 _..\nFor a normal  distribution,  recall that: f, (x) = e?\nFirst, taking the special case of the standard  normal  random  variable,  we have the following:\ne2 fy (x)=\n27\nls—TtNVU esy\n; - 1 -ie? 1 ¢-Plugging  this into the above MGF yields: M, (s) =| e** Tan” 2 dx = rd e° dx\ns\ne2oa +9x-\"— = ] -” sCompleting  the square yields: M, (s) = ik 2 de =e? Fe) * dx=e?\nNote that the last step uses the fact that the expression  within the integral is a PDF for a normally\ndistributed  random  variable  with mean s and variance |, and hence the integral evaluates  to 1.\nTo solve for a general random variable,  you can plug in X = oY +, where Y is standard  normal\nvariable,  to yield: M(s) = e*, M,(so) = o(s0%/2) + sp\nSolution  #6.34\nDenote  the n i.i.d. draws as: x,, x,, .....%, where, for any individual  draw, we have the pdf: f(x) = Ae-AX,\nTherefore  the likelihood  of the data  is given by the following:\n=T] fy, (x )= 2\" exo - ay X, |\nAce the Data Science  Interview 73"
  },
  {
    "page_number": 86,
    "content": "CHAPTER  6: STATISTICS\nTaking the log of the equation  above to obtain the log-likelihood  results in the following:\nlog L(A; x, ...x,) = nlog (A) - > x,\nt=]\n, ; nrTaking the derivative  with respect to A and setting the results to 0 yields: 1 >> x, =0\ni=l\nTherefore,  the best estimate  of A is given by: 4 =n\nxX.\ni=)\nSolution  #6.35\nDefine Y = log X. We then want to solve for: E[e*] = E[X]\nRecall that a moment  generating  function  has the following  form: M,(s) = E[e*”]\nTherefore,  we want the moment  generating  function  for Y~ N(0, 1), which was derived  in problem\n33 and has the form: M,(s) = es7/2\nTherefore,  evaluating  at s = | (since we want the mean) gives: (1) = e’” which is the desired\nanswer.\nSolution  #6.36\nSay that the two have two distinct  group sizes: n, = size of group 1, and n, = size of group 2.\nGiven the means of two groups, [t, and ,t,, the blended mean can be found simply by taking a\nweighted  average:\nnw, + nop.11 20-2\nn, +n,i =\nWe know that the blended  standard  deviation  for the total data set has the form:\nny +No —\\25- jes (2, ~#)\nnm +N,\nwhere  2, is the union of the points from both groups.\nHowever,  since we are not given the initial data points from the two groups,  we have to rearrange  this\nformula  by using instead  the given variations  of these groups,  s; and s3, as follows:\ns= ns; + n,s; +n, (¥,-¥) +n, (¥,- 7)\nn, +n,\nApplying  the Bessel correction,  the blended  standard  deviation  for the two groups  is as follows:\nntn,-1\nK>. hE,\nKTo extend  the definition  above to subsets,  the mean is as follows: it =\nie]!\nAnd the standard  deviation  is: 8, =\nwhere n are the sizes of initial groups,  4. and s are their respective  means  and standard  deviations.\n74 Ace the Data Science  Interview  | Statistics"
  },
  {
    "page_number": 87,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nSolution  #6.37\nIndependence  is defined  as follows:  P(X = x, Y=y)=P(X=x)P(Y=y)  for all x, y. Equivalently,  we\ncan use the following  definitions:  P(X = x| Y=y)=P(X=x),  (PY=y|X=x)  =P(Y=y)\nWhen two random variables  X and Y are uncorrelated,  their covariance,  which is calculated  as\nfollows,  is 0: Cou(X, Y) = E[XY] - E[XJE[Y]\nFor an example  of uncorrelated  but not independent  variables,  let Y take on values —1, 0, or 1 with\nequal probability,  and let Y = 1 if X = 0 and Y = 0 otherwise.  Then we can verify that X and Y are\nuncorrelated:\n]E(XY)  = =(-1)(0)  += (0)(1)  += ()(0)  = 0\nAnd E{X] = 0, so the covariance  between  the two random  variables  is zero. However,  it is clear that\nthe two are not independent,  since we defined  Y in such a way that it obviously  depends  on_X.\nPY =y|X=x)  # P(Y=y)\nFor example,  P(Y=1|X=0)=1\nSolution  #6.38\nBy definition  of the covariance,  we have: Cou(X, Y) = Cou(X,  X”) = E[(X - E[X])(X?  - ELX?})]\nExpanding  terms of the equation  above yields: Cou(X, Y) = E{(X? — XE[X?]  - X?E|[X]  + ELXJELX?])]\nUsing linearity  of expectation,  we obtain: Cov(X, Y) = ELX?] - E[X]ELX?]  - E{(X?) E[X] + ELX]  EX?)\nSince the second  and last terms cancel one another,  we end up with the following:\nCov(X, Y) = E[X?] — EDCELX]\nHere, we conclude  that E[X] = 0 (based  on the definition  of X) and that E[X?] = 0 by evaluating  the\nprobability  density  function  of X as follows:\n1 _—fy (x) = —— = =\n| 1 1].Since we are evaluating  X from —1 to |, we then have: E|X*] ={ xf (x)dx =| pe ax =0\nThus, the covariance  between  X and  Y is 0.\nSolution  #6.39\nThis can be proved using the inverse-transform  method, whereby we sample from a uniform\ndistribution  and then simulate  the points on the circle employing  the inverse  cumulative  distribution\nfunctions  (i.e., inverse  CDFs).\nWe can define a random  point within the circle using a given radius value and an angle (and obtain\nthe corresponding  x, v values from polar coordinates).  To sample a random radius, consider  the\nfollowing.  [f we sample points from a radius r, we know that there are 2x7 points to consider  (1.e.,\nthe circumference  of the circle). Likewise,  if we sample  a radius 2r, there are 4ar points to consider.\nTherefore,  we have the following  probability  density  function  given by the following:\n_ 2r\nR? ;\n. r\nThis follows from the CDF, which is given by the ratio of the areas of the two circles: F, (r) = Refpr)\nAce the Data Science  Interview 75"
  },
  {
    "page_number": 88,
    "content": "CHAPTER  6: STATISTICS\nTherefore,  for the inverse sampling,  we want the following:  y = Ls\nThis simplifies  to the following:  JR’y =r\nTherefore,  we can sample Y ~ U(0, 1) and the corresponding  radius will be the following:\nr= RJ y\nFor the corresponding  angles, we can sample  theta uniformly  from the range 0 to 2x: @ € [0, 27] and\nthen set the following:  x = r cos(8), y = r sin(8)\nSolution  #6.40\nLet us define: N, = smallest  n such that: YU, >t for any value t between  0 and 1. Then we want\nto find: m(t) = E[N|] i=l\nConsider  the first draw. Assuming  that result is some value x, we then have two cases as follows.  The\nfirst is that x > t, in which  N, = 1\nThe second  is that x < t, necessitating  that we sample  again, yielding: VN =1+N,__.,\n1\nPutting  these two together,  we have: m(t)=1+ mi(t — x)dx\n0\nEmploying  the following  change  of variables:  u = t — x, du = —dx\nWe then substitute  and simplify  to obtain: m(t) =1+ { m(u)du\n0)\nDifferentiating  both sides, we then obtain: m'(t) = m(t)\nSince m(0) = 1, we then have: m(t) = e'\nSince we actually  need to find m(N__,),  we can plug in ¢ = 1 into the equation,  which yields the\ndesired  result m(1) = e.\n76 Ace the Data Science  Interview | Statistics"
  },
  {
    "page_number": 89,
    "content": "Machine  Learning\nCHAPTER  7\nHow much machine  learning  do you actually  need  to know to land  a top  job in Silicon  Valley\nor Wall Street?  Probably  less than you think! From coaching  hundreds  of data folks on the\njob hunt, one of the most common  misconceptions  we saw was candidates  thinking  their\nlack of deep learning  expertise  would tank their performance  in data science  interviews.\nHowever,  the truth is that most data scientists  are hired to solve business  problems  — not\nblindly  throw complicated  neural networks  on top of dirty data. As such, a data scientist\nwith strong  business  intuition  can create  more business  value by applying  linear  regression\nin an Excel sheet than a script kiddie whose knowledge  doesnt  extend  bevond  the Keras\nAPI.\nSo, unless you're interviewing  for ML Engineering  or research  scientist  roles, a solid\nunderstanding  of  the classical  machine  learning  techniques  covered  in this chapter  is all you\nneed to ace the data science  interview.  However,  if  you are aiming  for ML-heavy  roles that\ndo require  advanced  knowledge,  this chapter  will still be handy! Throughout  this chapter,\nwe frequently  call attention  to which topics and types of questions  show up in tougher  ML\ninterviews.  Plus, the 35 questions  at the end of the chapter  — especially  the hard ones —\nwill challenge  even the most seasoned  ML practitioner.\nWhat  to Expect  for ML Interview  Questions\nWhen machine  learning  1s brought up in an interview  context, the problems  fall into three major\nbuckets:\nAce the Data Science  Interview 77"
  },
  {
    "page_number": 90,
    "content": "CHAPTER  7 : MACHINE  LEARNING\n° Conceptual  questions:  Do you have a strong theoretical  ML background?\ne Resume-driven  questions:  Have you actively  applied  ML before?\n¢ End-to-end  modeling  questions:  Can you apply ML to a hypothetical  business  problem?\nConceptual  Questions\nConceptual  questions  usually center around what different  machine learning terms mean and how\npopular  machine  learning  techniques  operate.  For example,  two frequently  asked questions  are “What\nis the bias-variance  tradeoff?”  and “How does PCA work?”  To test your ability to communicate  with\nnontechnical  stakeholders,  a common  twist on these conceptual  questions  is to ask you to explain  the\nanswer  as if they (the interviewer)  were five years old (similar  to Reddit’s  popular  r/ELI5 subreddit).\nBecause many data science roles don’t require hardcore machine learning knowledge,  easier,\nstraightforward  questions  such as these represent  the vast majority  of questions  you’d expect during\na typical interview.  Being asked easier ML questions  is especially  the case when interviewing  for a\ndata science role that’s more product  and business  analytics  oriented,  as having to build models  just\nsimply  isn’t part of the day-to-day  work.\nFor ML-intensive  positions  like ML Engineer  or Research  Scientist,  interviews  also start with high-\nlevel easier conceptual  questions  but then push you to dive deeper into the details via follow-up\nquestions.  Companies  do this to make sure you aren't a walking,  talking ML buzzword  generator.\nFor example,  as a follow-up  to defining  the bias-variance  trade-off,  you might be asked to whiteboard\nthe math behind  the concept.  Instead  of simply  asking  you how PCA (principal  components  analysis)\nworks,  you might also be asked about the most common  pitfalls  of using PCA.\nSince ML interviews  are so expansive  in scope, if asked about a particular  technique  you may not\nbe overly familiar  with, it’s perfectly  okay to say, “I’ve read about it in the past. | don’t have any\nea\n. experience  with these types of techniques,  but | am interested  to learn more about them\nThis signals  honesty  and an eagerness  to learn (and don’t be ashamed  to admit not knowing  something\nnobody knows all the techniques  in detail! Trust us, it’s better than pretending  you know the\ntechniques  and then falling  apart when questions  are asked).\nIf nothing  on your resume seems interesting  to an interviewer,  but they still want to go deep into\none ML topic, they have you pick the topic. They do this by either asking “What’s  your favorite\nML algorithm?”  or “What's  a model you use often and why?” Consequently,  it pays to have a deep\nunderstanding  of at least a single technique  -— something  you’ve actually  used before and that is\nlisted on your resume.\nWord of caution:  don’t choose something  about  a state-of-the-art  transformer  model to discuss  as\nyour favorite  technique.  Your details  on it may be hazy, and your interviewer  might not know enough\nabout il to carry on a good conversation.  You are better off picking something  fundamental  yet\ninteresting  (to you) so that you and your interviewer  can have a meaningful  discussion.  For example,\nour answer  happens  to be that we both like random  forests  because  they can handle  classification  or\nregression  tasks with all kinds of input features  with minimal  preprocessing  needed.  Additionally,  we\nboth have projects  on our resume  to back up our interest  in random  forests.\nResume-Driven  Questions\nThe next most common  type of interview  question  for ML interviews  is the resume-driven  question.\nResume-driven  questions  are ofien about showcasing  that you have practical  experience  (as opposed\n78 Ace the Data Science  Interview  | Machine  Learning"
  },
  {
    "page_number": 91,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nto conceptual  knowledge).  As such, if you have job experience  that is directly  relevant,  interviewers\nwill often ask about that. If not, they’l]  often fall back to asking  about your projects.\nWhile anything  listed on your resume  is fair game to be picked  apart, this is especially  true for more\nML-heavy  roles. Because  the field is so vast and continually  evolving,  an interviewer  isn’t able to\nassess your fit tor the job by asking about some niche topic unrelated  to the position  at hand. For\nexample,  say you are going for a general  data science  role — it’s not fair to ask a candidate  about\nCNNs  and their use in computer  vision if they have no experience  with this topic and it’s not relevant\nto the job. But, suppose  you hacked  together  a self-driving  toy car last summer,  and listed it on your\nresume.  In that case — even though  the role at hand may not require  computer  vision  — it’s totally\nfair game to be asked more about the neural network  architecture  you used, model training  issues\nyou faced, and trade-offs  you made versus other techniques.  Plus, in an effort to see if you know\nthe details not just of your project,  but of the greater  landscape,  you'd also be expected  to answer\nquestions  tangentially  related  to the project.\nEnd-to-End  Modeling  Questions\nFinally,  the last type of ML-related  problems  can expect during interviews  are end-to-end  modeling\nquestions.  Interviewers  are testing your ability to go beyond  the ML theory covered  in books like\nAn Introduction  to Statistical  Learning  and actually  apply what you learned to solve real-world\nproblems.  Examples  of questions  include  “How would  you match Uber drivers  to riders?”  and “How\nwould you build a search  autocomplete  feature  for Pinterest?”  While these open-ended  problems  are\nan interview  staple for any machine-learmming-heavy  role, they do also pop up during  generalist  data\nscience  interviews.\nAt the end of this chapter,  we cover the end-to-end  machine  learning  workflow,  which can serve as a\nframework  for answering  these broad ML questions.  We cover steps like problem  definition,  feature\nengineering,  and performance  metric selection  — things you'd do before  jumping  into the various\nML techniques  we soon cover. To better solve these ML case study problems,  we also recommend\nreading Chapter 11: Case Study to understand  the non-ML-specific  advice we offer for tackling\nopen-ended  problems.\nThe Math Behind  Machine  Learning\nWhile the probability  and statistics  concepts  upon which machine  learning’s  foundation  is built are\nfair game for interviews,  you're less likely to be asked about the linear algebra and multivariable\ncalculus concepts  that underlie machine learning. There are, however,  two notable exceptions:  if\nyou’re interviewing  for a research  scientist  position or for quant finance. In these cases, you may\nbe expected  to whiteboard  proofs and derive formulas.  For example,  you could be asked to derive\nthe least squares  estimator  in linear regression  or explain  how to calculate  the principal  components\nin PCA. Sometimes,  to see how strong your first principles  are, you'll be given a math problem\nmore indirectly.  For instance,  you could be asked to analyze the statistical  factors driving portfolio\nreturns (which essentially  boils down to explaining  the math behind PCA). Regardless  of the role and\ncompany,  we still recommend  you review the basics, since understanding  them will help you grok the\ntheoretical  underpinnings  of the techniques  covered  later in this chapter.\nLinear  Algebra\nThe main linear algebra subtopic  worth touching  on for interviews  is eigenvalues  and eigenvectors.\nMechanically,  for some n x n matrix A, x is an eigenvector  of A if: Ax = Ax, where > is a scalar. A\nAce the Data Science  Interview 79"
  },
  {
    "page_number": 92,
    "content": "CHAPTER  7 : MACHINE  LEARNING\nmatrix can represent  a linear transformation  and, when applied  to a vector  x, results in another  vector\ncalled an eigenvector,  which has the same direction  as x and is in fact x multiplied  by a scaling factor\ni. known  as an eigenvalue.\nThe decomposition  of a square  matrix into its eigenvectors  is called an eigendecomposition.  However,\nnot all matrices are square. Non-square  matrices are decomposed  using a method called singular\nvalue decomposition  (SVD). A matrix to which SVD is applied  has a decomposition  of the form: A =\nUV\", where U is an m x m matrix,  = is an m x n matrix, and Vis an n x n matrix.\nThere are many applications  of linear algebra  in ML, ranging  from the matrix multiplications  during\nbackpropagation  in neural networks,  to using eigendecomposition  of a covariance  matrix in PCA.\nAs such, during technical  interviews  for ML engineering  and quantitative  finance roles, you should\nbe able to whiteboard  any follow-up  questions  on the linear algebra  concepts  underlying  techniques\nlike PCA and linear regression.  Other linear algebra  topics you’re  expected  to know are core building\nblocks like vector  spaces,  projections,  inverses,  matrix  transformations,  determinants,  orthonormality,\nand diagonalization.\nGradient  Descent\nMachine  learning  is concerned  with minimizing  some particular  objective  function  (most commonly\nknown as a loss or cost function).  A loss function  measures  how well a particular  model fits a given\ndataset, and the lower the cost, the more desirable.  Techniques  to optimize  the loss function  are\nknown  as optimization  methods.\nOne popular  optimization  method 1s gradient  descent,  which takes small steps in the direction  of\nsteepest  descent  for a particular  objective  function.  It’s akin to racing  down  a hill. To win, you always\ntake a “next step” in the steepest  direction  downhill.\nCost Initial !\nMinimum  Cost\nDerivative  of Cost\nWeight\nFor convex  functions,  the gradient  descent  algorithm  eventually  finds the optimal  point  by updating the\nbelow equation  until the value at the next iteration  is very close to the current  iteration  (convergence):\nX,,=%X,-  a,Vf(x,)\nthat is, it calculates  the negative  of the gradient  of the cost function  and scales that by some constant\ngu. Which ts known as the learning  rate, and then moves in that direction  at each iteration  of the\nalgorithm.\nSince many cost functions  in machine  learning  can be broken down into the sum of individual\nfunctions,  the gradient  step can be broken  down into adding  separate  gradients.  However,  this process\ncan be computationally  expensive,  and the algorithm  may get stuck at a local minimum  or saddle\n80 Ace the Data Science  Interview  | Machine  Learning"
  },
  {
    "page_number": 93,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\npoint. Therefore,  we can use a version  of gradient  descent  called  stochastic  gradient  descent  (SGD),\nwhich adds an element  of randomness  so that the gradient  does not get stuck. SGD uses one data\npoint at a time for a single step and uses a much smaller  subset  of data points at any given step, but is\nnonetheless  able to obtain an unbiased  estimate  of the true gradient.  Alternatively,  we can use batch\ngradient  descent  (BGD),  which uses a fixed, small number  (a mini-batch)  of data points per step.\nGradient  descent  and SGD are popular  topics for ML interviews  since they are used to optimize\nthe training  of almost all machine  learning  methods.  Besides  the usual questions  on the high-level\nconcepts  and mathematical  details,  you may be asked when you would  want to use one or the other.\nYou might  even be asked to implement  a basic version  of SGD in a coding  interview  (which  we cover\nin Chapter  9, problem  #30).\nModel  Evaluation  and Selection\nWith the math underlying  machine  learning  techniques  out of the way, how do we actually  choose\nthe best model for our problem,  or compare  two models  against  each other? Model  evaluation  is the\nprocess  of evaluating  how well a model performs  on the test set after it’s been trained  on the train\nset, Separating  out your training  data — usually  80% for the train set — from the 20% of the test set\nis critical  because  the usefulness  of a model boils down to how good predictions  are on data that has\nnot been seen before.\nModel  selection,  as the name implies,  is the process  of selecting  which  model  to implement  after each\nmodel has been evaluated.  Both steps (evaluation  and selection)  are critical  to get right, because  even\ntiny changes  in model performance  can lead to massive  gains at big tech companies.  For example,  at\nFacebook,  a model that can cause even a 0.1% lift in ad click-through  rates can lead to $10+ million\nin extra revenue.\nThat’s why in interviews,  especially  during case-study  questions  where you solve an open-ended\nproblem,  discussions  often head toward comparing  and contrasting  models,  and selecting  the most\nsuitable  one after factoring  in business  and product constraints.  Thus, internalizing  the concepts\ncovered  in this section  is key to succeeding  in ML interviews.\nBias-Variance  Trade-off\nThe bias-variance  trade-off  is an interview  classic,  and is a key framework  for understanding  different\nkinds of models.  With any model, we are usually  trying to estimate  a function  f(x), which predicts  our\ntarget variable  y based on our input x. This relationship  can be described  as follows:\ny= f(x) +w\nwhere w is noise, not captured  by f(x), and is assumed  to be distributed  as a zero-mean  Gaussian\nrandom  variable  for certain  regression  problems.  To assess how well the model fits, we can decompose\nthe error of y into the following:\n1 Bias: how close the model’s predicted  values come to the true underlying  f(x) values, with\nsmaller  being better\n2 Variance:  the extent to which model prediction  error changes based on training inputs, with\nsmaller  being better\n3. Irreducible  error: variation  due to inherently  noisy observation  processes\nAce the Data Science  Interview 81"
  },
  {
    "page_number": 94,
    "content": "CHAPTER  7 : MACHINE  LEARNING\nThe trade-off  between bias and variance provides a lens through which you can analyze different\nmodels. Say we want to predict housing  prices given a large set of potential  predictors  (square footage\nof a house, the number of bathrooms,  and so on). A model with high bias but low variance,  such as\nlinear regression,  is easy to implement  but may oversimplify  the situation  at hand. This high bias but\nlow variance situation would mean that predicted  house prices are frequently  off from the market\nvalue, but the variance in these predicted  prices is low. On the flip side, a model with low bias and\nhigh variance,  such as neural networks,  would lead to predicted  house prices closer to market value,\nbut with predictions  varying  wildly based on the input features.\nLow Bias\nHigh Bias\nWhile the bias-variance  trade-off  equation  occasionally  shows up in dala science interviews,  more\nfrequently,  you'll be asked to reason about the bias-variance  trade-off  given a specific situation.\nFor example,  presented  with a model that has high variance,  you could mention  how you'd source\nadditional  data to fix the issue. Posed with a situation  where the mode! has high bias, you could\ndiscuss  how increasing  the complexity  of the model could help. By understanding  the business  and\nproduct  requirements,  you'll know how to make the bias-variance  trade-off  for the interview  problem\nposed.\nModel  Complexity  and Overfitting\n“All models are wrong, but some are useful” is a well-known  adage, coined by statistician  George\nBox. Ultimately,  our goal is to discover  a model that can generalize  to learn some relationship  within\ndatasets.  Occam’s  razor, applied to machine  learning,  suggests  that simpler  models are generally\nmore useful and correct than more complicated  models.  That’s because  simpler,  more parsimonious\nmodels  tend to generalize  better.\nSaid another  way, simpler,  smaller models are less likely to overfit (fit too closely to the training\ndata). Overtit models tend not to generalize  well out of sample.  That’s because  during overfitting,\nthe models pick up too much noise or random fluctuations  using the training  data, which hinders\nperformance  on data the model has never seen before.\n82 Ace the Data Science  Interview  | Machine  Learning"
  },
  {
    "page_number": 95,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nA High Variance High Bias A Low Bias, Low Variance\ne °.°y y a. *. *e e ‘ee e y\n@ ee @\ne ° e\nXx ia x >\nOverfitting Underfitting Good Balance\nUnderfitting  refers to the opposite  case — the scenario  where the model is not learning  enough  of\nthe true relationship  underlying  the data. Because  overfitting  is so common  in real-world  machine\nlearning,  interviewers  commonly  ask you how you can detect it, and what you can do to avoid it,\nwhich  brings us to our next topic: regularization.\nRegularization\nRegularization  aims to reduce the complexity  of models.  In relation  to the bias-variance  trade-off,\nregularization  aims to decrease  complexity  in a way that significantly  reduces  variance  while only\nslightly  increasing  bias. The most widely  used forms of regularization  are L! and L2. Both methods\nadd a simple  penalty  term to the objective  function.  The penalty  helps shrink coefficients  of features,\nwhich reduces  overfitting.  This is why, not surprisingly,  they are also known  as shrinkage  methods.\nSpecifically,  L1, also known  as /asso, uses the absolute  value of a coefficient  to the objective  function\nas a penalty.  On the other hand, L2, also known  as ridge, uses the squared  magnitude  of a coefficient\nto the objective  function.  The L] and L2 penalties  can also be linearly  combined,  resulting  in the\npopular  form of regularization  called  elastic  net. Since having  models  overfit  is a prevalent  problem  in\nmachine  learning,  it’s important  to understand  when to use each type of regularization.  For example,\nLI serves as a feature selection  method,  since many coefficients  shrink to 0 (are zeroed out), and\nhence, are removed  from the model. L2 ts less likely to shrink any coefficients  to 0. Therefore,  L1\nregularization  leads to sparser  models,  and is thus considered  a more strict shrinkage  operation.\nInterpretability  & Explainability\nIn Kaggle competitions  and classwork,  you might be expected  to maximize  a model performance\nmetric like accuracy.  However,  in the real world, rather than just maximizing  a particular  metric,  you\nmight also be responsible  for explaining  how your model came up with that output. For example,  if\nyour model  predicts  that someone  shouldn’t  get a loan, doesn’t  that person  deserve  to know why? More\nbroadly,  interpretable  models  can help you identify  biases in the model, which leads to more ethical\nAI. Plus, in some domains  like healthcare,  there can be deep auditing  on decisions,  and explainable\nmodels can help you stay compliant.  However,  there's usually a trade-off  between  performance  and\nmodel interpretability.  Often, using a more complex  model might increase  performance,  but make\nresults harder  to interpret.\nVarious models have their own way of interpreting  feature importance.  For example,  linear models\nhave weights which can be visualized  and analyzed  to interpret the decision making. Similarly,\nrandom forests have feature importance  readily available  to identify what the model is using and\nlearning. There are also some general frameworks  that can help with more “black-box”  models.\nOne is SHAP (SHapley  Additive  exPlanation),  which uses “Shapley”  values to denote the average\nmarginal contribution  of a feature over all possible combinations  of inputs. Another technique  1S\nLIME (Local Interpretable  Model-agnostic  Explanations),  which uses sparse linear models built\naround various predictions  to understand  how any model performs  in that local vicinity.\nAce the Data Science  Interview 83"
  },
  {
    "page_number": 96,
    "content": "CHAPTER  7 : MACHINE  LEARNING\nWhile it’s rare to be asked about the details of SHAP and LIME during interviews,  having a basic\nunderstanding  of why model interpretability  matters, and bringing up this consideration  in more\nopen-ended  problems  is key.\nModel  Training\nWe’ ve covered frameworks  to evaluate  models, and selected the best-performing  ones, but how do\nwe actually train the model in the first place? If you don’t master the art of model training (aka\nteaching  machines  to learn), even the best machine  learning  techniques  will fail. Recall the basics: we\nfirst train models on a training  dataset and then test the models on a testing dataset. Normally,  80%\nof the data will go towards  training  data, and 20% serves as the test set. But as we soon cover, there’s\nmuch more to model training  than the 80/20 train vs. test split.\nCross-Validation\nCross-validation  assesses  the performance  of an algorithm  in several subsamples  of training  data. It\nconsists  of running  the algorithm  on subsamples  of the training  data, such as the original  data without\nsome of the original observations,  and evaluating  model performance  on the portion  of the data that\nwas excluded  from the subsample.  This process  is repeated  many times for the different  subsamples,\nand the results are combined  at the end.\nCross-validation  helps you avoid training  and testing  on the same subsets  of data points, which would\nlead to overfitting.  As mentioned  earlier, in cases where there isn’t enough  data or getting  more data\nis costly, cross-validation  enables  you to have more faith in the quality  and consistency  of a model’s\ntest performance.  Because  of this, questions  about how cross-validation  works and when to use it are\nroutinely  asked in data science  interviews.\nOne popular  way to do cross-validation  is called k-fold  cross-validation.  The process  1s as follows:\n1. Randomly  shuffle  data into equally-sized  blocks  (folds).\n2. For each fold &, train the model on all the data except for fold 7, and evaluate  the validation  error\nusing block 7.\n3. Average  the 4 validation  errors from step 2 to get an estimate  of the true error.\nDataset\n| \\ |\nEstimation  1 Test 1 Train Train Train Train\nEstimation  2 Train Test 2 Train Train Train\nEstimation  3 Train Train Test 3 Train Train\nEstimation  4 Train Train Train Test 4 Train\nEstimation  5 Train Train Train Train Test 5\nExample  of 5-Fold Cross Validation\nAnother  form of  cross-validation  you’re expected  to know for the interview  is /eave-one-out  cross-\nvalidation,  LOOCY  ts a special case of k-fold cross-validation  where & is equal to the size of the\ndataset (77). That is, it is where the model is testing on every single data point during the cross-\nvalidation.\nIn the case of larger  datasets,  cross-validation  can become  computationally  expensive,  because  every\nfold is used for evaluation.  In this case, it can be better to use train validation  split, where you split\n84 Ace the Data Science  Interview | Machine  Learning"
  },
  {
    "page_number": 97,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nthe data into three parts: a training  set, a dedicated  validation  set (also known  as a “dev” set), and a\nlest set. The validation  set usually  ranges  from 10%-20%  of the entire dataset.\nAn interview  question  that comes up from time to time is how to apply cross-validation  for time-\nseries data. Standard  k-fold  CV can’t be applied,  since the time-series  data is not randomly  distributed\nbut instead  is already  in chronological  order. Therefore,  you should not be using data “in the future”\nfor predicting  data “from the past.” Instead,  you should  use historical  data up until a given point in\ntime, and vary that point in time from the beginning  till the end.\nBootstrapping  and Bagging\nThe process  of bootstrapping  is simply drawing  observations  from a large data sample  repeatedly\n(sampling  with replacement)  and estimating  some quantity  of a population  by averaging  estimates\nfrom multiple  smaller  samples.  Besides  being useful in cases where  the dataset  is small,  bootstrapping\nis also useful for helping  deal with class imbalance:  for the classes  that are rare, we can generate  new\nsamples  via bootstrapping.\nAnother  common  application  of bootstrapping  is in ensemble  learning:  the process  of averaging  estimates\nfrom many smaller  models into a main model. Each individual  model is produced  using a particular\nsample  from the process.  This process  of bootstrap  aggregation  is also known as bagging.  Later in this\nchapter,  we'll show concrete  examples  of how random  forests utilize  bootstrapping  and bagging.\nEnsemble  methods  like random  forests,  AdaBoost,  and XGBoost  are industry  favorites,  and as such,\ninterviewers  tend to ask questions  about bootstrapping  and ensemble  learning.  For example,  one of the\nmost common  interview  questions  1s: “What  1s the difference  between  XGBoost  and a random  forest?”\nHyperparameter  Tuning\nHyperparameters  are important  because  they impact  a model’s  training  time, compute  resources  needed\n(and hence cost), and, ultimately,  performance.  One popular  method  for tuning hyperparameters  is\ngrid search, which involves  forming  a grid that is the Cartesian  product of those parameters  and\nthen sequentially  trying all such combinations  and seeing which yields the best results. While\ncomprehensive,  this method  can take a long time to run since the cost increases  exponentially  with\nthe number  of hyperparameters.  Another  popular  hyperparameter  tuning method  is random  search,\nwhere we define  a distribution  for each parameter  and randomly  sample from the joint distribution\nover all parameters.  This solves the problem  of exploring  an exponentially  increasing  search space,\nbut is not necessarily  guaranteed  to achieve  an optimal  result.\nWhile not generally  asked about in data science  interviews  for research  scientist  or machine  learning\nengineering  roles, hyperparameter  tuning techniques  such as the methods  mentioned  earlier, along\nwith Bayesian  hyperparameter  optimization,  might be brought  up. This discussion  mostly happens  in\nthe context  of neural networks,  random forests, or XGBoost.  For interviews,  you should be able to\nlist a couple of the hyperparameters  for your favorite  modeling  technique,  along with what impacts\nthey have on generalization.\nTraining  Times  and Learning  Curves\nTraining  time is another  factor to consider  when it comes to model selection,  especially  for exceedingly\nlarge datasets.  As we explain later in the coding chapter, it’s possible  to use big-O notation  to clarify\nthe theoretical  bounds on training time for each algorithm.  These training time estimates  are based\non the number  of data points and the dimensionality  of the data.\nFor real-life  training  ML models, you should also factor in training  time considerations  and resource\nconstraints  during model selection. While you can always train more complex models that might\nAce the Data Science  Interview 85"
  },
  {
    "page_number": 98,
    "content": "CHAPTER  7: MACHINE  LEARNING\nachieve  marginally  higher model performance  metrics,  the trade-off  versus increased  resource usage\nand training  time might make such a decision  suboptimal.\nLearning  curves are plots of model learning performance  over time. The y-axis is some metric of\nlearning  (for example,  classification  accuracy),  and the x-axis is experience  (time).\n1.0-4 —— train\n; —— validation\n0.9-\n0.8-\n0.774\nLoss\n0.6-\n—T T ! T ! T\n0 100 200 300 400 500\nNumber  of {terations\nA popular  data science  interview  question  involving  learning  curves is “How would you identify  if\nyour model was overfitting?”  By analyzing  the learning  curves,  you should  be able to spot whether  the\nmodel is underfitting  or overfitting.  For example,  above, you can see that as the number  of iterations\nIs increasing,  the training  error is getting  better. However,  the validation  error 1s not improving  -— 1n\nfact, it 1s Increasing  at the end — aclear  sign that the model is overfitting  and training  can be stopped.\nAdditionally,  learning  curves should help you discover  whether  a dataset is representative  or not.\nIf the data was not representative,  the plot would show a large gap between  the training  curve and\nvalidation  curve, which doesn’t  get smaller  even as training  time increases.\nLinear  Regression\nLinear regression  ts a form of supervised  learning,  where a model is trained  on labeled input data.\nLinear regression  is one of the most popular  methods  employed  in machine  learning  and has many\nreal-life  applications  due to its quick runtime  and interpretability.  That’s why there’s the joke about\nregression  to regression:  where you try to solve a problem  with more advanced  methods  but end up\nfalling  back to tried and true linear regression.\nAs such, linear regression  questions  are asked in all types of data science and machine  learning\ninterviews.  Essentially,  interviewers  are trying to make sure your knowledge  goes beyond  importing\nlinear regression  from scikit-learn  and then blindly calling linear_regression.fil(X.Y).  That’s why\ndeep knowledge  of linear regression ~ understanding  its assumptions,  addressing  edge cases that\ncome up in real-life  scenarios,  and knowing  the different  evaluation  metrics -- will set you apart\nfrom other candidates.\n86 Ace the Data Science  Interview | Machine  Learning"
  },
  {
    "page_number": 99,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nIn linear regression,  the goal is to estimate  a function  f(x), such that each feature has a linear\nrelationship  to the target variable  y, or:\ny = XB\nwhere  X is a matrix  of predictor  variables  and f is a vector  of parameters  that determines  the weight\nof each variable  in predicting  the target variable.  So, how do you compare  the performance  of two\nlinear regression  models?\nEvaluating  Linear  Regression\nEvaluation  of a regression  model is built on the concept  of a residual:  the distance  between  what the\nmodel predicted  versus the actual value. Linear regression  estimates  8 by minimizing  the residual\nsum of squares  (RSS),  which is given by the following:\nRSS(B)  = (y — XB)\" — XB)\nTwo other sum of squares  concepts  to know besides  the RSS are the total sum of squares (TSS)\nand explained  sum of squares  (ESS). The total sum of squares  is the combined  variation  in the data\n(ESS + RSS). The explained  sum of squares  is the difference  between  TSS and RSS. R?, a popular\nmetric for assessing  goodness-of-fit,  is given by R* = 1 — a It ranges between zero and one,\nand represents  the proportion  of variability  in the data explained  by the model. Other prominent\nerror metrics  to measure  the goodness-of-fit  of linear regression  are MSE (mean squared  error) and\nMAE (mean absolute  error). MSE measures  the variance  of the residuals,  whereas  MAE measures\nthe average  of the residuals;  hence, MSE penalizes  larger errors more than MAE, making  it more\nsensitive  to outliers.\nActual  vs Predicted  Values  from the Dummy  Dataset\n° e\nRSS: Unexplained  Random  Variance 7°\n4-\nESS: Explained  Variance °\ne e ° e\n3 ° °\nMEAN LINE\ny\n2-\nObserved  Data\n1-\ne@\n®\nREGRESSION  LINE\ne\n0- 1 T T T F y\n—4 —2 0 2 4 6\nx\nAce the Data Science  interview 87"
  },
  {
    "page_number": 100,
    "content": "CHAPTER  7 : MACHINE  LEARNING\nA common  interview  question  is “What’s  the expected  impact on R* when adding more features to\na model?”  While adding more features to a model always increases  the R’, that doesn’t necessarily\nmake for a better model. Since any machine  learning  model can overfit by having more parameters,\na goodness-of-fit  measure like R? should likely also be assessed  with model complexity  in mind.\nMetrics  that take into account the number  of features  of linear regression  models include  AIC, BIC,\nMallow’s  CP, and adjusted  R?.\nSubset  Selection\nSo, how do you reduce model complexity  of a regression  model? Subset selection.  By default, we\nuse all the predictors  in a linear model. However,  in practice, it’s important  to narrow down the\nnumber  of features,  and only include the most important  features.  One way is best subset  selection,\nwhich tries each model with & predictors,  out of p possible  ones, where k < p. Then, you choose the\nbest subset model using a regression  metric like R?. While this guarantees  the best result, it can be\ncomputationally  infeasible  as p increases  (due to the exponential  number  of combinations  to try).\nAdditionally,  by trying every option in a large search space, you're likely to get a model that overfits\nwith a high variance  in coefficient  estimates.\nTherefore,  an alternative  is to use stepwise  selection.  In forward  stepwise  selection,  we start with an\nempty model and iteratively  add the most useful predictor.  In backward  stepwise  selection,  we start\nwith the full model and iteratively  remove  the least useful predictor.  While doing stepwise  selection,\nwe aim to find a model with high R? and low RSS, while considering  the number  of predictors  using\nmetnics  like AIC or adjusted  R?.\nLinear  Regression  Assumptions\nBecause  linear regression  ts one of the most commonly  applied  models,  it has the honor  of also being\none of the most misapplied  models. Before you can use this technique,  you must validate  its four\nmain assumptions  to prevent  erroneous  results:\n¢ Linearity:  The relationship  between  the feature  set and the target variable  is linear.\n¢ Homoscedasticity:  The variance  of the residuals  is constant.\n¢ Independence:  A\\\\ observations  are independent  of one another.\n¢ Normality:  The distribution  of Y is assumed  to be normal.\nThese assumptions  are crucial to know. For example,  in the figures  that follows,  there are four lines\nof best fit that are the same. However,  only in the top left dataset  are these four assumptions  met.\n124\n104 0\nOO Os 8- O O\nO\n6 -\nO44\nT T T T TT T 1) is O4 6 8 10 12 14 16 18 4 6 8 10 12 14 16 18\nx x 42\n88 Ace the Data Science  Interview  | Machine  Learning"
  },
  {
    "page_number": 101,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nO\n124 124\n105 10 -\n. fe)> 8- 50° > 8-\n6- 64\n4- 4-4\n4 6 8 10 12 14 16 18 4 6 8 10 12 14 16 18\nX, X,\nNote: for the independence  and normality  assumption,  use of the term “i.i.d.” (independent  and\nidentically  distributed)  is also common.  If any of these assumptions  are violated,  any forecasts  or\nconfidence  intervals  based on the model will most likely be misleading  or biased. As a result, the\nlinear regression  model will likely perform  poorly  out of sample.\nAvoiding  Linear  Regression  Pitfalls\nHeteroscedasticity\nLinear  regression  assumes  that the residuals  (the distance  between  what the model predicted  versus\nthe actual value) are identically  distributed.  If the variance  of the residuals  is not constant,  then\nheteroscedasticity  1s most likely present,  meaning  that the residuals  are not identically  distributed.  To\nfind heteroscedasticity,  you can plot the residuals  versus the fitted values. If the relationship  between\nresiduals  and fitted values has a nonlinear  pattern,  this indicates  that you should try to transform  the\ndependent  variable  or include  nonlinear  terms in the model.\nExample  of heteroscedasticity.  As the x-value  increases,  the residuals  increase  too\nAnother  useful diagnostic  plot is the scale-location  plot, which plots standardized  residuals  versus\nthe fitted values. If the data shows heteroscedasticity,  then you will not see a horizontal  line with\nequally  spread  points.\nAce the Data Science  Interview 89"
  },
  {
    "page_number": 102,
    "content": "CHAPTER  7 : MACHINE  LEARNING\nCase 1 Case 2\nScale-Location Scale-Location\n380 20 - 980\na 98 2 081\nBg 157 0 936 0 2 ° 920\nO roo) ° @ 1.597 o ®\ns °° 0°2 2 9 0 2 fe) 08 C0 .\nuo] _ fe) aap © 00 re) _ Oo 0° 9 © ° 5\n8 \" * oom . o 0° @ oo fe) NX 10- 900 © 08 9s °\n8 0 098 3 oS oF 0 08 8 9e051 9°, 00 $a 5 054 68% Wan ® 0_ re] (eo) oO re) —_ rete (e) Qo fe)2 °@ o 2 9, o°8 S fe) °0 °oO (@)\n0.0 - 0.0 -TOT T T ! l l T |\n-—6 -4 -2 0 2 4 6 —5 5\nFitted values Fitted values\nNormality\nLinear regression  assumes  the residuals  are normally  distributed.  We can test this through  a QQ\nplot. Also known as a quantile  plot, a QQ plot graphs the standardized  residuals  versus theoretical\nquantiles  and shows whether  the residuals  appear  to be normally  distributed  (i.e., the plot resembles\na straight line). If the QQ plot is not a reasonably  straight  line, this is a sign that the residuals  are\nnot normally  distributed,  and hence, the model should  be reexamined.  In that case, transforming  the\ndependent  variable  (with a log or square-root  transformation,  for example)  can help reduce  skew.\nNormal  Q-Q Plot\nLe)\nO° oO\n®@ 08- 0°\n= 0°S 0.6 4 °°\noOSs) fo)\n2 04- °2 °° LIGHT-TAILED\n& 02-4 5°)\n.@)\nre)\nl T T T l\n—2 —1 0 1 2\nTheoretical  Quantiles\nNormal  Q-Q Plot\n2 Oo\noOan 0= Oo\nCc 0?\ns 60°\nCG O- 90°\n® foes\nrot 0°\n= —1-4 Oo °°\nPe NORMAL\n—? +O\nT_ l T T\n—2 —1 0 4 2\nTheoretical  QuantilesSample Quantiles\nSample QuantilesNormal  Q-Q Plot\n1.0 - 50 0\n00°0.9 - (0°\n(e)O\n0.8 : 5°\n(e)\n.@)\n0.7 - O\n6 LEFT  SKEW\n0.6 -\nOo\nT | T |\n—2 —1 0 1\nTheoretical  Quantiles\nNormal  Q-Q Plot\n@)_] .@]0.8 500°\n0.6 7\noO\n0.4 -\n6 BIMODAL\n0.2 00 . - 0 Oo\ne)\nT ] ] I\n~2 ~ 0 1\nTheoretical  Quantiles\n90Ace the Data Science  Interview  | Machine  Learning"
  },
  {
    "page_number": 103,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nNormal  Q-Q Plot Normal  Q-Q Plot\no) (e)\n8 10- g 0.4 - 5\nc 65 o = 03- 0=) oO © =) te)\nOo 0- 900000000 CG °® 00° o 02 °°a _s5- O ror oo\nE HEAVY-TAILED FE ot- °°\n? ~107 0 500°”  = RIGHT  SKEW\n.@) 0 0 ce) °\n| to | | | ° To | | {\n—2 —1 0 1 2 —2 —1 0 4 2\nTheoretical  Quantiles Theoretical  Quantiles\nOutliers\nOutliers  can have an outsized  impact  on regression  results.  There  are several  ways to identify  outliers.\nOne of the more popular  methods  is examining  Cook + distance,  which is the estimate  of the influence\nof any given data point. Cook’s  distance  takes into account  the residual  and leverage  (how far away\nthe X value differs from that of other observations)  of every point. In practice,  it can be useful to\nremove  points with a Cook’s  distance  value above  a certain  threshold.\nMulticollinearity\nAnother  pitfall is if the predictors  are correlated.  This phenomenon,  known as multicollinearity,\naffects  the resulting  coefficient  estimates  by making  it problematic  to distinguish  the true underlying\nindividual  weights  of variables.  Multicollinearity  is most commonly  observed  by weights  that flip\nmagnitude.  It is one of the reasons  why model  weights  cannot  be directly  interpreted  as the importance\nof a feature  in linear regression.  Features  that initially  would appear  to be independent  variables  can\noften be highly correlated:  for example,  the number  of Instagram  posts made and the number  of\nnotifications  received  are most likely highly correlated,  since both are related to user activity  on the\nplatform,  and one generally  causes  another.\nOne way to assess multicollinearity  is by examining  the variance inflation  factor (VIF), which\nquantifies  how much the estimated  coefficients  are inflated  when multicollinearity  exists. Methods  to\naddress  multicollinearity  include  removing  the correlated  variables,  linearly  combining  the variables,\nor using PCA/PLS  (partial  least squares).\nConfounding  Variables\nMulticollinearity  is an extreme  case of confounding,  which occurs when  a variable  (but not the main\nindependent  or dependent  variables)  affects the relationship  between  the independent  and dependent\nvariables.  This can cause invalid correlations.  For example,  say you were studying  the effects of\nice cream consumption  on sunburns  and find that higher ice cream consumption  leads to a higher\nlikelihood  of sunburn.  That would be an incorrect  conclusion  because  temperature  is the confounding\nvariable --- higher summer  temperatures  lead to people eating more ice cream and also spending\nmore time outdoors  (which leads to more sunburn).\nConfounding  can occur in many other ways, too. For example,  one way is selection  bias, where the\ndata are biased due to the way they were collected  (for example,  group imbalance).  Another  problem,\nknown as omitted  variable bias, occurs when important  variables  are omitted, resulting  1n a linear\nregression  model that is biased and inconsistent.  Omitted  variables  can stem from dataset generation\nissues or choices made during modeling. A common  way to handle confounding  is stratification,  a\nAce the Data Science  Interview 91"
  },
  {
    "page_number": 104,
    "content": "CHAPTER  7 : MACHINE  LEARNING\nprocess  where you create multiple  categories  or subgroups  in which the confounding  variables  do not\nvary much, and then test significance  and strength  of associations  using chi square.\nKnowing  about these regression  edge cases, how to identify  them, and how to guard against them 1s\ncrucial. This knowledge  separates  the seasoned  data scientists  from the data neophyte  -— precisely\nwhy it’s such a popular  topic for data science  interviews.\nGeneralized  Linear  Models\nIn linear regression,  the residuals  are assumed  to be normally  distributed.  The generalized  linear model\n(GLM) is a generalization  of linear regression  that allows for the residuals  to not just be normally\ndistributed.  For example,  if Tinder wanted to predict the number  of matches  somebody  would get in a\nmonth, they would likely want to use a GLM like the one below with a Poisson  response  (called Poisson\nregression)  instead of a standard  linear regression.  The three common  components  to any GLM are:\nLink Function Systematic Random\nComponent Component\nIni, b, + b,x, +€\nPoisson  (A.,)\n¢ Random  Component:  is the distribution  of the error term, i.e., normal distribution  for linear\nregression.\n¢ Systematic Component:  consists  of the explanatory  variables,  1.e., the predictors  combined  in a\nlinear combination.\n¢ Link  function:  is the link between  the random  and system  components,  1.e., a linear regression,\nlogit regression,  etc.\nNote that in GLMs,  the response  variable  1s still a linear combination  of weights  and predictors.\nRegression  can also use the weights  and predictors  nonlinearly;  the most common  examples  of this\nare polynomial  regressions,  splines,  and general  additive  models.  While  interesting,  these techniques\nare rarely asked about in interviews  and thus are beyond  the scope of this book.\nClassification\nGeneral  Framework\nInterview  questions  related to classification  algorithms  are commonly  asked during interviews  due\nto the abundance  of real-life  applications  for assigning  categories  to things. For example,  classifying\nusers as likely to churn or not, predicting  whether  a person  will click on an ad or not, and distinguishing\nfraudulent  transactions  from legitimate  ones are all applications  of the classification  techniques  we\nmention  in this section.\nThe goal of classification  is to assign a given data point to one of K possible  classes instead of\ncalculating  a continuous  value (as in regression).  The two types of classification  models are\ngenerative  models  and discriminative  models.  Generative  models  deal with the joint distribution  of\nX and Y, which is defined  as follows:\nP(X, Y) = p(X p(X)\n92 Ace the Data Science  Interview  | Machine  Learning"
  },
  {
    "page_number": 105,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nMaximizing  a posterior  probability  distribution  produces  decision  boundaries  between  classes  where\nthe resulting  posterior  probability  is equivalent.  The second  type of model  is discriminative.  It directly\ndetermines  a decision  boundary  by choosing  the class that maximizes  the probability:\naY =argmax  p(Y = klix)\nk\nThus, both methods  choose  a predicted  class that maximizes  the posterior  probability  distribution;\nthe difference  is simply the approach.  While traditional  classification  deals with just two classes  (0\nor |), multi-class  classification  is common,  and many of the below  methods  can be adapted  to handle\nmultiple  labels.\nEvaluating  Classifiers\nBefore we detail the various  classification  algorithms  like logistic  regression  and Naive Bayes, it’s\nessential  to understand  how to evaluate  the predictive  power  of a classification  model.\nSay you are trying to predict whether  an individual  has a rare cancer that only happens  to | in\n10,000 people. By default,  you could simply predict  that every person doesn’t  have cancer  and be\naccurate  99.99%  of the time. But clearly,  this isn’t a helpful  model — Pfizer  won’t be acquiring  our\ndiagnostic  test anytime  soon! Given imbalanced  classes,  assessing  accuracy  alone is not enough  —\nthis is known  as the “accuracy  paradox”  and is the reason why it’s critical to look at other measures\nfor misclassified  observations.\nBuilding  and Interpreting  a Confusion  Matrix\nWhen building  a classifier,  we want to minimize  the number  of misclassified  observations,  which in\nbinary  cases can be termed  false positives  and false negatives.  In a false positive,  the model incorrectly\npredicts  that an instance  belongs  to the positive  class. For the cancer  detection  example,  a false positive\nwould be classifying  an individual  as having cancer, when in reality, the person does not have it. On\nthe other hand, a false negative  occurs when the model incorrectly  produces  a negative  class. In the\ncancer  diagnostic  case, this would mean saying a person doesn’t  have cancer,  when in fact they do.\nA confusion  matrix helps organize  and visualize  this information.  Each row represents  the actual\nnumber  of observations  in a class, and each column  represents  the number  of observations  predicted\nas belonging  to a class.\nPredicted\nPositive Negative\nFalse Negative (FN) Sensitivity\n! wa: alse Negative - - Pp\nPositive | True Positive  (TP) ,\nType 2 Error =| (TP + FN)\nActual Class { . Specificity\n: False Positive  (FP ; nn y,\\;\n| Negative Type 1 Error True Negative  (TN) Ns  FP)\nPrecision Negative Faouictve Accuracy\nTP TN TP +TN\n(TP + FP) (TN + FN) (TP +TN + FP + FN)\nPrecision  and Recall\nTwo metrics that go beyond accuracy are precision  and recall. In classification,  precision is the\nactual positive  proportion  of observations  that were predicted  positive by the classifier.  In the cancer\nAce the Data Science  Interview 93"
  },
  {
    "page_number": 106,
    "content": "CHAPTER  7 : MACHINE  LEARNING\ndiagnostic  example,  it’s the percentage  of people you said would have cancer who actually ended\nup having the disease. Recall, also known as sensitivity,  is the percentage  of total positive cases\ncaptured,  out of all positive  cases. It’s essentially  how well you do in finding people with cancer.\nIn real-world  modeling,  there’s a natural trade-off  between optimizing  for precision  or recall. For\nexample,  having high recall — catching  most people who have cancer -— ends up saving the lives\nof some people with the disease. However,  this often leads to misdiagnosing  others who didn't truly\nhave cancer, which subjects healthy people to costly and dangerous  treatments  like chemotherapy\nfor a cancer they never had. On the flip side, having high precision  means being confident  that when\nthe diagnostic  comes back positive,  the person really has cancer. However,  this often means missing\nsome people  who truly have the disease.  These patients  with missed  diagnoses  may gain  a false sense\nof security,  and their cancer,  left unchecked,  could lead to fatal outcomes.\nDuring interviews,  be prepared  to talk about the precision  versus recall trade-off.  For open-ended\ncase questions  and take-home  challenges,  be sure to contextualize  the business  and product impact\nof a false positive  or a false negative.  In cases where both precision  and recall are equally  important,\nyou can optimize  the F'/ score: the harmonic  mean of precision  and recall.\nggrecision  * recall\nprecision  + recall\nVisualizing  Classifier  Performance\nBesides precision,  recall, and the F1 score, another popular  way to evaluate  classifiers  1s the receiver\noperating  characteristic  (ROC)  curve. The ROC curve plots the true positive  rate versus the false positive\nrate for various  thresholds.  The area under the curve (AUC)  measures  how well the classifier  separates\nclasses. The AUC of the ROC curve is between  zero and one, and a higher number  means the model\nperforms  better in separating  the classes.  The most optimal  is a curve that “hugs”  the top left of the plot, as\nshown  below.  This indicates  that a model has a high true-positive  rate and relatively  low false-positive  rate.\nROC Curve\n1.0 5\n0.8 -\n0.6 -\n044\nTrue Positive Rate\n0.2 -\n0.0 -\nyl yl\n0.0 0.1 0.4 0.6 0.8 1.0\nFalse Positive  Rate\n94 Ace the Data Science  Interview  | Machine  Learning"
  },
  {
    "page_number": 107,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nLogistic  Regression\nOne of the most popular  classification  algorithms  is logistic  regression,  and it is asked about almost\nas frequently  as linear  regression  during  interviews.  In logistic  regression,  a linear  output  is converted\ninto a probability  between  0 and 1 using the sigmoid  function:\n1\nl+eS(x) =XB\nIn the equation  above,  X is the set of predictor  features  and 8 is the corresponding  vector  of weights.\nComputing  S(x) above produces  a probability  that indicates  if an observation  should  be Classified  as\na “1” (if the calculated  probability  is at least 0.5), and a “0” otherwise.\nP(Y = 1|X) = S(XB)\nLinear  Regression Logistic  Regression\n@\n<if—_\n— a am em ee a eee ee oe - = = ee se - --<I—\nStraight  line\nDependent Variable<\nDependent Variable<\n----f------- ® <IIfo)<It-)\nIndependent  Variable Independent  Variable\nThe loss function  for logistic  regression,  also known  as log-loss,  is formulated  as follows:\nL(w) = ye sl saxq } (0-2 08( Sra\nNote that in cases where more than two outcome  classes exist, softmax  regression  is a commonly\nused technique  that generalizes  logistic  regression.\nIn practice,  logistic  regression,  much like its cousin  linear  regression,  is often used because  it is highly\ninterpretable:  its output,  a predicted  probability,  is easy to explain  to decision  makers.  Additionally,\nits quickness  to compute  and ease of use often make it the first model employed  for classification\nproblems  in a business  context.\nNote, however,  that logistic  regression  does not work well under certain circumstances.  Its relative\nsimplicity  makes  it a high-bias  and low-variance  model,  so 1l may not perform  well when the decision\nboundary  ts not linear. Additionally,  when features  are highly correlated,  the coefficients  B won't be\nas accurate.  To address  these cases, you can use techniques  similar  to those used in linear regression\n(regularization,  removal  of features,  etc.) for dealing  with this issue. For interviews,  it is critical to\nunderstand  both the mechanics  and pitfalls  of logistic  regression.\nNaive Bayes\nNaive Bayes classifiers  require only a small amount of training data to estimate the necessary\nparameters.  They can be extremely  fast compared  to more sophisticated  methods  (such as support\nAce the Data Science  Interview 95"
  },
  {
    "page_number": 108,
    "content": "CHAPTER  7 : MACHINE  LEARNING\nvector machines).  These advantages  lead to Naive Bayes being a popularly  used first technique  in\nmodeling,  and is why this type of classifier  shows up in interviews.\nNaive Bayes uses Bayes’ rule (covered  in Chapter  6: Statistics)  and a set of conditional  independence\nassumptions  in order to learn P(Y|X). There are two assumptions  to know about Naive Bayes:\n1. It assumes  each X is independent  of any other X, given Y for any pair of features  X, and X..\n2. It assumes  each feature  is given the same weight.\nThe decoupling  of the class conditional  feature distributions  means that each distribution  can be\nindependently  estimated  as a one-dimensional  distribution.  That is, we have the following:\nP(X....X |Y) = Il P(XIY)\nUsing the conditional  independence  assumption,  and then applying  Bayes’  theorem,  the classification\nrule becomes:\ny= arg max P(Y = y:) | [P(X |Y = y;)\nTo understand  the beauty of Naive Bayes, recall that for any ML model having  & features,  there are 2‘\npossible  feature interactions  (the correlations  between  them all). Due to the large number  of feature\ninteractions,  typically  you'd need 2‘ data points for a high-performing  model. However,  due to the\nconditional  independence  assumption  in Naive Bayes, there only need to be & data points, which\nremoves  this problem.\nFor text classification  (e.g., classifying  spam, sentiment  analysis),  this assumption  is convenient\nsince there are many predictors  (words)  that are generally  independent  of one another.\nWhile the assumptions  simplify  calculations  and make Naive Bayes highly scalable  to run, they are\noften not valid. In fact, the first conditional  independence  assumption  generally  never holds true,\nsince features  do tend to be correlated.  Nevertheless,  this technique  performs  well in practice  since\nmost data is linearly  separable.\nSVMs\nThe goal of SVM ts to form a hyperplane  that linearly  separates  the training  data. Specifically,  it aims\nto maximize  the margin,  which ts the minimum  distance  from the decision  boundary  to any training\npoint. The points closest to the hyperplane  are called the support  vectors. Note that the decision\nboundaries  for SVMs can be nonlinear,  which 1s unlike that of logistic  regression,  for example.\nMarin,  Um\nA 7\nv\n96 Ace the Data Science  interview  | Machine  Learning"
  },
  {
    "page_number": 109,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nIn the image above, it’s easy to visualize  how  a line can be found that separates  the points correctly\ninto their two classes.  In practice,  splitting  the points isn’t that straightforward.  Thus, SVMs rely ona\nkernel to transform  data into a higherdimensional  space, where it then finds the hyperplane  that best\nseparates  the points.  The image  below  visualizes  this kernel transformation:\n&-\nInput Space Feature  Space\nMathematically,  the kernel generalizes  the dot product  to a higher  dimension:\nK(x, y) = o(x)\" oy)\nThe RBF (radial basis function)  and Gaussian  kernels are the two most popular  kernels used in\npractice.  The general  rule of thumb js this: for linear problems,  use a linear kernel,  and for nonlinear\nproblems,  use a nonlinear  kernel like RBF. SVMs can be viewed as a kernelized  form of ridge\nregression  because  they modify  the loss function  employed  in ridge regression.\nSVMs  work well in high-dimensional  spaces  (a larger  number  of dimensions  versus  the number  of data\npoints)  or when a clear hyperplane  divides  the points. Conversely,  SVMs  don’t work well on enormous\ndata sets, since computational  complexity  1s high, or when the target  classes  overlap  and there ts no clean\nseparation.  Compared  to simpler  methods  with linear decision  boundaries  such as logistic regression\nand Naive Bayes,  you may want to use SVMs if you have nonlinear  decision  boundanes  and/or  a much\nsmaller  amount  of data. However.  if interpretability  is important,  SVMs are not preferred  because  they\ndo not have simple-to-understand  outputs  (like logistic  regression  does with a probability).\nFor ML-heavy  roles, know when to use which kernel,  the kernel trick, and the underlying  optimization\nproblem  that SVMs solve.\nDecision  Trees\nDecision  trees and random  forests are commonly discussed  during interviews  since they are flexible  and\noften perform  well in practice  for both classification  and regression  use cases. Since both use cases are\npossible,  decision  trees are also known as CART (classification  and regression  trees). For this section,\nwe'll focus on the classification  use case for decision  trees. While reading  this section,  keep in mind that\nfor interviews,  it helps to understand  how both decision  trees and random forests are trained. Related\ntopics of entropy  and information  gain are also crucial to review before a data science interview.\nTraining\nA decision  tree is a model that can be represented  in a treelike form determined  by binary splits made\nin the feature space and resulting  in various leaf nodes, each with a different  prediction.  Trees are\nAce the Data Science  Interview 97"
  },
  {
    "page_number": 110,
    "content": "CHAPTER  7: MACHINE  LEARNING\ntrained in a greedy and recursive  fashion,  starting  at a root node and subsequently  proceeding  through\na series of binary splits in features (i.e., variables)  that lead to minimal error in the classification  of\nobservations.\nSurvival  of Passengers  on the Titanic\ngender\nmale female\naa\nsurvived\nage 0.73, 36%7\n95 < age age <= 95\ndied\n0.17, 60% SBISP\naoTMN\n3<=sbisp sbisp <3\ndied survived\n0.02, 2% 0.89, 2%\nEntropy\nThe entropy  of a random  variable  Y quantifies  the uncertainty  in Y. For a discrete  variable  Y (assuming\nk states) it 1s stated as follows:\nk\nH(Y)=-)  P(y=k)logP(Y  =k)\nr=]\nFor example.  for a simple  Bernoulli  random  variable,  this quantity  1s highest  when p = 0.5 and lowest\nwhen p = 0 or p = 1, a behavior  that aligns intuitively  with its definition  since if p = 0 or I, then\nthere is no uncertainty  with respect to the result. Generally,  1f a random  vanable  has high entropy,\nits distribulion  ts closer to uniform  than a skewed  one. There are many measures  of entropy  — in\npractice,  the Gini index is commonly  used for decision  trees.\nIn the context of decision  trees, consider  an arbitrary  split. We have H(Y) from the initial training\nlabels and assume  that we have some feature Y on which we want to split. We can characterize  the\nreduction  in uncertainty  given by the feature  X, known  as information  gain, which can be formulated\nas follows:\nIG(Y, X) = H(Y) — H(V1X)\nThe larger /G(Y, X) 1s, the higher the reduction  in uncertainty  in Y by splitting  on_X. Therefore,  the\ngeneral process assesses  all features  in consideration  and chooses  the feature that maximizes  this\ninformation  gain, then recursively  repeats  the process  on the two resulting  branches.\nRandom  Forests\nTypically,  an individual  decision  tree may be prone to overfitting  because  a leaf node can be created\nfor each observation.  In practice,  random  forests  yield better out-ofsample  predictions  than decision\n98 Ace the Data Science  Interview | Machine  Learning"
  },
  {
    "page_number": 111,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\ntrees. A random  forest is an ensemble  method  that can utilize many decision  trees, whose decisions\nit averages.\nTwo characteristics  of random  forests  allow a reduction  in overfitting  and the correlation  between  the\ntrees. The first is bagging,  where individual  decision  trees are fitted following  each bootstrap  sample\nand then averaged  afterwards.  Bagging  significantly  reduces  the variance  of the random  forest versus\nthe variance  of any individual  decision  trees. The second  way random  forests reduce overfitting  is\nthat a random  subset of features  is considered  at each split, preventing  the important  features  from\nalways  being present  at the tops of individual  trees.\nRandom  forests are often used due to their versatility,  interpretability  (you can quickly  see feature\nimportance),  quick training  times (they can be trained in parallel),  and prediction  performance.  In\ninterviews,  you'll be asked about how they work versus a decision  tree, and when you would use a\nrandom  forest over other techniques.\nTest Sample  Input\nTree 500\n[Prediction  1 | | Prediction  2 (--) | Preiction 500 _|\nall Average  All Predictions  i\nv\n| Random Forest Predictions  |\nBoosting\nBoosting  is a type of ensemble  model that trains a sequence  of “weak”  models (such as small decision\ntrees), where each one sequentially  compensates  for the weaknesses  of the preceding  models. Such\nweaknesses  can be measured  by the current model’s  error rate, and the relative error rates can be used\nto weigh which observations  the next models should focus on. Each training point within a dataset\nis assigned  a particular  weight and is continually  re-weighted  in an iterative fashion such that points\nthat are mispredicted  take on higher weights in each iteration. In this way, more emphasis is placed\non points that are harder to predict. This can lead to overfitting  if the data is especially  noisy.\nOne example is AdaBoost  (adaptive boosting),  which is a popular technique used to train a model\nbased on tuning a variety of weak learners. That is, it sequentially  combines decision trees with a\nsingle split, and then weights are uniformly  set for all data points. At each iteration, data points are\nre-weighted  according  to whether each was classified  correctly or incorrectly  by a classifier.  At the\nend, weighted  predictions  of each classifier  are combined  to obtain a final prediction.\nAce the Data Science  Interview 99"
  },
  {
    "page_number": 112,
    "content": "CHAPTER  7: MACHINE  LEARNING\nThe generalized  form of AdaBoost  is called gradient boosting. A well-known  form of gradient\nboosting  used in practice  is called XGBoost  (extreme  gradient  boosting).  Gradient  boosting  is similar\nto AdaBoost,  except that shortcomings  of previous  models are identified  by the gradient  rather than\nhigh weight points, and all classifiers  have equal weights instead of having different  weights. In\nindustry,  XGBoost  is used heavily due to its execution  speed and model performance.\nSince random forests and boosting  are both ensemble  methods,  interviewers  tend to ask questions\ncomparing  and contrasting  the two. For example,  one of the most common  interview  questions  1s\n“What is the difference  between  XGBoost  and a random  forest?”\nDimensionality  Reduction\nImagine  you have a dataset with one million rows but two million features,  most of which are null\nacross the data points. You can intuitively  guess that it would be hard to tease out which features  are\npredictive  for the task at hand. In geometric  terms, this situation  demonstrates  sparse data spread  over\nmultiple  dimensions,  meaning  that each data point is relatively  far away from other data points. This\nlack of distance  is problematic,  because  when extracting  patterns  using machine  learning,  the idea of\nsimilarity  or closeness  of data often matters  a great deal. If a particular  data point has nothing  close\nto it, how can an algorithm  make sense of it?\nThis phenomenon  is known as the curse of dimensionality.  One way to address  this problem  is to\nincrease  the dataset size, but often, in practice,  it’s costly or infeasible  to get more training  data.\nAnother  way is to conduct feature selection,  such as removing  multicollinearity,  but this can be\nchallenging  with a very large number  of features.\nInstead, we can use dimensionality  reduction,  which reduces  the complexity  of the problem  with\nminimal loss of important  information.  Dimensionality  reduction  enables you to extract useful\ninformation  from such data, but can sometimes  be difficult or even too expensive,  since the\nalgorithm  we would use would incorporate  so many features.  Decomposing  the data into a smaller\nset ofvariables  1s also useful for summarizing  and visualizing  datasets.  For example,  dimensionality\nreduction  methods  can be used to project  a large dataset  into 2D or 3D space for easier visualization.\nPrincipal  Components  Analysis\nThe most commonly  used method  to reduce  the dimensionality  of a dataset  is principal  components\nanalysis  (PCA).  PCA combines  highly  correlated  variables  into a new, smaller  set of constructs  called\nprincipal  coniponents,  which capture  most of the variance  present  in the data. The algorithm  looks\nfor a small number  of linear combinations  for each row vector to explain the variance  within X.\nFor example,  in the image below, the variation  in the data is largely summarized  by two principal\ncomponents.\nMore specitically,  PCA finds the vector w of weights  such that we can define the following  linear\ncombination:\nsubject  to the following:\ny, is uncorrelated  with y,, var(y  ) is maximized\n100 Ace the Data Science  Interview | Machine  Learning"
  },
  {
    "page_number": 113,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\n0.4--\n0.4}-\n0.4\n0.4\n0.4\n0.4;-\n0.3-\n0.2r\n0.1\n0.0 l l 1 l l N l !\n-10 -8 —6 —4 —2 0 2 4 6\nx\nHence,  the algorithm  proceeds  by first finding  the component  having maximal  variance.  Then, the\nsecond  component  found is uncorrelated  with the first and has the second-highest  variance,  and so on\nfor the other components.  The algorithm  ends with some number  & dimensions  such that\ny,, +, explain  the majority  of k variance,  k << p\nThe final result is an eigendecomposition  of the covariance  matrix of X, where the first principal\ncomponent  is the eigenvector  corresponding  to the largest eigenvalue  and the second principal\ncomponent  corresponds  to the eigenvector  with the second  largest  eigenvalue,  and so on. Generally,\nthe number  of components  you choose is based on your threshold  for the percent  of variance  your\nprincipal  components  can explain.  Note that while PCA 1s a linear dimensionality  reduction  method,\nt-distributed  stochastic  neighbor  embedding  (t-SNE)  is a non-linear,  non-deterministic  method  used\nfor data visualization.\nIn interviews,  PCA questions  often test your knowledge  of the assumptions  (like that the variables  need\nto have  a linear relationship).  Commonly  asked about as well are pitfalls  of PCA, like how it struggles\nwith outliers,  or how it is sensitive  to the units of measurement  for the input features  (data should be\nstandardized).  For more ML-heavy  roles, you may be asked to whiteboard  the eigendecomposition.\nClustering\nClustering  is a popular  interview  topic since it is the most commonly  employed  unsupervised  machine\nlearning  technique.  Recall that unsupervised  learning  means that there is no labeled  training  data, i.e.,\nthe algorithm  is trying to infer structural  patterns  within the data, without a prediction  task in mind.\nClustering  is often done to find “hidden”  groupings  in data, like segmenting  customers  into different\ngroups, where the customers  in a group have similar characteristics.  Clustering  can also be used for\ndata visualization  and outlier identification,  as in fraud detection,  for instance.  The goal of clustering\nis to partition  a dataset into various clusters  or groups by looking  only at the data’s input features.\nIdeally,  the clustered  groups  have two properties:\n¢ Points within a given cluster are similar  (i.e., high intra-cluster  similarity).\n¢ Points in different  clusters  are not similar (i.e., low inter-cluster  similarity).\nAce the Data Science  Interview 101"
  },
  {
    "page_number": 114,
    "content": "CHAPTER  7 : MACHINE  LEARNING\nK-Means  clustering\nA well-known  clustering  algorithm,  k-means  clustering  is often used because  it is easy to interpret  and\nimplement.  It proceeds,  first, by partitioning  a set of data into é distinct clusters and then arbitrarily\nselects centroids  of each of these clusters. It iteratively  updates partitions  by first assigning  points to\nthe closest cluster, then updating  centroids,  and then repeating  this process until convergence.  This\nprocess  essentially  minimizes  the total inter-cluster  variation  across all clusters.\nBefore K-Means After K-Means\nA A Le\nEd ‘\n+ + * * K-Means +\n* * oF GP+ db + Ft. oy ap a ct a, >. a\n+ * i+ Poh\n> ——_—_>\nMathematically,  k-means  clustering  reaches  a solution  by minimizing  a loss function  (also known  as\ndistortion  function).  In this example,  we minimize  Euclidean  distance  (given x, points and centroid\nvalue H,): .\nL- ye al\nj=1 xeS,\nwhere  S_ represents  the particular  cluster.\nThe iterative  process continues  until the cluster assignment  updates fail to improve  the objective\nfunction.  Note that 4-means  clustering  uses Euclidean  distance  when assessing  how close points\nare to one another  and that A, the number  of clusters  to be estimated,  is set by the user and can be\noptimized  if necessary.\nK-means  Alternatives\nOne alternative  to k-means  1s hierarchical  clustering.  Hierarchical  clustering  assigns  data points to\ntheir own cluster  and merges  clusters  that are the nearest  (based on any variety  of distance  metrics)\nuntil there ts only one cluster  left, generally  visualized  using a dendrogram.  In cases where  there is not\na specific  number  of clusters,  or you want a more interpretable  and informative  output,  hierarchical\nclustering  1s more useful than k-means.\nWhile quite similar to A-means,  density clustering  is another  distinct technique.  The most well-\nknown implementation  of this technique  is DBSCAN.  Density  clustering  does not require a number\nof clusters  as a parameter.  Instead,  it infers that number,  and learns to identify  clusters  of arbitrary\nshapes.  Generally,  density  clustering  is more helpful  for outlier  detection  than &-means.\nGaussian  Mixture  Model  (GMM)\nA GMM assumes that the data being analyzed  come from a “mixture”  of & Gaussian/normal\ndistributions,  each having  a different  mean and variance,  where  the mixture  components  are basically\nthe proportion  of  observations  in each group.  Compared  to k-means,  which  is a deterministic  algorithm\nwhere k is set in advance,  GMMs  essentially  try to learn the true value of k.\n102 Ace the Data Science  Interview  | Machine  Learning"
  },
  {
    "page_number": 115,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nFor example,  TikTok  may be on the lookout  for anomalous  profiles,  and can use GMMs  to cluster\nvarious  accounts  based on features  (number  of likes sent, messages  sent, and comments  made)\nand identify  any accounts  whose activity  metrics  don’t seem to fall within the typical user activity\ndistributions.\n‘ Cluster  2\nCluster  1\n\\\n\\\n\\.\\\nt.\n1\\\n|\nISIO  ET. f. ope ;\n| | {—>| 0, +- —h Oo, t+- — 16,\nH, jt. LH,\nCompared  to k-means,  GMMs  are more flexible  because  k-means  only takes into account  the mean\nof a cluster,  while GMMs  take into account  the mean and variance.  Therefore,  GMMs  are particularly\nuseful in cases with low-dimensional  data or where  cluster  shapes  may be arbitrary,  While  practically\nnever asked about for data science interviews  (compared  to k-means),  we brought  up GMMs for\nthose seeking  more technical  ML research  and ML engineering  positions.\nNeural  Networks\nWhile the concepts  behind neural networks  have been around since the 1950s, it’s only in the last\n15 years that they’ve  grown in popularity,  thanks to an explosion  of data being created,  along with\nthe nse of cheap cloud computing  resources  needed to store and process  the massive  amounts  of\nnewly created  data. As mentioned  earlier in the chapter,  if your résumé has any machine  learning\nprojects  involving  deep learning  experience,  then the technical  details behind neural networks  will\nbe considered  fair game by most interviewers.  But for a product  data science position  or a finance\nrole (where  data can be very noisy, so most models  are not purely neural networks),  don’t expect to\nbe bombarded  with tough neural network  questions.  Knowing  the basics of classical  ML techniques\nshould  suffice.\nWhen neural nets are brought up during interviews,  questions  can range anywhere  from qualitative\nassessments  on how deep learning compares to more traditional  machine learning models to\nmathematical  details on gradient descent and backpropagation.  On the qualitative  side, it helps\nto understand  all of the components  that go into training neural networks,  as well as how neural\nnetworks  compare  to simpler  methods.\nPerceptron\nNeural networks  function  in a way similar to biological  neurons.  They take in various inputs (al input\nlayers), weight these inputs, and then combine the weighted inputs through a linear combination\n(much like linear regression).  If the combined  weighted output is past some threshold set by an\nactivation  function,  the output is then sent out to other layers. This base unit is generally  referred to as\nAce the Data Science  Interview 103"
  },
  {
    "page_number": 116,
    "content": "CHAPTER  7: MACHINE  LEARNING\na perceptron.  Perceptrons  are combined  to form neural networks,  which is why they are also known as\nmulti-layer  perceptrons  (MLPs).\nLearninga process\nError\nee ed o*rLey (output)we\" _!\n“wi | Weighted  Sum Activation\nBO weions\nInputs\nSingle Layer Perception\nWhile the inputs for a neural network  are combined  via a linear combination,  often, the activation\nfunction  is nonlinear.  Thus, the relationship  between  the target variable  and the predictor  features\n(variables)  frequently  ends up also being nonlinear.  Therefore,  neural networks  are most useful when\nrepresenting  and learning  nonlinear  functions.\nFor reference,  we include a list of common  activation  functions  below. The scope of when to use\nwhich activation  function  is outside  of this text, but any person interviewing  for an ML-intensive  role\nshould know these use cases along with the activation  function’s  formula.\nActivation  Function Equation Example 1D Graph\nUnit Step (Heaviside) 0, z< 0, | Perception  variant A\n(z)= ¢ 0.5,z=0, oo\n1, z>0, |\nSign (Signum) —1, z<0, | Perception  variant A.\no(z)-< 0.5,z=0, —\n1, z>0,\nLinear Adaline,  linear\no(z) =z regression a\nPiece-wise  linear 1, z>0, | Support  vector t\no(z)= 1 z4+%, —Kez<y,  | Machine sre\n0, ZS —-‘%, |\nLogistic  (sigmoid) 1 Logistic  regression\n(Zz) = tne? Multi-layer  NN x\nHyperbolic  tangent ee? Multi-layer\nb(z) — after Neural Network f _\nINA Awos thn PNw Ee CBDR"
  },
  {
    "page_number": 117,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nRectifier,  ReLU o(z) = max (0, z) Multi-layer\n(Rectifier  Linear  Unit) Neural  Network\nRectifier,  softplus o(z) = In (1 + e”) Multi-layer\nNeural! Network\nIn neural  networks,  the process  of receiving  inputs and generating  an output  continues  until an output\nlayer is reached.  This is generally  done in a forward  manner,  meaning  that layers process  incoming\ndata in a sequential  forward  way (which  is why most neural networks  are known  as “feed-forward”.\nThe layers of neurons  that are not the input or output layers are called the hidden layers (hence the\nname “deep learning”  for neural networks  having  many of these). Hidden  layers allow for specific\ntransformations  of the data within each layer. Each hidden layer can be specialized  to produce  a\nparticular  output  — for example,  in a neural  network  used for navigating  roads, one hidden  layer may\nidentify  stop signs, and another  hidden layer may identify  traffic lights. While those hidden layers\nare not enough to independently  navigate  roads, they can function  together  within a larger neural\nnetwork  to drive better than Nick at age 16.\nSf  Neuron\nInput Data Output\nOutput  Layer\nHidden  Layer\nBackpropagation\nThe learning process for neural networks is called backpropagation.  This technique  modifies the\nweights  of the neural network  iteratively  through  calculation  of deltas between  predicted  and expected\noutputs. After this calculation,  the weights are updated  backward  through earlier layers via stochastic\ngradient  descent. This process continues  until the weights that minimize  the loss function  are found.\nFor regression  tasks, the commonly  used loss function  to be optimized  is mean squared error, whereas\nfor classification  tasks the common loss function used is cross-entropy.  Given a loss function  L, we\ncan update the weights w’ through the chain rule of the following  form, where z is the model’s output\n(and the best guess of our target variable  y):\nAran tha Data Criancea  Interview 108"
  },
  {
    "page_number": 118,
    "content": "CHAPTER  7: MACHINE  LEARNING\nOL(z,y)  _ OL (z,y) ox , dz\now —«K oz Ow\nand the weights  are updated  via:\ndL (z,y)w= w-a———ow\nFor ML-heavy  roles, we’ve seen interviewers  expect candidates  to explain the technical details\nbehind  basic backpropagation  on a whiteboard,  for basic methods  such as linear regression  or logistic\nregression.\nInterviewers  also like to ask about the hyperparameters  involved  in neural networks.  For example,\nthe amount that the weights  are updated  during each training  step, a, is called the /earning  rate. \\t\nthe learning  rate is too small, the optimization  process  may freeze. Conversely,  if the learning  rate 1s\ntoo large, the optimization  might converge  prematurely  at a suboptimal  solution.  Besides  the learning\nrate, other hyperparameters  in neural networks  include the number  of hidden layers, the activation\nfunctions  used, batch size, and so on. For an interview,  it’s helpful  to know how each hyperparameter\naffects a neural network’s  training  time and model performance.\nTraining  Neural  Networks\nBecause  neural networks  require  a gargantuan  amount  of weights  to train, along with a considerable\namount  of hyperparameters  to be searched  for, the training  process  for a neural network  can run into\nmany problems.\nGeneral  Framework\nOne issue that can come up in training  neural nets 1s the problem  of vanishing  gradients.  Vanishing\ngradients  refers to the fact that sometimes  the gradient  of the loss function  will be tiny, and may\ncompletely  stop the neural network  from training  because  the weights  aren’t updated  properly.  Since\nbackpropagation  uses the chain rule, multiplying  ” small numbers  to compute  gradients  for early\nlayers in a network  means that the gradient  gets exponentially  smaller  with more layers. This can\nhappen particularly  with traditional  activation  functions  like hyperbolic  tangent,  whose gradients\nrange between  zero and one. The opposite  problem,  where activation  functions  p create large\nderivatives,  is known  as the exploding  gradient  problem.\nOne common  technique  to address  extremes  in gradient  values is to allow gradients  from later layers\nto directly  pass into earlier  layers without  being multiplied  many times ---- something  which residual\nneural networks  (ResNets)  and LSTMs  both utilize. Another  approach  to prevent  extremes  in the\ngradient  values is to alter the magnitude  of the gradient  changes  by changing  the activation  function\nused (for example,  ReLU).  The details behind these methods  are beyond  this book’s scope but are\nworth looking  into for ML-heavy  interviews.\nTraining  Optimization  Techniques\nAdditionally,  there are quite a few challenges  in using vanilla  gradient  descent  to train deep learning\nmodels.  A few examples  include getting trapped  in suboptimal  local minima  or saddle points, not\nusing a good learning  rate, or dealing  with sparse  data with features  of different  frequencies  where  we\nmay not want all features  to update  to the same extent.  To address  these concerns,  there are a variety\nof optimization  algorithms  used.\nMomentum  is one such optimization  method  used to accelerate  learning  while using SGD. While\nusing SGD, we can sometimes  see small and noisy gradients.  To solve this, we can introduce  a new\n106 Ace the Data Science  Interview  | Machine  ltanrinn"
  },
  {
    "page_number": 119,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nparameter,  velocity,  which is the direction  and speed at which the learning  dynamics  change.  The\nvelocity  changes  based on previous  gradients  (in an exponentially  decaying  manner)  and increases\nthe step size for learning  in any iteration,  which helps the gradient  maintain  a consistent  direction  and\npace throughout  the training  process.\nTransfer  Learning\nLastly, for training  neural networks,  practitioners  use or repurpose  a pre-trained  layer (components\nof a model that have already  been trained  and published).  This approach  is called transfer  learning\nand is especially  common  in cases where models  require  a large amount  of data (for example,  BERT\nfor language  models and ImageNet  for image classification).  Transfer  learning  is beneficial  when\nyou have insufficient  data for a new domain,  and there is a large pool of existing  data that can be\ntransferred  to the problem  of interest.  For example,  say you wanted  to help Jian Yang from the TV\nShow Silicon Valley build an app to detect whether  something  was a hot dog or not a hotdog.  Rather\nthan just using your !00 images of hot dogs, you can use ImageNet  (which was trained on many\nmillions  of images)  to get a great model  right off the bat, and then layer on any extra specific  training\ndata you might have to further  improve  accuracy.\nAddressing  Overfitting\nDeep neural networks  are prone to overfitting  because  of the model complexity  (there are many\nparameters  involved).  As such, interviewers  frequently  ask about the variety of techniques  which\nare used to reduce the likelihood  of a neural network  overfitting.  Adding  more training  data is the\nsimplest  way to address  variance  if you have access to significantly  more data and computational\npower  to process  that data. Another  way is to standardize  features  (so each feature  has 0 mean and unit\nvariance),  since this speeds up the learning  algorithm.  Without  normalized  inputs,  each feature  takes\non a wide range of values, and the corresponding  weights  for those features  can vary dramatically,\nresulting  in larger updates in backpropagation.  These large updates may cause oscillation  in the\nweights  during the learning  stage, which causes overfitting  and high variance.\nBatch normalization  is another technique  to address overfitting.  In this process,  activation  values\nare normalized  within a given batch so that the representations  at the hidden layers do not vary\ndrastically,  thereby  allowing  each layer of a network  to learn more independently  of one another.  This\nis done for each hidden neuron, and also improves  training  speed. Here, applying  a standardization\nprocess  similar  to how inputs are standardized  is recommended.\nLastly, dropout is a regularization  technique  that deactivates  several neurons randomly  at each\ntraining step to avoid overfitting.  Dropout enables simulation  of different architectures,  because\ninstead of a full original neural network,  there will be random nodes dropped at each layer. Both\nbatch normalization  and dropout help with regularization  since the effects they have are similar to\nadding  noise to various  parts of the training  process.\nTypes of Neural  Networks\nWhile a deep dive into the various neural network architectures  is beyond the scope of this book,\nbelow, we jog your memory  with a few of the most popular  ones along with their applications.  For\nML-heavy  roles, interviewers  tend to ask about the different  layer types used within each architecture,\nand to compare  and contrast  each architecture  against  one another.\nCNNs\nConvolutional  neural networks  (CNNs)  are heavily used in computer  vision because  they can capture\nthe spatial dependencies  of an image through  a series  of filters. Imagine  you were looking  al a picture\nARN tha Nata Criancea  Intarviaw 107"
  },
  {
    "page_number": 120,
    "content": "CHAPTER  7 : MACHINE  LEARNING\nof some traffic lights. Intuitively,  you need to figure out the components  of the lights (1.e., red, green,\nyellow) when processing  the image. CNNs can determine  elements  within that picture by looking at\nvarious  neighborhoods  of the pixels in the image. Specifically,  convolution  layers can extract features\nsuch as edges, color, and gradient  orientation.  Then, pooling  lavers apply a version  of dimensionality\nreduction  in order to extract the most prominent  features that are invariant  to rotation and position.\nLastly, the results are mapped  into the final output by a fully connected  layer.\nLd — —CAR&& + — TRUCK\na — VAN\na a a\n6 ~ A :s A 0 O —BICYCLE\n; , Fully SoftmaxInput olution + ReLU Poolinp Conv n ing Flatten Connected\nNe —~ \\ —/ a “Yo\nFEATURE  LEARNING CLASSIFICATION\nRNNs\nRecurrent  neural networks  (RNNs) are another common  type of neural network.  In an RNN, the\nnodes form a directed  graph along a temporal  sequence  and use their internal  state (called  memory).\nRNNs are often used in learning  sequential  data such as audio or video — cases where the current\ncontext depends  on past history. For example,  say you are looking through  the frames of a video.\nWhat will happen in the next frame is likely to be highly related to the current frame, but not as\nrelated  to the first frame of the video. Therefore,  when dealing  with sequential  data, having  a notion\nof memory  is crucial for accurate  predictions.  In contrast  to CNNs,  RNNs  can handle  arbitrarily  input\nand output lengths  and are not feed-forward  neural networks,  instead  using this internal  memory  to\nprocess  arbitrary  sequences  of data.\nLSTMs\nLong Short-Term  Memory  (LSTMs)  are a fancier  version  of RNNs. In LSTMs,  a common  unit is\ncomposed  of a cell, an input gate (writing  to a cell or not), an output gate (how much to write to a\ncell), and a forget gate (how much to erase from a cell). This architecture  allows for regulating  the\nflow of information  into and out of any cell. Compared  to vanilla  RNNs,  which only learn short-term\ndependencies,  LSTMs  have additional  properties  that allow them to learn long-term  dependencies.\nTherefore,  in most real-world  scenarios,  LSTMs  are used instead  of RNNs.\nReinforcement  Learning\nReinforcement  learning  (RL) is an area of machine  Jearning  outside  of  supervised  and unsupervised\nlearning.  RL is about teaching  an agent to learn which decisions  to make in an environment  to\nmaximize  some reward function.  The agent takes a series of actions throughout  a variety of states\nand is rewarded  accordingly.  During  the learning  process,  the agent receives  feedback  based on the\nactions  taken and aims to maximize  the overall  value acquired.\n108 Ace the Data Science  Interview | Machine  Learning"
  },
  {
    "page_number": 121,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nThe main components  of an RL algorithm  include:\n* Reward  function:  defines  the goal of the entire RL problem  and quantifies  what a “good”  or\n“bad” action is for the agent in any given state.\n° Policy:  defines  how the agent picks its actions  by mapping  states to actions.\n¢ Model:  defines  how the agent predicts  what to do next as the agent understands  the environment\n‘— given a state and action,  the model will predict  the reward  and the next state.\n¢ Value  function:  predicts  the overall  expected  future  reward  discounted  over time; it is consistently\nre-estimated  over time to optimize  long-term  value.\nRL is most widely  known  for use cases in gaming  (AlphaGo,  chess, Starcraft,  etc.) and robotics.  It is\nbest used when the problem  at hand is one of actions  rather than purely predictions  (that is, you do\nnot know what constitutes  “good”  actions  — supervised  learning  assumes  you already  know what\nthe output  should  be). Unless  you have particular  projects  or experience  with reinforcement  learning,\nit won’t  be brought  up in data science  interviews.\nThe End-to-End  ML Workflow\nEnd-to-end  machine  learning  questions  are asked in interviews  to see how well you apply machine\nlearning  theory to solve business  problems.  It isn’t just about confronting  a real-world  problem  like\n“How would you design Uber’s surge pricing algorithm?”  and then jumping  to a technique  like\nlinear regression  or random  forests. Instead,  it’s about asking the right questions  about the business\ngoals and constraints  that inform the machine  learning  system design. It’s about walking  through\nhow you’d explore  and clean the data and the features  you would try to engineer.  It’s about picking\nthe right model evaluation  metrics  and modeling  techniques,  contextualizing  model performance  in\nterms of business  impact,  mentioning  model deployment  strategies,  and much, much more.\nTo help you solve these all-encompassing  problems  -—- something  most ML-theory  textbooks  don’t\ncover —-- we walk you through  the entire machine  learning  workflow  below. However,  to really ace\nthese open-ended  ML problems  come interview  time, also read Chapter  11: Case Studies.  In addition\nto tips on dealing with open-ended  problems,  the case study chapter includes  ML case interview\nquestions  that force you to apply the concepts  detailed  below.\nStep 1: Clarify  the Problem  and Constraints\nIf you frame the business  and product  problem  correctly,  you've done half the work. That’s because\nit’s easy to throw random ML techniques  at data — but it’s harder to understand  the business\nmotivations,  technical  requirements,  and stakeholder  concerns  that ultimately  affect the success  of a\ndeployed  machine  leaming  solution.  As such, make sure to start your answer  by discussing  what the\nproblem  at hand is, the business  process  and context surrounding  the problem,  any assumptions  you\nmight have, and what prospective  stakeholder  concerns  are likely to be.\nSome questions  you can use to clarify the problem  and the constraints  include:\n¢ What is the dependent  variable  we are trying to model? For example,  if we are building  a user\nchurn model, what criteria  are we using to define churn in the first place?\n* — How has the problem  been approached  in the past by the business?  Is there a baseline  performance\nwe can compare against? How much do we need to beat this baseline for the project to be\nconsidered  a success?\nARK tha Nantn Crianra  Intarviavws 1N9"
  },
  {
    "page_number": 122,
    "content": "CHAPTER  7: MACHINE  LEARNING\n* Is ML even needed? Maybe  a simple heuristics  or a rules-based  approach  works well enough?\nOr perhaps  a hybrid approach  with humans  in the loop would work best?\n¢ Is it even legal or ethical to apply ML to this problem?  Are there regulatory  issues at play\ndictating  what kinds of data or models you can use? For example,  lending institutions  cannot\nlegally  use some demographic  variables  like race.\n¢ How do end users benefit  from the solution,  and how would they use the solution  (as a standalone,\nor an integration  with existing  systems)?\n¢ Is there a clear value add to the business from a successful  solution?  Are there any other\nstakeholders  who would be affected?\n¢ If an incorrect prediction  is made, how will it impact the business?  For example, a spam\nemail making its way into your inbox isn’t as problematic  as a high-risk  mortgage  application\naccidentally  being approved.\n* Does ML need to solve the entire problem,  end-to-end,  or can smaller decoupled  systems be\nmade to solve sub-problems,  whose output is then combined?  For example,  do you need to make\na full self-driving  algorithm,  or separate  smaller algorithms  for environment  perception,  path\nplanning,  and vehicle  control?\nOnce you’ ve understood  the business  problem  that your ML solution  is trying to solve, you can clarify\nsome of the technical  requirements.  Aligning  on the technical  requirements  is especially  important\nwhen confronted  with a ML systems  design  problem.  Some  questions  to ask to anchor  the conversation:\n¢ What’s the latency needed?  For example,  search autocomplete  is useless if it takes predictions\nlonger  to load than it takes users to type out their full query. Does every part of the system  need\nto be real time—while  inference  may need to be fast, can training  be slow?\n¢ Are there any throughput  requirements?  How many predictions  do you need to serve every\nminute?\n¢ Where  1s this model being deployed?  Does the model need to fit on-device?  If so, how big is too\nbig to deploy?  And how costly is deployment?  For example,  adding  a high-end  GPU to a car is\nfeasible  cost-wise,  but adding  one to a drone might not be.\nWhile spending  so much time on problem  definition  may seem tedious,  the reality  is that defining  the\nright solution  for the right problem  can save you many weeks  of technical  work and painful  iterations\nlater down the road. That’s why interviewers,  when posing  open-ended  ML problems,  expect  you to\nask the right questions  —- ones that scope down your solution.  By clarifying  these constraints  and\nobjectives  up front, you make better decisions  on downstream  steps of the end-to-end  workflow.  To\nfurther  your business  and product  clarification  skills, read the sections  on product  sense and company\nresearch  in Chapter  10: Product  Sense.\nAnd one last piece of advice:  don’t go overboard  with the questions!  Remember,  this is a time-bound\nInterview,  SO make sure your questions  and assumptions  are reasonable  and relevant  (and concise).\nYou don’t want to be like a toddler  and ask 57 questions  without  getting  anywhere.\nStep 2: Establish  Metrics\nOnce you've  understood  the stakeholder  objectives  and constraints  imposed,  it’s best to pick simple,\nobservable,  and attributable  metrics  that encapsulate  solving  the problem.  Note that sometimes  the\nbusiness  1s only interested  in optimizing  their existing  business  KPIs (for example,  the time to resolve\na customer  request).  In that case, you need to be able to align your model performance  metrics  with\n110 Ace the Data Science  Interview | Machine  Learning"
  },
  {
    "page_number": 123,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nsolving  the business  problem.  For example,  for a customer  support  request  classification  model, a\n90% model accuracy  means that 50% of the customer  tickets that previously  needed  to be rerouted\nnow end up in the right place, resulting  in a 10% decrease  in time to resolution.\nIn real-world  scenarios,  it’s best to opt for a single  metric  rather  than picking  multiple  metrics  to capture\ndifferent  sub-goals.  That’s  because  a single metric makes  it easier  to rank model performance.  Plus, it’s\neasier to align the team around  optimizing  a single number.  However,  in interview  contexts,  it may be\nbeneficial  to mention  multiple  metrics  to show you’ve  thought  about the various  goals and trade-offs\nyour ML solution  needs to satisfy.  As such, in an interview,  we recommend  you start your answer  with\na single metric,  but then hedge your answer  by mentioning  other potential  metrics  to track.\nFor example,  if posed a question  about evaluation  metrics  for a spam classifier,  you could start off\nby talking about accuracy,  and then move on to precision  and recall as the conversation  becomes\nmore nuanced.  In an effort to optimize  a single metric,  you could recommend  using the F-1 score. A\nnuanced  answer  could also incorporate  an element  of satisficing  — where a secondary  metric is just\ngood enough.  For example,  you could  optimize  precision  @ recall 0.95 — i.e., constraining  the recall\nto be at least 0.95 while optimizing  for precision.  Or you could suggest  blending  multiple  metrics\ninto one by weighting  different  sub-metrics,  such as false positives  versus false negatives,  to create\na final metric to track. This is often known  as an OEC (overall!  evaluation  criterion),  and gives you a\nbalance  between  different  metrics.\nOnce you’ve picked a metric, you need to establish  what success  looks like. While for a classifier,\nyou might desire 100% accuracy,  is this a realistic  bar for measuring  success?  Is there a threshold\nthat’s good enough?  This is why inquiring  about baseline  performance  in Step 1 becomes  crucial. If\npossible,  you should use the performance  of the existing  setup for comparison  (for example,  if the\naverage  time to resolution  for customer  support  tickets is 2 hours, you could aim for | hour —- not\na 97% ticket classification  accuracy).  Note: in real-world  scenarios,  the bar for model performance\nisn’t as high as you’d think to still have a positive  business  impact.\nBe sure to voice all these metric considerations  to your interviewer  so that you can show you’ve\nthought  critically  about the problem.  For more guidance,  read Chapter 10: Product Sense, which\ncovers the nuances  and pitfalls  of metric selection.\nStep 3: Understand  Your Data Sources\nYour machine  learning  model is only as good as the data it sees; hence, the phrase “garbage  in, garbage\nout.” For classroom  projects  or Kaggle competitions,  there isn’t much you can do since you usually\nhave  a fixed dataset,  and it’s all about fitting a model that maximizes  some metric. However,  in the real\nworld, you have leeway  in what data you use to solve the business  problem.  As such, clearly articulate\nwhat data sources you would prefer to solve the interview  problem.  While it’s intuitive to use the\ninternal  company  data relevant  to the problem  at hand, that’s not the be-all end-all of data sourcing.\nFor open-ended  ML questions,  especially  at startups that might be testing your scrappiness,  you\nshould think outside the box regarding  what data to use. Data sources to consider:\n¢ Can you acquire  more data by crowdsourcing  it via Amazon  Mechanical  Turk?\n¢ Can you ask users for data as part of the user onboarding  process?\n¢ Can you buy second-  and third-party  datasets?\n¢ Can you ethically  scrape the data from online sources?\n¢ Can you send your unlabeled  internal data off to a Jabeling  and annotation  service?\nAce the Data Science  Interview 111"
  },
  {
    "page_number": 124,
    "content": "CHAPTER  7 : MACHINE  LEARNING\nTo boost model performance,  it might not be about collecting  more data generally.  Instead, you can\nintentionally  source more examples  of edge cases via data augmentation  or artificial  data synthesis.\nFor example,  suppose  your traffic light detector  struggles  in low-contrast  situations.  You could make\na version of your training images that has less contrast in order to give your neural network more\npractice  on these trickier  photos. Taken to the extreme,  you can even simulate  the entire environment,\nas is common in the self-driving  car industry. Simulation  is used in the autonomous  vehicle space\nbecause encountering  the volume of rare and risky situations  needed to adequately  train a model\nbased on only real-world  driving  is infeasible.\nFinally,  do you understand  the data? Questions  to consider:\n¢ How fresh is the data? How often will the data be updated?\n¢ Is there a data dictionary  available?  Have you talked to subject matter experts about it?\n¢ How was the data collected?  Was there any sampling,  selection,  or response  bias?\nStep 4: Explore  Your Data\nA good first step in exploratory  data analysis  is to profile the columns  at first glance:  which ones might\nbe useful? Which ones have practically  no variance  and thus wouldn’t  offer up any real predictive\nvalue?  Which  columns  look noisy?  Which  ones have  a lot of missing  or odd values?  Besides  skimming\nthrough  your data, also look at summary  statistics  like the mean, median,  and quantiles.\n“The greatest  value of a picture  is when it forces\nus to notice what we never expected  to see.”\n—John  Tukey\nBecause  a picture  is worth  a thousand  words,  visualizing  your data 1s also a crucial  step in exploratory\ndata analysis.  For columns  of interest,  you want to visualize  their distributions  to understand  their\nstatistical  properties  like skewness  and kurtosis.  Certain  features  (e.g., age, weight)  may be better\nvisualized  with a histogram  through  binning.  It also helps to visualize  the range of continuous  variables\nand plot categories  of categorical  variables.  Finally,  you can visually  inspect  the basic relationships\nbetween  variables  using a correlation  matrix. This can help you quickly spot which variables  are\ncorrelated  with one another,  as well as what might be correlated  with the target variable  at hand.\nStep 5: Clean Your Data\nDid you get suckered  into data science  and machine  learning  because  you thought  most of your time\nwould be spent doing sexy data analysis  and modeling  work? Don’t worry, we got fooled too! The\njoke (and grievance)  that most data scientists  spend 80% of their time cleaning  data stems from a\nfrustrating  truth: if you feed garbage  data into a model, you get garbage  out. Between  the logging\nIssues, missing  values,  data entry errors, data merges  gone wrong,  changing  schemas  due to product\nchanges,  and columns  whose meaning  changes  over time, there are many reasons  why your data\nmight be a hot mess. That's precisely  why  a critical  step before modeling  is data munging.\nOne aspect of data munging  is dropping  irrelevant  data or erroneously  duplicated  rows and columns.\nYou should also handle incorrect  values that don’t match up with the supposed  data schema. For\nexample,  for fields containing  human-inputted  data, there is often a pattern  of errors  or typos that you\ncould find and then use to clean up.\nTo handle missing  data, you should first understand  the root cause. Based on the results, several\ntechniques  to deal with missing  values include:\n¢ Imputing  the missing  values  via basic methods  such as column  mean/median\n112 Ace the Data Science  Interview  | Machine  Learning"
  },
  {
    "page_number": 125,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\n* Using  a model  or distribution  to impute  the missing  data\n* Dropping  the rows with missing  values  (as a last resort)\nAnother  critical data cleaning  step is dealing  with outliers.  Outliers  may be due to issues in data\ncollection,  like manual  data entry issues or logging  hiccups.  Or maybe they accurately  reflect the\nactual data. Outliers  can be removed  outright,  truncated,  or left as is, depending  on their source  and\nthe business  implications  of including  them or not.\nNote that outliers  may be univariate,  while others are multivariate  and require  looking  over many\ndimensions.  For an example  of a multivariate  outlier,  consider  a dataset  of human  ages and heights.\nA 4-year-old  human  wouldn’t  be strange,  a 5-foot-tall  person  isn’t odd, but a 4-year-old  that’s 5-feet\ntall would  either  be an outlier,  data entry error, or a Guinness  world record holder.\nStep 6: Feature  Engineering\nFeature  engineering  is the art of presenting  data to machine  learning  models  in the best way possible.\nOne part of feature engineering  is feature selection:  the process  of selecting  a relevant  subset of\nfeatures  for model construction,  based on domain  knowledge.  The other is feature preprocessing:\ntransforming  your data in a way that allows an algorithm  to learn the underlying  structure  of the\ndata without  having  to sift through  noisy inputs. Careful  feature  selection  and feature  processing  can\nboost model performance  and let you get away with using simpler  models.  The specific  workflows\nfor feature  engineering  depend  on the type of data involved.\nFor quantitative  data, several  common  operations  are performed:\n¢ = Transformations:  applying  a function  (like log, capping,  or flooring)  can help when the data is\nskewed  or when you want to make the data conform  to more standard  statistical  distributions  (a\nrequirement  for certain  models).\ne Binning:  also known as discretization  or bucketing,  this process breaks down a continuous\nvariable  into discrete  bins, enabling  a reduction  of noise associated  with the data.\n¢ Dimensionality  Reduction:  to generate  a reduced  set of uncorrelated  features,  you can use a\ntechnique  like PCA.\nAlso, standardize  and scale data as needed: this is especially  important  for machine learning\nalgorithms  that may be sensitive  to variance in the feature values. For example,  K-means  uses\nEuclidean  distance  to measure  distances  between  points and clusters,  so all features  need comparable\nvariances.  To normalize  data, you can use min/max  scaling,  so that all data lies between  zero and one.\nTo standardize  features,  you can use z-scores,  so that data has a mean of zero and a variance  of one.\nFor categorical  data, two common  approaches  are:\n© One-hot  encoding:  turns each category  into a vector of all zeroes, with the exception  of a “one”\nfor the category  at hand\n¢ Hashing:  turns the data into a fixed dimensional  vector using a hashing  function;  great for when\nfeatures have a very high cardinality  (large range of values) and the output vector of one-hot\nencoding  would be too big\nWhile you might not be asked about NLP during most interviews,  if it’s listed on your resume, It’s\ngood to know  a few text preprocessing  techniques  as well:\n¢ Stemming:  reduces  a word down to a root word by deleting  characters  (for example,  turning the\nwords “liked”  and “likes” into “like’’).\nAce the Data Science  Interview 113"
  },
  {
    "page_number": 126,
    "content": "CHAPTER  7 : MACHINE  LEARNING\nLemmatization:  somewhat  similar  to stemming,  but instead of just reducing  words into roots, it\ntakes into account  the context  and meaning  of the word (for example,  it would turn “caring”  to\n“care,”  whereas  stemming  would turn “caring”  to “car’’).\nFiltering:  removes  “stop words”  that don’t add value to a sentence  — like “the” and “a”, along\nwith removing  punctuation.\nBag-of-words:  represents  text as a collection  of words by associating  each word and its frequency.\nN-grams:  an extension  of bag-of-words  where we use N words in a sequence.\nWord embeddings:  a representation  that converts  words to vectors that encode the meaning  of\nthe word, where words that are closer in meaning  are closer in vector space (popular  methods\ninclude  word2vec  and GloVe).\nStep 7: Model Selection\nGiven the business  constraints,  evaluation  metrics,  and data sources  available,  what types of models\nmake the most sense to try? Factors  to consider  when selecting  a model include:\nTraining  & Prediction  Speed: for example,  linear regression  is much quicker than neural\nnetworks  for the same amount  of data\nBudget:  neural networks,  for instance,  can be computationally  intensive  models  to train\nVolume  & Dimensionality  of Data: for example,  neural networks  can handle  large amounts  of\ndata and higher-dimensional  data versus  k-NN)\nCategorical  vs. Numerical  Features:  for example,  linear regression  cannot handle categorical\nvariables  directly,  as they need to be one-hot  encoded,  versus  trees (which  can generally  handle\nthem directly)\nExplainability:  choosing  interpretable  models  like linear regression  may be favorable  to “black\nbox” neural networks  due to regulatory  concerns  or their ease of debugging\nBelow  is a quick cheat sheet to also consider:\nMachine  Learning  Algorithms  Cheat  Sheet\nUnsupervised  Leaming:  ClusteringSTART\nk-means k-modes\nYES Nof NO ves] nS Topic « -YESe YESGaussian Prefer | intra DIMension opic «-'*5. ati, — Latent Dirichiet\nMixture Model — Probability < S#egorca! Hierarchical | (ERIM?) Modeling — Probabilistic  —> \"AT ce\nvest YES NOY. _ NOJ\n. os Principal Singular  ValueNO - Needto “NO .): ice Have Component iti DBSCAN <— Specify  k <— Hierarchical Respurises an eee Decomposition\nSupervised  Learning:  Classification\nY\nLinearSVM N° Datais NO _, SPEED Speedor | M@ERICIM | --< SPEED Decision T\n| — Too Large *~ Expleinable  <— Aecuracy Numeric eto Naive Bayes i i Accuracy Linear  Regression\nNaive Bayes DecisionTree  § KemelSVM Random  Forest\nLogistic _ Random Forest Neural  NetworkRegression ‘Neural  Network Gradient\nGradient Boosting  Tree\nBoosting  Tree\n114 Ace the Data Science  Interview  | Machine  Learning"
  },
  {
    "page_number": 127,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nStep 8: Model  Training  & Evaluation\nSo you picked a type of model, or at least narrowed  it down to a few candidates.  At this point,\ninterviewers  will expect  you to talk about how to train the model.  This is a good time to mention  the\ntrain-validation-test  split, cross-validation,  and hyper-parameter  tuning.\nYou might also be asked how to compare  your model to other models,  how to learn its parameters,\nand how you'd  know if its performance  is good enough  to ship. They might  ask about edge cases, like\nhandling  biased  training  data, how you’d assess feature  importance,  or dealing  with data imbalances.\nThe primary  ways to deal with these issues, like picking  the right model evaluation  metric, using\na validation  set, regularizing  your models to avoid overfitting,  and using learning  curves to find\nperformance  plateaus,  are explained  in-depth  earlier  in the chapter.\nOn the flip side, if you are dealing  with so much data that your models  are taking forever  to train,\nthen it is worth looking  into various sampling  techniques.  The most common  ones in practice  are\nrandom  sampling  (sampling  with or without  replacement)  and stratified  sampling  (sampling  from\nspecific  groups among the entire population).  In addition,  for imbalanced  datasets,  undersampling\nand oversampling  techniques  are frequently  used (SMOTE,  for example).  It is important  to make sure\nyou understand  the sampling  method  at hand because  a wrong  sampling  strategy  may lead to incorrect\nresults.\nStep 9: Deployment\nSo, now that you’ve  picked  out a model,  how do you deploy it? The process  of operationalizing  the\nentire deployment  process is referred  to as “MLOps”  — when DevOps  meets ML. Two popular\ntools in this space are Airflow  and MLFlow.  Because  frameworks  come and go, and many large tech\ncompanies  use their own internal  versions  of these tools, it’s rare to be asked about these specific\ntechnologies  in interviews.  However,  knowledge  of high-level  deployment  concepts  1s still helpful.\nGenerally,  systems  are deployed  online,  in batch, or as a hybrid  of the two approaches.  Online  means\nlatency  is critical,  and thus model  predictions  are made in real time. Since model predictions  generally\nneed to be served in real time, there will typically  be a caching  layer of cached features.  Downsides\nfor online deployment  are that it can be computationally  intensive  to meet latency  requirements  and\nrequires  robust infrastructure  monitoring  and redundancy.\nBatch means predictions  are generated  periodically  and is helpful for cases where you don’t need\nimmediate  results (most recommendation  systems,  for example)  or require high throughput.  But the\ndownside  is that batch predictions  may not be available  for new data (for example,  a recommendation\nlist cannot be updated  until the next batch is computed).  Ideally, you can work with stakeholders  to\nfind the sweet spot, where a batch predictor  is updated frequently  enough to be “good enough”  to\nsolve the problem  at hand.\nOne deployment  issue worth bringing up, that’s common to both batch and online ML systems,\nis model degradation.Models  degrade because the underlying  distributions  of data for your model\nchange. For a concrete  example,  suppose  you were working  for a clothing  e-commerce  site and were\ntraining  a product  recommendation  model in the winter time. Come summer,  you might accidentally\nbe recommending  Canada  Goose  jackets in July —-- not a very relevant  product  suggestion  to anyone\nbesides  Drake.\nThis feature drift leads to the phenomenon  known as the training-serving  skew, where there’s a\nperformance  hit between  the model in training  and evaluation  time versus when the model is served\nin production.  To show awareness  for the training-serving  skew issue, be sure to mention to your\nAce the Data Science  Interview 115"
  },
  {
    "page_number": 128,
    "content": "CHAPTER  7 : MACHINE  LEARNING\ninterviewer  how often you'd retrain a model, what events might trigger a model refresh, and how\nmuch new data to use versus how much to rely on historical  data. Also, mention how you'd add\nlogging  to monitor  and catch model degradation.\nStep 10: iterate\nDeploying  a model isn’t the end of the machine learning workflow.  Business objectives  change.\nEvaluation  metrics change. Data features change. As such, you should plan to iterate on deployed\nmodels.\nA big part of knowing  how to iterate on your system can be learned via error analysis  —- the act of\nanalyzing  the wrong predictions  manually.  By looking  at bad predictions  and bucketing  them into the\ntypes of reasons  they occurred,  you can better prioritize  what projects  to work on next. For example,\nsay our traffic light detector  tended to mislabel  photos taken during rain — this tells you that you\nshould collect more images in inclement  weather. Or maybe this tells you to source whether it’s\nraining  or not as a feature  for your model.\nBy focusing  on iterating  your design,  and continuously  seeking  ways for your system  to improve,  you\ncan generate  increasing  amounts  of business  value with your deployed  models.\nMachine  Learning  Interview  Questions\nAs mentioned  in the introduction,  most machine  learning  interview  questions  take the simplistic\nform of “What does term XY mean?”  or “How does technique  Y work?”  To make the 35 interview\nquestions  more useful to you, dear reader, we intentionally  shied away from including  too many\nboring Google-able  definitional  questions.  Instead, we chose to include the more interesting  ML\nproblems  that showed  up in real interviews.\nIn addition  to the problems  below, look to the multi-part  case study problems  in Chapter 11. We\nIntentionally  put these end-to-end  machine  learning  modeling  questions  in the last chapter,  because\nthey incorporate  system  design and product-sense  skills —- concepts  which we cover later.\nEasy\n7.1. Robinhood:  Say you are building  a binary classifier  for an unbalanced  dataset  (where  one class\nis much rarer than the other, say 1% and 99%, respectively).  How do you handle  this situation?\n7.2. Square:  What are some differences  you would expect in a model that minimizes  squared  error\nversus a model that minimizes  absolute  error? In which cases would each error metric be\nappropriate?\n7.3, Facebook:  When performing  K-means  clustering,  how do you choose  k?\n7.4. Salesforce:  How can you make your models  more robust to outliers?\n7.5. AQR: Say that you are running  a multiple  linear regression  and that you have reason  to believe\nthat several  of the predictors  are correlated.  How will the results  of the regression  be affected  if\nseveral  are indeed correlated?  How would  you deal with this problem?\n7.6, Point72: Describe  the motivation  behind random forests. What are two ways in which they\nImprove  upon individual  decision  trees?\n7.7, PayPal: Given a large dataset of payment  transactions,  say we want to predict the likelihood\nof a given transaction  being fraudulent.  However,  there are many rows with missing  values for\nvarious  columns.  How would you deal with this?\n116 Ace the Data Science  Interview  | Machine  Learning"
  },
  {
    "page_number": 129,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\n7.8. Airbnb:  Say you are running  a simple  logistic  regression  to solve a problem  but find the results\nto be unsatisfactory.  What are some ways you might  improve  your model,  or what other models\nmight you look into using instead?\n7.9. Two Sigma: Say you were running  a linear regression  for a dataset but you accidentally\nduplicated  every data point. What happens  to your beta coefficient?\n7.10. PWC: Compare  and contrast  gradient  boosting  and random  forests.\n7.11. DoorDash:  Say that DoorDash  is launching  in Singapore.  For this new market,  you want to\npredict  the estimated  time of arrival  (ETA)  for a delivery  to reach a customer  after an order has\nbeen placed on the app. From an earlier beta test in Singapore,  there were 10,000 deliveries\nmade. Do you have enough  training  data to create an accurate  ETA model?\nMedium\n7.12. Affirm:  Say we are running  a binary  classification  loan model,  and rejected  applicants  must be\nsupplied  with a reason why they were rejected.  Without  digging  into the weights  of features,\nhow would  you supply  these reasons?\n7.13. Google:  Say you are given a very large corpus  of words. How would you identify  synonyms?\n7.14. Facebook:  What is the bias-variance  trade-off?  How is it expressed  using an equation?\n7.15. Uber: Define  the cross-validation  process.  What is the motivation  behind  using it?\n7.16. Salesforce:  How would you build a lead scoring  algorithm  to predict  whether  a prospective\ncompany  ts likely to convert  into being an enterprise  customer?\n7.17. Spotify:  How would  you approach  creating  a music recommendation  algorithm?\n7.18. Amazon:  Define  what it means for a function  to be convex.  What is an example  of a machine\nlearning  algorithm  that is not convex  and describe  why that 1s so?\n7.19. Microsoft:  Explain  what information  gain and entropy  are in the context  of a decision  tree and\nwalk through  a numerical  example.\n7.20. Uber: What is L] and L2 regularization?  What are the differences  between  the two?\n7.21. Amazon:  Describe  gradient  descent  and the motivations  behind stochastic  gradient  descent.\n7.22. Affirm:  Assume  we have  a classifier  that produces  a score between  0 and | for the probability\nof a particular  loan application  being fraudulent.  Say that for each application’s  score, we take\nthe square root of that score. How would the ROC curve change? If it doesn’t  change, what\nkinds of functions  would change  the curve?\n7.23. IBM: Say X is a univariate  Gaussian  random  variable.  What is the entropy  of X?\n7.24. Stitch Fix: How would you build a model to calculate a customer’s  propensity  to buy a\nparticular  item? What are some pros and cons of your approach?\n7.25. Citadel: Compare  and contrast Gaussian  Naive Bayes (GNB) and logistic regression.  When\nwould you use one over the other?\nHard\n7.26. Walmart: What loss function is used in k-means clustering  given k clusters and n sample\npoints? Compute the update formula using (1) batch gradient descent and (2) stochastic\ngradient  descent  for the cluster  mean for cluster  & using a learning  rate €.\nAce the Nata Science  Interview 117"
  },
  {
    "page_number": 130,
    "content": "CHAPTER  7: MACHINE  LEARNING\n7.27. Two Sigma: Describe  the kernel trick in SVMs and give a simple example.  How do you decide\nwhat kernel to choose?\n7.28. Morgan Stanley: Say we have N observations  for some variable which we model as being\ndrawn from a Gaussian distribution.  What are your best guesses for the parameters  of the\ndistribution?\n7.29. Stripe: Say we are using a Gaussian mixture model (GMM) for anomaly detection of\nfraudulent  transactions  to classify incoming  transactions  into K classes. Describe  the model\nsetup formulaically  and how to evaluate the posterior  probabilities  and log likelihood.  How\ncan we determine  if a new transaction  should be deemed  fraudulent?\n7.30. Robinhood:  Walk me through how you’d build a model to predict whether a particular\nRobinhood  user will churn?\n7.31. Two Sigma: Suppose you are running a linear regression  and model the error terms as\nbeing normally  distributed.  Show that in this setup, maximizing  the likelihood  of the data is\nequivalent  to minimizing  the sum of the squared  residuals.\n7.32. Uber: Describe the idea behind Principle Components  Analysis (PCA) and describe its\nformulation  and derivation  in matrix form. Next, go through the procedural  description  and\nsolve the constrained  maximization.\n7.33. Citadel: Describe  the model formulation  behind logistic  regression.  How do you maximize  the\nlog-likelihood  of a given model (using the two-class  case)?\n7.34. Spotify: How would you approach  creating  a music recommendation  algorithm  for Discover\nWeekly  (a 30-song  weekly  playlist  personalized  to an individual  user)?\n7.35. Google: Derive the variance-covariance  matrix of the least squares parameter  estimates  in\nmatrix form.\nMachine  Learning  Solutions\nSolution  #7.1\nUnbalanced  classes  can be dealt with 1n several  ways.\nFirst, you want to check whether  you can get more data or not. While in many scenarios,  data may\nbe expensive  or difficult  to acquire,  tt’s important  to not overlook  this approach,  and at least mention\nit to your interviewer.\nNext, make sure you're looking  at appropriate  performance  metrics.  For example,  accuracy  is not a\ncorrect metric to use when classes  are imbalanced  --- instead,  you want to look at precision,  recall,\nF1 score, and the ROC curve.\nThen, you can resample  the training  set by either  oversampling  the rare samples  or undersampling  the\nabundant  samples;  both can be accomplished  via bootstrapping.  These approaches  are easy and quick\nto run, so they should  be good starting  points. Note, if the event is inherently  rare, then oversampling\nmay not be necessary,  and you should  focus more on the evaluation  function.\nAdditionally,  you could try generating  synthetic  examples.  There are several  algorithms  for doing so —\nthe most popular  1s called SMOTE:  (synthetic  minority  oversampling  technique),  which creates  synthetic\nsamples  of the rare class rather than pure copies by selecting  various  instances.  It does this by modifying\nthe attributes  sightly  by a random  amount  proportional  to the difference  in neighboring  instances.\n118 Ace the Data Science  Interview  | Machine  Learning"
  },
  {
    "page_number": 131,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nAnother  way is to resample  classes  by running  ensemble  models  with different  ratios of the classes,\nor by running  an ensemble  model using all samples  of the rare class and a differing  amount  of the\nabundant  class. Note that some models,  such as logistic  regression,  are able to handle unbalanced\nclasses relatively  well in a standalone  manner. You can also adjust the probability  threshold  to\nsomething  besides  0.5 for classifying  the unbalanced  outcome.\nLastly, you can design your own cost function  that penalizes  wrong classification  of the rare class\nmore than wrong  classifications  of the abundant  class. This is useful if you have to use a particular\nkind of model! and you’re unable to resample.  However,  it can be complex  to set up the penalty\nmatrix,  especially  with many classes.\nSolution  #7.2\nWe can denote squared  error as MSE and absolute  error as MAE. Both are measures  of distances\nbetween  vectors  and express  average  model prediction  in units of the target variable.  Both can range\nfrom 0 to infinity;  the lower the score, the better  the model.\nThe main difference  is that errors are squared  before being averaged  in MSE, meaning  there is\na relatively  high weight given to large errors. Therefore,  MSE is useful when large errors in the\nmodel are trying to be avoided.  This means that outliers  disproportionately  affect MSE more than\nMAE — meaning  that MAE is more robust to outliers.  Computation-wise,  MSE is easier to use,\nsince the gradient  calculation  is more straightforward  than that of MAE, which requires  some linear\nprogramming  to compute  the gradient.\nTherefore,  if the model needs to be computationally  easier to train or doesn’t  need to be robust to\noutliers,  then MSE should  be used. Otherwise,  MAE is the better option.  Lastly,  MSE corresponds  to\nmaximizing  the likelihood  of Gaussian  random  variables,  and MAE does not. MSE is minimized  by\nthe conditional  mean, whereas  MAE is minimized  by the conditional  median.\nSolution  #7.3\nThe elbow  method  1s the most well-known  method  for choosing  & in k-means  clustering.  The intuition\nbehind this technique  is that the first few clusters  will explain  a lot of the variation  in the data, but\npast a certain  point, the amount  of information  added is diminishing.  Looking  at a graph of explained\nvariation  (on the y-axis) versus the number  of clusters (4), there should be a sharp change in the\ny-axis at some level of k. For example,  in the graph that follows,  we see a dropoff  at approximately\nk= 6.\nNote that the explained  variation  is quantified  by the within-cluster  sum of squared  errors. To calculate\nthis error metric, we look at, for each cluster, the total sum of squared errors (using Euclidean\ndistance).  A caveat to keep in mind: the assumption  of a drop in variation  may not necessarily  be true\n—- the y-axis may be continuously  decreasing  slowly (1.e., there is no significant  drop).\nAnother  popular  alternative  to determining  & in k-means  clustering  is to apply the silhouette  method,\nwhich aims to measure  how similar points are in its cluster compared  to other clusters. Concretely,\nit looks at:\n(x- y)\nmax (x, y)\nwhere x is the mean distance  to the examples  of the nearest cluster, and is the mean distance  to other\nexamples  in the same cluster. The coefficient  varies between  -1 and | for any given point. A value of\n| implies that the point is in the “right” cluster and vice versa for a score of -1. By plotting  the score\nAce the Data Science  Interview 119"
  },
  {
    "page_number": 132,
    "content": "CHAPTER  7: MACHINE  LEARNING\non the y-axis versus k, we can get an idea for the optimal number of clusters based on this metric.\nNote that the metric used in the silhouette  method is more computationally  intensive  to calculate  for\nall points versus the elbow method.\nElbow  Method  for Optimal  k\n400-\n350-4\n”—)\n® 300-\nCc\n&\n2 250-\nae)\n®\n© 200-\nog\n—”\nSo 150-\nE\na)\n° 400-\n50-\nO-\ni i 1 | | ! a\n0 4 6 8 10 12 14\nk\nTaking a step back. while both the elbow and silhouette  methods  serve their purpose,  sometimes  it\nhelps to lean on your business  intuition  when choosing  the number  of clusters.  For example,  if you\nare clustering  patients  or customer  groups, stakeholders  and subject matter experts should have a\nhunch concerning  how many groups they expect to see in the data. Additionally,  you can visualize\nthe features  for the different  groups  and assess whether  they are indeed  behaving  similarly.  There is\nno perfect method  for picking  4, because  if there were, 1t would be a supervised  problem  and not an\nunsupervised  one.\nSolution  #7.4\nInvestigating  outliers  1s often the first step in understanding  how to treat them. Once you understand\nthe nature of why the outliers  occurred,  there are several  possible  methods  we can use:\n¢ Add regularization:  reduces  variance,  for example  L1 or L2 regularization.\n¢ Try different  models:  can use a model that is more robust to outliers.  For example,  tree-based\nmodels (random  forests, gradient  boosting)  are generally  less affected  by outliers than linear\nmodels.\n¢ Winsortze  data: cap the data at various  arbitrary  thresholds.  For example,  at a 90% winsorization,\nwe can take the top and bottom 5% of values and set them to the 95th and Sth percentile  of\nvalues,  respectively.\n* Transform  data: for example,  do a log transformation  when the response  variable  follows  an\nexponential  distribution  or is right skewed.\n* Change  the error metric to be more robust: for example,  for MSE, change  1( to MAE or Huber\nloss.\n120 Ace the Data Science  Interview  | Machine  Learning"
  },
  {
    "page_number": 133,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\n* Remove  outliers:  only do this if you’re certain that the outliers  are true anomalies  not worth\nincorporating  into the model.  This should  be the last consideration,  since dropping  data means\nlosing information  on the variability  in the data.\nSolution  #7.5\nThere will be two primary  problems  when running  a regression  if several  of the predictor  variables\nare correlated.  The first is that the coefficient  estimates  and signs will vary dramatically,  depending  on\nwhat particular  variables  you included  in the model. Certain  coefficients  may even have confidence\nintervals  that include  0 (meaning  it is difficult  to tell whether  an increase  in that X value is associated\nwith an increase  or decrease  in Y or not), and hence results will not be statistically  significant.  The\nsecond is that the resulting  p-values  will be misleading.  For instance,  an important  variable  might\nhave a high p-value  and so be deemed  as statistically  insignificant  even though  it is actually  important.\nIt is as if the effect of the correlated  features  were “split”  between  them, leading  to uncertainty  about\nwhich features  are actually  relevant  to the model.\nYou can deal with this problem  by either removing  or combining  the correlated  predictors.  To\neffectively  remove  one of the predictors,  it is best to understand  the causes of the correlation  (i.e.,\ndid you include  extraneous  predictors  such as X and 2X or are there some latent variables  underlying\none or more of the ones you have included  that affect both? To combine  predictors,  it is possible  to\ninclude  interaction  terms (the product  of the two that are correlated).  Additionally,  you could also (1)\ncenter  the data and (2) try to obtain a larger size of sample,  thereby  giving  you narrower  confidence\nintervals.  Lastly, you can apply regularization  methods  (such as in ridge regression).\nSolution  #7.6\nRandom  forests are used since individual  decision  trees are usually prone to overfitting.  Not only\ncan these utilize multiple  decision  trees and then average  their decisions,  but they can be used for\neither classification  or regression.  There are a few main ways in which they allow for stronger  out-\nof-sample  prediction  than do individual  decision  trees.\n¢ Asin other ensemble  models,  using a large set of trees created  in a resample  of the data (bootstrap\naggregation)  will lead to a model yielding  more consistent  results. More specifically,  and in\ncontrast  to decision  trees, it leads to diversity  in training  data for each tree and so contributes  to\nbetter results in terms of bias-variance  trade-off  (particularly  with respect to variance).\n¢ Using only m < p features  at each split helps to de-correlate  the decision  trees, thereby  avoiding\nhaving very important  features always appearing  at the first splits of the trees (which would\nhappen  on standalone  trees due to the nature of information  gain).\n° They’re  fairly easy to implement  and fast to run.\n¢ They can produce very interpretable  feature-importance  values, thereby improving  model\nunderstandability  and feature  selection.\nThe first two bullet points are the main ways random forests improve  upon single decision  trees.\nSolution  #7.7\nStep 1: Clarify  the Missing  Data\nSince these types of problems  are generally  context dependent,  it’s best to start your answer with\nclarifying  questions.  For example,\n¢ Is the amount  of missing  values uniform  by feature?\nAce the Data Science  Interview 121"
  },
  {
    "page_number": 134,
    "content": "CHAPTER  7: MACHINE  LEARNING\ne Are these missing  values numerical  or categorical?\n¢ How many features  with missing  data are there?\n* — Is there a pattern in the types of transactions  that have  a lot of missing  data?\nIt would also be useful to think about why the data is missing,  because  this affects how you’d impute\nthe data. Missing  data is commonly  classified  as:\n° Missing  completely  at random (MCAR):  the probability  of being missing is the same for all\nclasses\n¢ Missing  at random  (MAR):  the probability  of being missing  is the same within groups defined\nby the observed  data\n¢ Not missing  at random  (NMAR):  it the data is not MCAR  and not MAR\nStep 2: Establish  a Baseline\nOne reason to ask these questions  is because  a good answer would consider  that the missing  data\nmay not actually  be a problem.  What if the missing  data was in transactions  that were almost never\nfraud? What if the missing  data is mostly in columns  whose data features  don’t have good predictive\nvalue? For example,  if you were missing  the IP-address  derived geolocation  of the person making\nthe payment,  that would likely be bad for model performance.  On the other hand, if you were missing\nthe user’s middle name, it likely wouldn’t  have any bearing  on whether  the transaction  was fraud or\nnot. Even simpler  yet, can a baseline  model be built that meets the business  goals, without  having  to\ndeal with any missing  data?\nStep 3: Impute  Missing  Data\nIf the baseline  model indicates  that dealing  with the missing  data is worth it, one technique  we could\nuse 1s imputation.  For continuous  features,  we can start by using the mean or median  for missing\nvalues within  any feature.  However,  the downside  to this approach  is that it does not factor in any of\nthe other features  and correlations  between  them --— it 1s unlikely  that two transactions  in differing\nlocations  for different  category  codes would have the same transaction  price. An alternative  could\nbe to use a nearest neighbors  method to estimate  any given feature based on the other features\navailable.\nStep 4: Check  Performance  with Imputed  Data\nWith these modeled  features,  we can then run a set of classification  algorithms  to predict fraud or\nnot fraud. With this imputation  technique,  we can also cross-validate  to check whether  performance\nincreases  by including  the imputed  data, relative  to just the original  data. Note that a performance\nIncrease  is only expected  if the feature contains  valuable  information  (for those rows/entries  that\nhave it). If this isn*t the case, and you won't see a significant  impact  as a result, it may be easiest to\ndrop the existing  missing  data before  training  the model.\nStep 5: Other  Approaches  for Missing  Data\nFinally, thinking  outside the box, is it possible  to use a third-party  dataset to fill in some of the\nmissing  information?  Suppose  a common  missing  piece of information  was the type of business  that\na person  paid. But, let’s say we have the business’s  address  on file. Could we use the address  against\na business  listings  dataset  to infer the type of business  the transaction  was made at? Lastly, note that\nsome models  are capable  of  dealing  with missing  data, without  requiring  imputation.\n122 Ace the Data Science  Interview  | Machine  Learning"
  },
  {
    "page_number": 135,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nSolution  #7.8\nThere are several  possible  ways to improve  the performance  of a logistic  regression:\nNormalizing  features:  The features  should be normalized  such that particular  weights  do not\ndominate  within  the model.\nAdding  additional  features:  Depending  on the problem,  it may simply  be the case that there\naren't  enough  useful  features.  In general,  logistic  regression  is high bias, so adding  more features\nshould  be helpful.\nAddressing  outliers:  \\dentify  and decide  whether  to retain or remove  them.\nSelecting  variables:  See if any features  have introduced  too much noise into the process.\nCross validation  and hyperparameter  tuning: Using k-fold cross validation  along with\nhyperparameter  tuning (for example,  introducing  a penalty term for regularization  purposes)\nshould  help improve  the model.\nThe classes may not be linearly separable  (logistic regression  produces linear decision\nboundaries),  and, therefore,  it would be worth looking  into SVMs, tree-based  approaches,  or\nneural  networks  instead.\nSolution  #7.9\nFor regular  regression,  recall we have the following  for our least Squares  estimator:\nB= (XT XY\" X7y\nX\\(YSo if we double the data, then we are using: pales\ninstead  of and respectively.  Then plugging  this into our estimator  from above,  we get:\n(THVT\nSimplifying  yields:\nB = (2X7 XY\" 2XTy\nTherefore,  we see that the coefficient  remains  unchanged.\nSolution  #7.10\nIn both gradient  boosting  and random  forests,  an ensemble  of decision  trees are used. Additionally,\nboth are flexible  models  and don’t need much data preprocessing.\nHowever,  there are two main differences.  The first main difference  is that, in gradient  boosting,  trees\nare built one at a time, such that successive  weak learners  learn from the mistakes  of preceding  weak\nlearners.  In random  forests,  the trees are built independently  at the same time.\nThe second difference  is in the output: gradient  boosting  combines  the results of the weak learners\nwith each successive  iteration,  whereas,  in random  forests,  the trees are combined  at the end (through\neither averaging  or majority).\nBecause  of these structural!  differences,  gradient  boosting  1s often more prone to overfitting  than are\nrandom  forests due to their focus on mistakes  over training  iterations  and the lack of independence\nin tree building.  Additionally,  gradient  boosting  hyperparameters  are harder to tune than those of\nAce the Data Science  Interview 123"
  },
  {
    "page_number": 136,
    "content": "CHAPTER  7 : MACHINE  LEARNING\nrandom forests. Lastly, gradient  boosting  may take longer to train than random forests because the\ntrees of the latter are built sequentially.  In real-life applications,  gradient  boosting  generally  excels\nwhen used on unbalanced  datasets  (fraud detection,  for example),  whereas  random  forests generally\nexcel at multi-class  object detection  with noisy data (computer  vision, for example).\nSolution  #7.11\nBecause  “accurate  enough”  is subjective,  it’s best to ask the interviewer  clarifying  questions  before\naddressing  the lack of training  data. To stand out, you can also proactively  mention  ways to source\nmore training  data at the end of your answer.\nStep 1: Clarify  What “Good”  ETA Means\nTo determine  how accurate  the ETA model needs to be, first ask what the ETA prediction  will be used\nfor. The level of accuracy  needed in ETA predictions  might be higher for the order-driver  matching\nalgorithm  than what DoorDash  needs to display to the customer  in the app. Also, consider  if your\nETA estimate  under-promises  and over-delivers.  Maybe that’s okay — customers  would likely be\nhappy that the delivery  arrived faster than expected.  At the same time, high ETA estimates  across the\nboard may lead people  to say, “Screw  it, I’ll just go to the store and pick it up myself.”  By considering\nthe context  around  how the ETA predictions  will be used, you’ll be one step closer to understanding\nwhat a good-enough  ETA model looks like.\nOne data-driven  approach  to establishing  how accurate  your ETA model  needs to be involves  looking  at\nETA models  in similar  markets. From data in other locations,  we could better understand  the economic\nimpact  of both over and underestimated  ETAs. Tying the model output  to its business  impact  can help\nDoorDash  decide if investing  money into solving  this problem  ts even warranted  in the first place.\nStep 2: Assess  Baseline  ETA Performance\nAfter you understand  what “good-enough”  ETA means, it’s best practice  to next see how a baseline\nmodel, trained  on the beta 10,000 deliveries  made, performs.  A baseline  model can be something  as\nsimple  as the estimated  driving  time plus the average  preparation  time (conditional  on the restaurant\nand time of day). Since predicting  the ETAs 1s a regression  problem,  potential  metrics  we can use to\nassess this baseline  model include  root mean square  error (RMSE),  MAE, R?, etc.\nStep 3: Determine  How More Data Improves  Accuracy\nSay that we use R’ as the main metric. One way to gauge the benefit  from having  more training  data\nwould be to build learning  curves.  A learning  curve depicts  how the accuracy  changes  when we train\na model on a progressively  larger  percentage  of the data. For example,  say that with 25% of the data,\nwe get an R* of 0.5, with 50% of  the data we get an R* of 0.65, and with 75% of the data we get an R?\nof 0.67. Note that the improvement  drops off significantly  between  the use of 50% and 75% of the\ntraining  data. The point at which the drop-off  starts to become  a problem  is the signal that we should\nlook into reevaluating  the features  rather than simply  adding  more training  data.\nThis process  is analogous  to looking  at the learning  curves discussed  earlier in this chapter,  which\nlook at how the training  error and validation  error change over the number  of iterations  — here,\nInstead,  we are looking  at how the model performance  changes  over the amount  of training  data used.\nStep 4: In Case Performance  Isn’t “Good  Enough”\nIf after learning  curves you realize that you don’t have sufficient  data to build an accurate  enough\nmodel, the interviewer  would likely shift the discussion  to dealing with this lack of data. Or, if\n124 Ace the Data Science  Interview | Machine  Learning"
  },
  {
    "page_number": 137,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nyou are feeling like an overachiever  with a can-do attitude,  you could proactively  bring up these\ndiscussion  points:\n* Are there too few features?  If so, you want to look into adding  features  like marketplace  supply\nand demand  indicators,  traffic patterns  on the road at the time of the delivery,  etc.\n* Are there too many features?  If there are almost  as many or more features  than data points,  then\nour model will be prone to overfitting  and we should look into either dimensionality  reduction\nor feature  selection  techniques.\n¢ Can different  models  be used that handle  smaller  training  datasets  better?\n¢ — Is it possible  to acquire  more data in a cost-effective  way?\n¢ — Is the less accurate  ETA model a true launch  blocker?  If we launched  in the new market,  which\ngenerates  more training  data, can the ETA model  be retrained?\nSolution  #7.12\nWithout  looking  at features,  we could look at partial dependence  plots (also called response  curves)\nto assess how any one feature affects the model’s  decision.  A partial dependence  plot shows the\nmarginal  effect of a feature  on the predicted  target of a machine  leaming  model. So, after the model\nis fit, we can take all the features  and start plotting  them individually  against the loan approval/\nrejection,  while keeping  all the other features  constant.\ng Partial  Dependence  of Loan Approval  for Features  with Gradient  Boosting\nO\not\nQ. ’< :\nce 10-5 1.0 1.07\ne)\nae |\nie\nco) 0.57 0.5- 05-4\nO\nCc\n5\nec 00- 0.0 0.0\njos\n®\n(a)\nge OST. 095 ao Liitttr tf = AUtitit t 4 Lriviti  jj , :\noO 300 550 800 $0 $50K $100K 0 3 6\nFICO Debt Number  of Credit Cards\nFor example,  if we believe that FICO score has a strong relationship  to the predicted  probability  of\nloan rejection,  then we can plot the loan approvals  and rejections  as we adjust the FICO score from\nlow to high. Thus, we can get an idea of how features  impact the model without  explicitly  looking  at\nfeature  weights,  and supply reasons  for rejection  accordingly.\nAs a concrete example,  consider  having four applicants:  1, 2, 3, and 4. Assume that the features\ninclude annual income,  current debt, number  of credit cards, and FICO score. Suppose  we have the\nfollowing  situation:\n1. $100,000  income,  $10,000  debt, 2 credit cards, and FICO score of 700.\n2. $100,000  income,  $10,000  debt, 2 credit cards, and FICO score of 720.\n3. $100,000  income,  $10,000  debt, 2 credit cards, and FICO score of 600.\n4. $100,000  income,  $10,000  debt, 2 credit cards, and FICO score of 650.\nAce the Data Science  Interview 125"
  },
  {
    "page_number": 138,
    "content": "CHAPTER  7 : MACHINE  LEARNING\nIf 3 and 4 were rejected  but | and 2 were accepted,  then we can intuitively  reason that a lower FICO\nscore was the reason the model made the rejections.  This is because  the remaining  features  are equal,\nso the model chose to reject 3 and 4 “all-else-equal”  versus 1 and 2.\nSolution  #7.13\nTo find synonyms,  we can first find word embeddings  through a corpus of words. Word2vec  is a\npopular  algorithm  for doing so. It produces  vectors for words based on the words’ contexts.  Vectors\nthat are closer in Euclidean  distance  are meant to represent  words that are also closer in context  and\nmeaning.  The word embeddings  that are thus generated  are weights on the resulting  vectors. The\ndistance  between  these vectors can be used to measure  similarity,  for example,  via cosine similarity\nor some other similar  measure.\nOnce we have these word embeddings,  we can then run an algorithm  such as K-means  clustering  to\nidentify  clusters within word embeddings  or run a K-nearest  neighbor  algorithm  to find a particular\nword for which we want to find synonyms.  However,  some edge cases exist, since word2vec  can\nproduce similar vectors even in the case of antonyms;  consider  the words “hot” and “cold,” for\nexample,  which have opposite  meanings  but appear  in many similar  contexts  (related  to temperature\nor in a Katy Perry song).\nSolution  #7.14\nThe bias-variance  trade-off  is expressed  as the following:  Total model error = Bias + Variance  +\nIrreducible  error. Flexible models tend to have low bias and high variance,  whereas  more ngid\nmodels have high bias and low variance.  The bias term comes from the error that occurs when a\nmodel underfits  data. Having  a high bias means that the machine  learning  model is too simple and\nmay not adequately  capture  the relationship  between  the features  and the target. An example  would\nbe using linear regression  when the underlying  relationship  is nonlinear.\nThe variance  term represents  the error that occurs  when a model  overfits  data. Having  a high variance\nmeans that a model ts susceptible  to changes  in training  data, meaning  that it is capturing  and so\nreacting  to too much noise. An example  would  be using a very complex  neural  network  when the true\nunderlying  relauonship  between  the features  and the target is simply  a linear one.\nThe irreducible  term is the error that cannot be addressed  directly  by the model,  such as from noise\nin data measurement.\nWhen  creating  a machine  learning  model,  we want both bias and variance  to be low, because  we want\nto be able to have a model that predicts  well but that also doesn’t  change  much when it is fed new\ndata. The key point here is to prevent  overfitting  and, at the same time, to attempt  to retain sufficient\naccuracy.\nSolution  #7.15\nCross validation  is a technique  used to assess the performance  of an algorithm  in several  resamples/\nsubsamples  of training  data. It consists  of running  the algorithm  on subsamples  of the training  data,\nsuch as the original  data less some of the observations  comprising  the training  data, and evaluating\nmodel performance  on the portion  of the data that was not present  in the subsample.  This process  1s\nrepeated  many umes for the different  subsamples,  and the results  are combined  at the end. This step Is\nvery important  in ML because  it reveals  the quality  and consistency  of the model's  true performance.\n126 Ace the Data Science  Interview  | Machine  Learning"
  },
  {
    "page_number": 139,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nThe process  is as follows:\n1. Randomly  shuffle  data into k equally-sized  blocks  (folds).\n2. Foreachi in fold 1...R, train the model  on all the data except  for fold i, and evaluate  the validation\nerror using block 1.\n3. Average  the k validation  errors from step 2 to get an estimate  of the true error.\nThis process aids in accomplishing  the following:  (1) avoiding  training  and testing on the same\nsubsets  of data points,  which would lead to overfitting,  and (2) avoiding  using a dedicated  validation\nset, with which no training  can be done. The second  of these points is particularly  important  in cases\nwhere very little training  data is available  or the data collection  process  is expensive.  One drawback\nof this process,  however,  is that roughly  k times more computation  is needed  than using a dedicated\nholdout  validation  set. In practice,  cross validation  works very well for smaller  datasets.\nSolution  #7.16\nStep 1: Clarify  Lead Scoring  Requirements\nLead scoring 1s the process  of assigning  numerical  scores for any leads (potential  customers)  in a\nbusiness.  Lead scores can be based on a variety of attributes,  and help sales and marketing  teams\npnioritize  leads to try and convert  them to customers.\nAs always,  it’s smart to ask the interviewer  clarifying  questions.  In this case, learning  more about the\nrequirements  for the lead scoring  algorithm  is critical.  Questions  to ask include:\n¢ Are we building  this for our own company’s  sales leads? Or, are we building  an extensible\nversion  as part of the Salesforce  product?\n¢ Are there any business  requirements  behind the lead scoring (1.e., does it need to be easy to\nexplain  internally  and/or  externally)?\n¢ Are we running  this algorithm  only on companies  in our sales database  (CRM),  or looking  at a\nlarger landscape  of all companies?\nFor our solution,  we'll assume the interviewer  means we want to develop  a leading scoring model\nto be used internally  — that means using the company’s  internal sales data to predict whether  a\nprospective  company  will purchase  a Salesforce  product.\nStep 2: Explain  the Features  You'd Use\nSome elements  which should influence  whether  a prospective  company  turns into a customer:\n¢ Firmographic  Data: What type of company  is this? Industry?  Amount of revenue?  Employee\ncount?\n° Marketing  Activity:  Have they interacted  with marketing  materials,  like clicking  on links within\nemail marketing  campaigns’?  Have employees  from that company  downloaded  whitepapers,  read\ncase studies,  or clicked  on ads? If so, how much activity  has there been recently?\n© Sales Activity:  Has the prospective  company  interacted  with sales? How many sales meetings\ntook place, and how recently  did the last one take place?\n© Deal Details:  What products  are being bought?  Some might be harder to close than others. How\nmany seats (licenses)  are being bought?  What’s the size of the deal? What’s the contract length?\nAce the Data Science  Interview 127"
  },
  {
    "page_number": 140,
    "content": "CHAPTER  7 : MACHINE  LEARNING\nAfter selecting  features, it is good to conduct the standard  set of feature engineering  best practices.  Note\nthat the model will only be as good as the data and judgement  in feature  engineering  applied  — in practice,\nmany companies  that predict lead scoring  can face issues with missing  data or lack of relevant  data.\nStep 3: Explain  Models  You’d Use\nLead scoring can be done through building  a binary classifier  that predicts the probability  of a lead\nconverting.  In terms of model selection,  logistic regression  offers a straightforward  solution  with an\neasily interpretable  result: the resulting  log-odds  is a probability  score for, in this case, purchasing  a\nparticular  item. However,  it cannot capture complex  interaction  effects between  different  variables\nand could also be numerically  unstable under certain conditions  (i.e., correlated  covariates  and a\nrelatively  small user base).\nAn alternative  to logistic  regression  would be to use a more complex  model, such as a neural network\nor an SVM. Both are great for dealing with high-dimensional  data and with capturing  the complex\ninteractions  that logistic regression  cannot. However,  unlike logistic regression,  neither is easy to\nexplain,  and both generally  require  a large amount  of data to perform  weil.\nA suitable  compromise  is tree-based  models, such as random forests or XGBoost,  which typtcally\nperform  well. With tree-based  models,  the features  that have the highest  influence  on predictions  are\nreadily  perceived,  a characteristic  that could be very useful in this particular  case.\nStep 4: Model Deployment  Nuances\nLastly, it is important  to monitor  for feature  shifts and/or  model degradations.  As the product  line and\ncustomer  base changes  over time, models trained on old data may not be as relevant.  For a mature\ncompany  like Salesforce,  for example,  it’s very likely that companies  signing  up now aren't exactly\nlike the companies  that signed up with Salesforce  5 or 10 years ago. That’s why it’s important  to\nmonitor  feature  drift and continuously  update  the model.\nSolution  #7.17\nCollaborative  filtering  would be a commonly  used method  for creating  a music recommendation\nalgorithm.  Such algorithms  use data on what feedback  users have provided  on certain items (songs\nIn this case) in order to decide recommendations.  For example,  a well-known  use case is for movie\nrecommendation  on Netflix.  However,  there are several  differences  compared  to the Netflix  case:\n¢ Feedback  for music does not have a |-to-5 rating scale as Netflix  does for its movies.\n¢ Music may be subject  to repeated  consumption;  that is, people  may watch a movie  once or twice\nbut will listen to a song many times over.\n¢ Music has a wider variety  (1.e., niche music).\n* The scale of music catalog  items is much larger than movies  (.e., there are many more songs\nthan movies).\nTherefore,  a user-song  matrix (or a user-artist  matrix)  would constitute  the data for this issue, with\nthe rows of the dataset  being users and the columns  various  songs. However,  in considering  the first\npoint above, since explicit  ratings  are lacking,  we can employ  a binary system  to count the number\nof times a song is streamed  and store this count.\nWe can then proceed  with matrix factorization.  Say there are M songs and N users in the matrix,\nwhich we will label R. Then, we want to solve: R = PQTM\nwhere user preferences  are captured  by the vectors: r,, = q’ p,\n128 Ace the Data Science  Interview  | Machine  Learning"
  },
  {
    "page_number": 141,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nVarious  methods  can be used for this matrix factorization;  a common  one is alternating  least squares\n(ALS),  and, since  the scale  of the data is large,  this would  likely  be done through  distributed  computing.\nOnce the latent user and song vectors  are discovered,  then the above  dot product  will be able to predict\nthe relevance  of a particular  song to a user. This process  can be used directly  for recommendation\nat the user level, where we sort by relevance  prediction  on songs that the user has not yet streamed.\nIn addition,  the vectors  given above can be employed  in such tasks as assessing  similarity  between\ndifferent  users and different  songs using a method  such as kKNN (K-nearest  neighbors).\nSolution  #7.18\nMathematically,  a convex  function  f satisfies  the following  for any two points x and y in the domain\nof f: f(1 - a)x + ay) < (1 - a)f(x) + afly), O< a <1\nThat is, the line segment  from  x to y lies above  the function  graph of f for any points  x and y. Convexity\nmatters  because  it has implications  about the nature of minima  in f. Stated more specifically,  any\nlocal minimum  of f is also a global minimum.\nNeural networks  provide  a significant  example  of non-convex  problems  in machine  learning.  At a\nhigh level, this is because  neural networks  are universal  function  approximators,  meaning  that they\ncan (with a sufficient  number  of neurons)  approximate  any function  arbitrarily  well. Because  not\nall functions  are convex (convex  functions  cannot approximate  non-convex  ones), by definition,\nthey must be non-convex.  In particular,  the cost function  for a neural network  has a number  of local\nminima;  you could interchange  parameters  of different  nodes in various  layers and still obtain exactly\nthe same cost function  output  (all inputs/outputs  the same, but with nodes swapped).  Therefore,  there\nis no particular  global  minima,  so neural networks  cannot  be convex.\nSolution  #7.19\nBecause  information  gain is based on entropy,  we’ ll discuss  entropy  first.\nThe formula  for entropy  is Entropy  = »y —~P(Y =k)log P(Y =k)\nr=]\nThe equation  above yields the amount of entropy present and shows exactly how homogeneous  a\nsample  is (based on the attribute  being split). Consider  a case where k = 2. Let a and b be two outputs/\nlabels that we are trying to classify.  Given these values, the formula  considers  the proportion  of values\nin the sample  that are a and the proportion  that are b, with the sample  being split on a different  attribute.\nA completely  homogeneous  sample will have an entropy of 0. For instance,  if a given attribute  has\nvalues a and ), then the entropy  of splitting  on that given attribute  would be\nEntropy  = —1 * Log,(1)  — 0 * Log, (0) =0\nwhereas  a completely  split (50%-50%)  would result in an entropy of 1. A lower entropy means a\nmore homogeneous  sample.\nInformation  gain is based on the decrease  in entropy  after splitting  on an attribute.\nIG(X, Y) = HY) - H(Y1X)\nThis concept  is better explained  with a simple numerical  example.  Consider  the above case again with\nk = 2. Let’s say there are 5 instances  of value a and 5 instances  of value 6. Then, we decide to split on\nsome attribute  X. When X = 1, there are 5 a’s and 1 6, whereas  when X = 0, there are 4 6’s and 0 a’s.\nNow, by splitting  on X, we have two classes: X = 1 and X = 0. However,  by splitting  on this attribute,\nwe now have X = 1, which has 5 a’s and 1 b, while X= 0 has 4 6’s and 0 a’s.\nAce the Data Science  Interview 129"
  },
  {
    "page_number": 142,
    "content": "CHAPTER  7 : MACHINE  LEARNING\n6Entropy(After)  = (+) * Entropy(X  = 0) — fa * Entropy(X  = 1)\nThe entropy  value for X = 0 is 0, since the sample is homogeneous  (all b’s, no a’s).\nEntropy(X  = 0) = 0\n1 1 5 5Entropy(X  =1) = -(2) * Log, (=) — (2) « Log, (2) = .65\nPlug these into the entropy(after)  formula  to obtain the following:\n4 6t After) =| — |}*0- 5} «65 =.39Entropy(ANer) (5) (-\nFinally,  we can go back to our original  formula  and obtain information  gain: /G = 1 — .389 = .61\nThese results make intuitive  sense, since, ideally,  we want to split on an attribute  that splits the output\nperfectly.  Therefore,  we ideally want to split on something  that is homogeneous  with regards  to the\noutput,  and this something  would thus have an entropy  equal to 0.\nSolution  #7.20\nIn machine  learning,  L, and L, penalization  are both regularization  methods  that prevent  overfitting  by\ncoercing  the coefficients  of a regression  model towards  zero. The difference  between  the two methods\nis the form of the penalization  applied  to the loss function.  For a regular  regression  model,  assume  the\nloss function  is given by L. Using L, regularization,  the least absolute  shrinkage  and selection  operator,\nor Lasso,  adds the absolute  value of the coefficients  as a penalty  term, whereas  ridge regression  uses L,\nregularization,  that 1s, adding  the squared  magnitude  of the coefficients  as the penalty  term.\nThe loss function  for the two are thus the following:\nLoss(L,)=L+iAlw,| Loss(L,)  = L+i|w?|\nwhere the loss function  Z 1s the sum of errors  squared,  given by the following,  where  f(x) is the model\nof interest  — for example,  a linear regression  with p predictors:\nn n Dp 2L= > ( y, ~ f(x, )y = » y,- > (x, ; ) for linear regression\ni=l i=l j=l\nIf we run gradient  descent  on the weights  w, L, regularization  forces  any weight  closer  to 0, irrespective\nof its magnitude.  whereas  with L, regularization,  the rate at which the weight  approaches  0 becomes\nslower  as the weight  approaches  0. Because  of this, L, is more likely  to “zero”  out particular  weights  and\nhence completely  remove  certain  features  from the model,  leading  to models  of increased  sparseness.\nSolution  #7.21\nThe gradient  descent algorithm  takes small steps in the direction  of steepest  descent to optimize\na particular  objective  function.  The size of the “steps” the algorithm  takes are proportional  to the\nnegative  gradient  of the function  at the current value of the parameter  being sought.  The stochastic\nversion  of the algorithm,  SGD. uses an approximation  of the nonstochastic  gradient  descent  algorithm\ninstead  of the function’s  actual gradient.  This estimate  is done by using only one randomly  selected\nsample  at each step to evaluate  the derivative  of the function,  making  this version  of the algorithm\nmuch faster and more attractive  for situations  involving  lots of data. SGD is also useful when\nredundancy  in the data is present  (i.e., observations  that are very similar),\n130 Ace the Data Science  Interview  | Machine  Learning"
  },
  {
    "page_number": 143,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nAssume  function  f at some point x and at time t. Then, the gradient  descent  algorithm  wil] update  x\nas follows  until it reaches  convergence:\nxitlagt—  a, Vf(x'’)\nThat is, we calculate  the negative  of the gradient  of f and scale that by some constant  and move in\nthat direction  at the end of each iteration.\nSince many loss functions  are decomposable  into the sum of individual  functions,  then the gradient\nstep can be broken down into addition  of discrete,  separate  gradients.  However,  for very large\ndatasets,  this process  can be computationally  intensive,  and the algorithm  might become  stuck at\nlocal minima  or at saddle  points.\nTherefore.  we use the stochastic  gradient  descent  algorithm  to obtain an unbiased  estimate  of the\ntrue gradient  without  going through  all data points by uniformly  selecting  a point at random  and\nperforming  a gradient  update  then and there.\nThe estimate  is therefore  unbiased  since we have the following:\n1 n\nVilx)=—  > VF (x)\nSince the data are assumed  to be 1.1.d., for the SGD, the expectation  of g(x) is: E[g(x)]  = Vf(x) where\ng(x) is the stochastic  gradient  descent.\nSolution  #7.22\nRecall that the ROC curve plots the true positive  rate versus the false positive  rate. If all scores\nchange  simultaneously,  then none of the actual classifications  change  (since thresholds  are adjusted),\nleading  to the same true positive  and false positive  rates, since only the relative  ordering  of the scores\nmatters.  Therefore,  taking a square root would not cause any change to the ROC curve because  the\nrelative  ordering  has been maintained.  If one application  had a score of X and another  a score of Y,\nand if Y> X, then VY > VX still. Only the model thresholds  would change.\nIn contrast.  any function  that 1s not monotonically  increasing  would change  the ROC curve, since the\nrelative  ordering  would not be maintained.  Some simple examples  are the following:\nf(x) = —x, f(x) = —x?, or a stepwise  function.\nSolution  #7.23\nWe have: X ~ Mu. 94), and entropy  for a continuous  random variable  is given by the following:\nH (x)= -[ p(x)log p(x)dx\n1\nOv2TFor a Gaussian,  we have the following:  p(x) = e 20\nSubstituting  into the above equation  yields\nH (x)= | p(x)loz oV2ndx J pta{ om Jlaxleras\nwhere the first term equals — log ov2n| p\\x)dx = —log oV2n\nsince the integral evaluates  to | (by the definition  of a probability  density function).  The second term\nis given by:\nAce the Data Science  Interview 131"
  },
  {
    "page_number": 144,
    "content": "CHAPTER  7 : MACHINE  LEARNING\no 1 1 ¢7 2— (x)(x-p) dx =—=>20° J  -? (—B) 20” 2\nsince the inner term is the expression  for the variance.  The entropy is therefore  as follows:\nH (x) == +logov2n\nSolution  #7.24\nThe standard approach is (1) to construct  a large dataset with the variable of interest (purchase\nor not) and relevant covariates  (age, gender, income, etc.) for a sample of platform users and\n(2) to build a model to calculate  the probability  of purchase  of each item. Propensity  models are a\nform of binary  classifier,  so any model that can accomplish  this could be used to estimate  a customer’s\npropensity  to buy the product.\nIn selecting  a model, logistic regression  offers a straightforward  solution with an easily interpretable\nresult: the resulting  log-odds  is a probability  score for, in this case, purchasing  a particular  item. However,\nit cannot capture  complex  interaction  effects between  different  variables  and could also be numerically\nunstable  under certain conditions  (i.e., correlated  covariates  and a relatively  small user base).\nAn alternative  to logistic  regression  would be to use a more complex  model,  such as a neural network\nor an SVM. Both are great with dealing  with high-dimensional  data and with capturing  the complex\ninteractions  that logistic regression  cannot. However,  unlike logistic regression,  neither 1s easy to\nexplain,  and both generally  require  a large amount  of data to perform  well.\nA good compromise  1s tree-based  models,  such as random  forests,  which  are typically  highly  accurate\nand are easily understandable.  With tree-based  models,  the features  which have the highest  influence\non predictions  are readily  perceived,  a characteristic  that could be very useful in this particular  case.\nSolution  #7.25\nBoth Gaussian  naive Bayes (GNB) and logistic  regression  can be used for classification.  The two\nmodels  each have advantages  and disadvantages,  which provide  the answer  as to which to choose\nunder what circumstances.  These are discussed  below,  along with their similarities  and differences:\nAdvantages:\n1. GNB requires  only a small number  of observations  to be adequately  trained;  it is also easy to\nuse and reasonably  fast to implement;  interpretation  of the results  produced  by GNB can also be\nhighly useful.\n2. Logistic  regression  has a simple interpretation  in terms of class probabilities,  and it allows\ninferences  to be made about features  (i.e., variables)  and identification  of the most relevant  of\nthese with respect  to prediction.\nDisadvantages:\n|. By assuming  features  (i.e., variables)  to be independent,  GNB can be wrongly  employed  in\nproblems  where that does not hold true, a very common  occurrence.\n2. Not highly flexible,  logistic  regression  may fail to capture  interactions  between  features  and so\nmay lose predicuon  power. This lack of flexibility  can also lead to overfitting  if very little data\nare available  for training.\nDifferences:\n1. Since logistic  regression  directly  leams P(Y|X),  it is a discriminative  classifier,  whereas  GNB\ndirectly  estimates  P(Y) and P(X|Y)  and so is a generative  classifier.\n132 Ace the Data Science  Interview | Machine  Learning"
  },
  {
    "page_number": 145,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\n2. Logistic  regression  requires  an optimization  setup (where weights  cannot be learned  directly\nthrough  counts),  whereas  GNB requires  no such setup.\nSimilarities:\n1. Both methods  are linear decision  functions  generated  from training  data.\n2. GNB’s  imphed  P(Y1X)  is the same as that of logistic  regression  (but with particular  parameters).\nGiven  these advantages  and disadvantages,  logistic  regression  would  be preferable  assuming  training\nprovided  data size 1s not an issue, since the assumption  of conditional  independence  breaks down\nif features  are correlated.  However,  in cases where training  data are limited  or the data-generating\nprocess  includes  strong  priors, using GNB may be preferable.\nSolution  #7.26\nAssume  we have  k clusters  and n sample  points:  x,...x,, H,...M,\nThe loss function  then consists  of minimizing  total error using a squared  L, norm (since it is a good\nway to measure  distance)  for all points within  a given cluster:\nk\nb= D-H\nJ=1 xe,\nTaking the derivatives  yields the following:  oe = (x, - M,) (x; -[,)= y= ¥ 2 2(x, —U,)\nH,, Hi, x,€S;, x, €S,\nFor batch gradient  descent,  the update step is then given by the following:\nU, =\",+e »y 2(x\nx,€S,\nHowever,  for stochastic  gradient  descent,  the update step is given by the following:\nHh, =H, + E(x; — H,)\nSolution  #7.27\nThe idea behind the kernel trick is that data that cannot be separated  by a hyperplane  in its current\ndimensionality  can actually  be linearly separable  by projecting  it onto a higher dimensional  space.\nk(x, y) = o(x)’ 60)\nand we can take any data and map that data to a higher dimension  through a variety of functions  9.\nHowever,  if is difficult to compute,  then we have a problem -— instead, it is desirable if we can\ncompute  the value of & without  blowing  up the computation.\nFor instance, say we have two examples  and want to map them to a quadratic space. We have the\nfollowing:\nbh\n—\n(x,,%,)  =\nBR & RRew “NM & -_\nKaet\nnNl\nand we can use the following:  A(x, y) = (1 + x’ y)? = o(x)’ 60)\nAce the Data Science  Interview 133"
  },
  {
    "page_number": 146,
    "content": "CHAPTER  7 : MACHINE  LEARNING\nIf we now change  n = 2 (quadratic)  to arbitrary  n, we can have arbitrarily  complex  o. As long as we\nperform  computations  in the original  feature space (without  a feature transformation),  then we avoid\nthe long compute  time while still mapping  our data to a higher  dimension!\nIn terms of which kernel to choose,  we can choose  between  linear and nonlinear  kernels,  and these will\nbe for linear and nonlinear  problems,  respectively.  For linear problems,  we can try a linear or logistic\nkernel. For nonlinear  problems,  we can try either radial basis function  (RBF) or Gaussian  kernels.\nIn real-life  problems,  domain knowledge  can be handy — in the absence  of such knowledge,  the\nabove defaults are probably  good starting points. We could also try many kernels, and set up a\nhyperparameter  search (a grid search, for example)  and compare  different  kernels to one another.\nBased on the loss function  at hand, or certain performance  metrics  (accuracy,  F1, AUC of the ROC\ncurve, etc.), we can determine  which kernel is appropriate.\nSolution  #7.28\nAssume  we have some dataset  X consisting  of n 1.1.d observations:  x,, ..., x\n(78)Our likelihood  function is then p(X|p,07)  = [yc  \\u,07) where N(x, \\,07) = 5 e260\ni=l Ov 41\nand therefore  the log-likelihood  is given by:\nn\n. 1\nHo\") \"86?log p(X |t,0\"] = ¥ log N(x, (x, - Lu) — = logo” — 5 loen\ni=l 4 t=]\nTaking the derivative  of the log-likelihood  with respect to p and setting the result to 0 yields the\nfollowing:\ndlogp(X|u.o’)  1 -\nSimplifying  the result yields: 5 x, = yi = nt, and therefore  the maximum  likelihood  estimate for\nWis given by: i=! i=l\n]\nh=— Dox t\ni=)\nTo obtain  the variance,  we take the derivative  of the log likelihood  with respect  to o? and set the result\nequal to 0\ndlog p(X|p,0°) 1x 2 on\ndo” — 26' dM -H) 20°\n; a ae _ idly, 2 nSimplifying  yields the following:  57 Dt —y) =\ni=]\n¥ (x —p)=no®\nre\nThe maximum  likelihood  estimate  for the variance  is thus given by the following:\na =— Si (x, - a)\n1-1\n134 Ace the Data Science  Interview  | Machine  Learning"
  },
  {
    "page_number": 147,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nSolution  #7.29\nThe GMM model assumes  a Gaussian  probability  distribution  function,  across  K classes:\nK\nk=]\nwhere the x coefficients  are the mixing  coefficients  on the clusters  and are normalized  so that they\nsum to 1.\nThe posterior  probabilities  for each cluster  are given by Bayes’  rule and can be interpreted  as “what\nis the probability  of being in class k given the data x”:\nz, = p(kix) = p(k) p(x\\|k)\n* p(x)\ntN (x|M,.2,)\nel 1,N (x|H,,2,)\nThe unknown  set of parameters  8 consists  of the mean and variance  parameters  for each of the K\nclasses,  along with the K coefficients.  The likelihood  is therefore  given by:\np(lx)=[]o@)=[]¥  x (xl. 35)\nt=] k=)and hence: z, = p(kl|x)  =\nn K\nand therefore  the log-likelihood  is log p(6|X)  = log} x, N(x|u,,,)\nt=1 k=1\nThe parameters  can be calculated  iteratively  using expectation-maximization  and the information\nabove.  After the model has been trained,  for any new transaction  we can then calculate  the posterior\nprobabilities  of any new transactions  over the K classes as above. If the posterior  probabilities\ncalculated  are low, then the transaction  most likely does not belong  to any of the K classes,  so we can\ndeem it to be fraudulent.\nSolution  #7.30\nStep 1: Clarify  What Churn Is & Why It’s Important\nFirst, it is important  to clarify  with your interviewer  what churn means.  Generally,  the word “churn”\ndefines  the process  of a platform’s  loss of users over time.\nTo determine  what qualifies  as a churned  user at Robinhood,  it’s helpful  to first follow  the money  and\nunderstand  how Robinhood  monetizes.  One primary  way is by trading  activity — whether  itis through\ntheir Robinhood  Gold offering  or order flow sold to market  makers  like Citadel.  Thus, a cancellation\nof their Robinhood  Gold membership  or a long period of no trading  activity  could constitute  churn.\nThe other way Robinhood  monetizes  is through a user’s account balance. By collecting  interest\non uninvested  cash and making stock loans to counterparties,  Robinhood  is incentivized  to have\nuser’s manage  a large portfolio  on the platform.  As such, a negligible  account  balance  or portfolio\nmaintained  over a period of time — say a quarter  — could constitute  a churned  user.\nChurn is a big deal, because  even a small monthly  churn can compound  quickly  over time: consider\nthat a 2% monthly  churn translates  to almost a 27% yearly churn. Since it 1s much more expensive\nto acquire  new customers  than to retain existing  ones, businesses  with high churn rates will need to\ncontinually  dedicate  more financial  resources  to support new customer  acquisition,  which is cosily,\nAce the Data Science  Interview 135"
  },
  {
    "page_number": 148,
    "content": "CHAPTER  7: MACHINE  LEARNING\nand therefore  to be avoided if possible.  So, if Robinhood  is to stay ahead of WeBull,  Coinbase,  and\nTD Ameritrade,  predicting  who will churn, and then helping  these at-risk users, is beneficial.\nAfter you’ve worked with your interviewer  to clarify what churn is in this context, and why it’s\nimportant  to mitigate, be sure to ask the obvious question: how is my model output going to be\nused? If it’s not clear how the model will be used for the business,  then even if the model has great\npredictive  power, it is not useful in practice.\nStep 2: Modeling  Considerations\nAny classification  algorithm  could be used to model whether  a particular  customer  would be in the\nchurned  or active state. However,  models that produce  probabilities  (e.g., logistic regression)  would\nbe preferable  if the interest is in the probability  of the customer’s  loss rather than simply  a final\nprediction  about whether  the customer  will be lost or not.\nAnother  key consideration  when picking a model in this instance would be model explainability.\nThis is because company  representatives  likely want to understand  the main reasons for chum to\nsupport marketing  campaigns  or customer  support programs.  In this case, interpretable  models such\nas logistic regression,  decision  trees, or random  forests should be used. However,  if by talking  with\nthe interviewer  you learn that it’s okay to simply detect churn, and that explainability  isn’t required,\nthen less interpretable  models  like neural networks  and SVMs can work.\nStep 3: Features  We'd Use to Model Churn\nSome feature  ideas include:\n¢ Raw Account  Balance:  \\s the portfolio  value close to the threshold  where it doesn’t  make sense\nto check the app anymore  (say under $10)?\n¢ Account  Balance  Trend: \\s there a negative  trend - they used to have $20k in their account,  but\nhave steadily  been withdrawing  money  out of the account?\n¢ Experienced  Heavy  Losses:  Maybe  they recently  lost a ton of money  trading  Dogecoin,  making\nthem want to quit investing  and rethink  their trading  strategies  (and their life).\n¢ Recent  Usage Patterns:  Maybe they used to be a Daily Active User, but recently  have started\nlogging  in less and less — a sign that the app isn’t as important  any more.\n¢ User Demographics:  Standard  user profile variables  like age, gender,  and location  can also be\nused to model churn.\nI's also wise to collaborate  with business  users to see their perspectives  and to look for basic heuristics\nthey might use that can be factored  into the model. For example,  maybe  the customer  support  team\nhas some insights  into signals  that indicate  a user will chum out.\nAfter running  the model,  it is good to double-check  the results  to see if the feature  importance  roughly\nmatches  what we would intuitively  expect; for example,  it is unlikely  that a higher  balance  would\nresult in a higher  likelihood  of churn.\nStep 4: Deploying  the Churn Model\nWe want lo make sure the various  metrics  of interest  (confusion  matrix,  ROC curve, F1 score,  etc.) are\nsatisfactory  during  offline  training  before deploying  the model in production.  As with any prediction\ntask, 1 1s Important  to monitor  model performance  and adjust features  as necessary  whenever  there\nIs new data or feedback  from customer-facing  teams. This helps prevent  model degradation,  which\nIs a common  problem  in real-world  ML systems.  We'd also continuously  conduct  error analysis  by\n136 Ace the Data Science  Interview | Machine  Learning"
  },
  {
    "page_number": 149,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nlooking  at where the model is wrong, in order to keep refining  the model. Finally,  we’d also make\nsure to A/B test our model  to validate  its impact.\nSolution  #7.31\nIn matrix  form, we assume  Y is distributed  as multivariate  Gaussian:  Y ~ N(XB,  07D)\nThe likelihood  of Y given above is L(B,o”)  = (20%n) 2 exp( -5-7(XB  ~Y) (XB- ¥)|\nOf which  we can take the log in order to optimize:\nlog L(B,o”) = - > log(20°n)  - =7(XB  ~Y)' (XB-Y)\nNote that, when taking  a derivative  with respect  to B, the first term is a constant,  so we can ignore  It,\nmaking  our optimization  problem  as follows:\n1 '\narg max-——,(XB-Y)  (XB-Y)\n8 20\nWe can ignore  the constant  and flip the sign to rewrite  as the following:  arg min(XB  -Y)' (XB-Y)\nB\nwhich is exactly  equivalent  to minimizing  the sum of the squared  residuals.\nSolution  #7.32\nPCA aims to reconstruct  data into a lower dimensional  setting,  and so it creates a small number  of\nlinear combinations  of a vector  x (assume  it to be p dimensional)  to explain  the variance  within x.\nMore specifically,  we want to find the vector w of weights  such that we can define the following\nlinear  combination:\nJi — Ww, x = > w, x;\nj=l\nsubject  to the constraint  that w is orthonormal  and that the following  is true:\ny, 18 uncorrelated  with y,, var(y,)  is maximized\nHence,  we perform  the following  procedure,  in which we first find y, = w, x with maximal  variance,\nmeaning  that the scores are obtained  by orthogonally  projecting  the data onto the first principal\ndirection,  w,. We then find y, =w,x, is uncorrelated  with y, and has maximal  variance,  and we\ncontinue  this procedure  iteratively  until ending  with the kth dimension  such that\ny,»--y,  explain  the majority  of variance,  k << p\nTo solve, note that we have the following  for the variance  of each y, utilizing  the covariance  matrix\nof x: var(y,) = w? var(x)w,  = w; Xw,\nWithout  any constraints,  we could choose  arbitrary  weights  to maximize  this variance,  and hence we\nwill normalize  by assuming  orthonormality  of w, which guarantees  the following:  w/ w.—1\nWe now have a constrained  maximization  problem where we can use Lagrange multipliers.\nSpecifically,  we have the function w/ Zw. — 4(w; w, - 1) = 0, which we differentiate  with respect to\nw to solve the optimization  problem:\n2 yt Sw, ~h,(w7w,  -1) = Dw, -A,(w,)=0\ndw,\nt\nAce the Data Science  Interview 137"
  },
  {
    "page_number": 150,
    "content": "CHAPTER  7 : MACHINE  LEARNING\nSimplifying,  we see that: Zw, =A(w,)\nThis is the result of an eigen-decomposition,  whereby  w is the eigenvector  of the covariance  matrix\nand A is this vector’s associated  eigenvalue.  Noting that we want to maximize  the variance  for\neach y, we pick: w; Zw, = w;A,w, = h,w;  w, = A. to be as large as possible.  Hence, we choose the\nfirst eigenvalue  to be the first principal  component,  the second  largest eigenvalue  to be the second\nprincipal  component,  and so on.\nSolution  #7.33\nLogistic  regression  aims to classify  X into one of k classes  by calculating  the following:\n= il|X =x)PIC =B,, + BP xP(C = K|X  =x)\nTherefore,  the model is equivalent  to the following,  where the denominator  normalizes  the numerator\nover the & classes:log\nePuo +Pj x\nt=]\nThe log-likelihood  over N observations  in general  is the following:\nL{ )\nUse the following  notation  to denote  classes 1 and 2 for the two-class  case:P(C=k|X  =x)=\ny, = 1if the class is 1, otherwise  0\nThen we have the following:  P(C = 2| X = x,0)=1-P(C=1|X=  x,, 9)\nUsing the following  notation,  P(C = 1|X = x,, 0) = p(x.) such that the log-likelihood  can be written\nas follows:\nB)= ¥(y log p(x,) + (1— y,)log (1— p(x,))]\nt=]\nSimplifying  yields the following:\n= Y1on( ~ p(x;))+ Ys 085 _\nSubstituting  for the probabilities  yields the following:\nL(B)= ¥ log (1 — efor thx: ) iy y;(B, +B,x,)\nt=1 i=]\nTo maximize  this log-likelihood,  take the derivative  and set it equal to 0\nNOp = lle, ~ la) =0\nt=)\nN A) N N eho tBix,We note that: = —= 3 L108  (1 — eFo fx) = 35 ¥ - log  (1 + e&  *8*) = »y -\n=] r=]i=]] + eo +8, x;\nN\nwhich is equivalent  to the latter half of the above expression:  -y p(x\n138 Ace the Data Science  Interview  | Machine  Learning"
  },
  {
    "page_number": 151,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nThe solutions  to these equations  are not closed form, however,  and, hence, the above should be\niterated  until convergence.\nSolution  #7.34\nStep 1: Clarify  Details  of Discover  Weekly\nFirst we can ask some clarifying  questions:\n¢ What is the goal of the algorithm?\n* Do we recommend  just songs, or do we also include  podcasts?\n* — Is our goal to recommend  new music  to a user, and push their musical  boundaries?  Or is it to just\ngive them the music they’IIl  want to listen to the most, so they spend more time on Spotify?  Said\nmore generally,  how do we think about the trade-off  of exploration  versus  exploitation?\n* What are the various  service-level  agreements  to consider  (e.g., does this playlist  need to change\nweek to week if the user doesn’t  listen to it?)\n¢ Do new users get a Discover  Weekly  playlist?\nStep 2: Describe  What Data Features  You'd Use\nThe core features  will be user-song  interactions.  This is because  users’  behaviors  and reactions  to various\nsongs should  be the strongest  signal for whether  or not they enjoy a song. This approach  is similar  to the\nwell-known  use case for movie recommendations  on Netflix,  with several  notable  differences:\n¢ Feedback  for music does not have a |-to-5 rating scale as Netflix  does for its movies.\n* Music may be subject  to repeated  consumption  (i.e., people may watch a movie once or twice\nbut will listen to a song many times).\n¢ Miusic  has a wider variety  (1.e., niche music).\n« The scale of music catalog  items is much larger than movies  (i.e., there are many more songs\nthan movies).\nThere 1s also a variety of other features  outside  of user-song  interactions  that could be interesting  to\nconsider.  For example,  we have plenty of metadata  about the song (the artist, the album, the playlists\nthat include that song) that could be factored  in. Additionally,  potential  audio features in the songs\nthemselves  (tempo,  speechiness,  instruments  used) could be used. And finally,  demographic  information\n(age, gender, location,  etc.) can also impact music listening  preferences  — people living in the same\nregion are more likely to have similar  tastes than someone  living on the other side of the globe.\nStep 3: Explain  Collaborative  Filtering  Model Setup\nThere are two types of recommendation  systems  in general: collaborative  filtering  (recommending\nsongs that similar users prefer) and content-based  recommendation  (recommending  similar  types of\nsongs).  Our answer  will use collaborative  filtering.\nCollaborative  filtering  uses data trom feedback  users have provided  on certain items (in this case,\nsongs) in order to decide recommendations.  Therefore,  a user-song  matrix (or a user-artist  matrix)\nwould constitute  the dataset  at hand, with the rows of the dataset  being users and the columns  various\nsongs. However,  as discussed  in the prior section, since explicit song ratings are lacking, we can\nproxy liking a song by using the number  of times a user streamed  it. This song play count is stored\nfor every entry in the user-song  matrix.\nAce the Data Science  Interview 139"
  },
  {
    "page_number": 152,
    "content": "CHAPTER  7: MACHINE  LEARNING\nThe output of collaborative  filtering is a latent user matrix and a song matrix. Using vectors from\nthese matrices,  a dot product denotes the relevance  of a particular  song to a particular  user. This\nprocess  can be used directly  for recommendation  at the user level, where we sort by relevance  scores\non songs that the user has not yet streamed.  You can also use these vectors to assess similarity\nbetween  different  users and different  songs using a method  such as ANN (K-nearest  neighbors).\nStep 4: Additional  Considerations\nAlso relevant  to this discussion  are the potential  pros and cons of collaborative  filtering.  For example,\none pro is that you can run it in a scalable  manner  to find correlations  behind user-song  interactions.\nOn the flip side, one con is the “cold start” problem,  where an existing  base of data is needed  for any\ngiven user.\nAnother important  consideration  is scale. Since Spotify has hundreds  of millions of users, the\nDiscover  Weekly algorithm  could be updated  in batch for various users at different  times to help\nspeed up data processing  and model training.\nAnother  consideration  is the dynamic  nature  of the problem;  the influx of new users and songs,  along\nwith fast-changing  music trends,  would necessitate  constant  retraining.\nLastly, it is important  to consider  how you can measure  and track the impact  of this system  over time\n— collaborative  filtering  doesn’t  come with clear metrics  of pertormance.  Ideally,  you'd use an A/B\ntest to find that users with the improved  recommendations  had increased  engagement  on the platform\n(as measured  by time spent listening,  for example).\nSolution  #7.35\nWe are attempting  to solve for V ar(B)\nRecall that the parameter  estimates  have the following  closed-form  solution  in matrix form:\naA\nB =(XTX)1XTy\nTo derive the variance  of the estimates,  recall that for any given random  variable  X:\nVar(X)  = E[X*] — E[X]?\nTherefore,  we have the following:  Var(8) = E(B?) = EIB]?\nWe can evaluate  the second  term since we can assume  the parameter  estimates  are unbiased.\nTherefore,  E[B] = B\nVar(B) = E(B’) -\nSubstituting  into the closed-form  solution  yields the following:  Var(B)  = E((X! X)) X\" y)*] - BP\nSince least squares  assumes  that: y = XB + € where « ~ N(0, 6”), we have the following:\nVar(B) = E(X? Xy\" X(XB + OF] - B?\nVar(B) = E{(X? X)* X\"(Xp) + (X7X)!  X7 6)?] - B?\nNote that: (X'X)! X7 X= 1\nSo simplifying  yields: Var(B)  = E[(B + (X7 X)7 X7 ©)?] — Be\nVar(B) = B? + E[(X\" X) | X76)\"] - 8\nwhere the middle  terms were canceled  since the expectation  of the error term is 0. Canceling  out the\nfirst and last squared  terms and simplifying  the middle  part ylelds the following:\nVar(B) = E[(X7X)  1X\" X (XTX) 12] = (XTX)  Ele] = (X7 XY 6?\n140 Ace the Data Science  Interview | Machine  Learning"
  },
  {
    "page_number": 153,
    "content": "SQL & DB Design\nCHAPTER  8\nUpon hearing  the term “data scientist,”  buzzwords  such as predictive  analytics,  big data,\nand deep learning  may leap to mind. So, let’s not beat around  the bush: data wrangling\nisnt the most  fun or sexv part of being a data scientist.  However,  as a data scientist,  you\nwill likely spend a great deal of your time working  writing  SOL queries to retrieve  and\nanalyze  data. As such, almost every company  you interview  with will test your ability to\nwrite SOL queries.  These questions  are practically  guaranteed  if vou are interviewing  for a\ndata scientist  role on a product  or analytics  team, or if vou’re  after a data science-adjacent\nrole like data analyst  or business  intelligence  analyst. Sometimes,  data science  interviews\nmay go beyond  just writing  SQL queries,  and cover the basic  principles  of database  design\nand other big data systems. This focus on data architecture  is particularly  true at early-\nstage startups,  where data scientists  often take an active role in data engineering  and data\ninfrastructure  development.\nSQL\nHow SQL Interview  Questions  Are Asked\nBecause  most analytics  workflows  require quick slicing and dicing of data in SQL. interviewers\nwill often present you with hypothetical  database  tables and a business  problem,  and then ask you\nto write SQL on the spot to get to an answer.  This is an especially  common  early interview  question,\nAce the Data Science  Interview 141"
  },
  {
    "page_number": 154,
    "content": "CHAPTER  8: SQL & DB DESIGN\nconducted  via a shared coding  environment  or through  an automated  remote  assessment  tool. Because\nof the many different flavors of SQL used by industry, these questions  aren't usually testing your\nknowledge  of database-specific  syntax or obscure  commands.  Instead,  interviews  are designed  to test\nyour ability to translate  reporting  requirements  into SQL.\nFor example,  at a company  like Facebook,  you might be given  a table on user analytics  and asked to\ncalculate  the month-to-month  retention.  Here, it’s relatively  straightforward  what the query should\nbe, and you’re expected  to write it. Some companies  might make their SQL interview  problems\nmore open-ended.  For example,  Amazon  might give you tables about products  and purchases  and\nthen ask you to list the most popular  products  in each category.  Robinhood  may give you a table and\nask why users are churning.  Here, the tricky part might not be just writing the SQL query, but also\nfiguring  out collaboratively  with the interviewer  what “popular  products”  or “user churn” means in\nthe first place.\nFinally, some companies  might ask you about the performance  of your SQL query. While these\ninterview  questions  are rare, and they don’t expect you to be a query optimization  expert, knowing\nhow to structure  a database  for performance,  and avoid slow running  queries,  can be helpful. This\nknowledge  can come in handy as well when you are asked more conceptual  questions  about database\ndesign  and SQL.\nTips for Solving  SQL Interview  Questions\nFirst off. don’t jump into SQL questions  without  fully understanding  the problem.  Before you start\nwhiteboarding  or typing out a solution,  it’s crucial to repeat back the problem  so you can be sure\nyou’ve understood  it correctly.  Next, try to work backwards,  especially  if the answer  needs multiple\njoins, subqueries,  and common  table expressions  (CTEs).  Don’t overwhelm  yourself  trying to figure\nout the multiple  parts of the final query at the same time. Instead,  imagine  you had all the information\nyou needed in a single table, so that your query was just a single SELECT  statement.  Working\nbackwards  slowly trom this ideal table, one SQL statement  at a time, try to end up with the tables\nyou originally  started  with.\nFor more general problem-solving  tips, be sure to also read the programming  interview  lps tn the\ncoding chapter. Most of what applies to solving  coding questions  — like showing  your work and\nasking for help if stuck — applies  to solving  SQL interview  questions  too.\nBasic SQL Commands\nBefore  we cover the must-know  SQL commands,  a quick note — please don’t be alarmed  by minor\nvariations  in syntax between  your favorite  query language  and our PostgreSQL  snippets:\n* CREATE  TABLE:  Creates  a table in a relational  database  and, depending  on what database  you\nuse (e.g., MySQL),  can also be used to define  the table’s  schema.\n¢ INSERT:  Inserts  a row (or a set of rows) into a given table.\n¢ UPDATE:  Modifies  already-existing  data.\n¢ DELETE:  Removes  a row (or a group of rows) from a database.\n* SELECT:  Selects  certain  columns  from a table. A common  part of most queries.\n© GROUP  BY: Groups/aggregates  rows having  the contents  of a specific  column  or set of columns.\n* WHERE:  Provides  a condition  on which to filter before  any grouping  is applied.\n* HAVING:  Provides  a condition  on which to filter after any grouping  is applied.\n142 Ace the Data Science  Interview  | SQL & DB Design"
  },
  {
    "page_number": 155,
    "content": "¢ UNION:  Combines  results  from multiple  SELECT  statements.\nJoins\nImagine  you worked  at Reddit,  and had two separate  tables: users and posts.ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nORDER  BY: Sorts results in ascending  or descending  order according  to the contents  of a\nspecific  column  or set of columns.\n¢ DISTINCT:  Returns  only distinct  values.\nReddit  users Reddit  Posts\nAa column_name  |= type Aa _ column_name  |= type\nuser_id integer post id integer\ncountry string user_id integer\nactive_  status boolean subreddit_id integer\njoin_ time datetime title string\nbody string\nactive_status boolean\npost_time datetime\nJoins are used to combine  rows from multiple  tables based on a common  column.  As you can see, the\nuser_id  column  is the common  column  between  the two tables and links them; hence it is known as\na join key. There are four main types of joins:\nINNER  JOIN OUTER  JOIN LEFT JOIN RIGHT  JOIN\nINNER  JOIN\nInner joins combine  multiple  tables and will preserve  the rows where column values match in the\ntables being combined.  The word INNER is optional  and is rarely used because  it’s the default type\nof join. As an example,  we use an inner join to find the number of Reddit users who have made a\npost:\nSELECT\nCOUNT (DISTINCT  u.user_1id)\nFROM\nusers u\nJOIN posts p ON u.user_ id = p.user_id\nA self join is a special case of an inner join where  a table joins itself. The most common  use case for\na self  join is to look at pairs of rows within the same table.\nAce the Data Science  Interview 143"
  },
  {
    "page_number": 156,
    "content": "CHAPTER  8: SQL & DB DESIGN\nOUTER  JOIN\nOuter joins combine  multiple  tables by matching  on the columns  provided,  while preserving  all rows.\nAs an example of an outer join, we list all inactive users with posts and all inactive posts from any\nuSer:\nSELECT\n*\nFROM\nusers u\nOUTER JOIN posts p ON u.user id = p.user_id\nWHERE\nu.active  status = False\nOR p.active  status = False\nLEFT  JOIN\nLeft joins combine  multiple  tables by matching  on the column names provided,  while preserving  all\nthe rows from the first table of the join. As an example,  we use a left join to find the percentage  of\nusers that made a post:\nSELECT\nCOUNT  (\nDISTINCT  CASE\nWHEN p.post_id  IS NOT NULL THEN u.user_id\nEND\n) / COUNT(*)  AS pct_users\nFROM\nusers u\nLEFT JOIN posts p ON u.user_ id = p.user_id\nRIGHT  JOIN\nRight joins combine  multiple  tables by matching  on the column  names provided,  while preserving  all\nthe rows from the second table of the join. For example,  we use the right join to find the percentage\nof posts made where the user is located  in the U:S.:\nSELECT\nCOUNT  (\nDISTINCT  CASE\nWHEN u.country  = ‘US’ THEN p.post_id\nEND\n) / COUNT(*)  AS pct posts\n\"ROM\nusers uU\nRIGHT JOIN posts p ON u.user  id = p.user id\n144 Ace the Data Science  Interview  | SQL & DB Design"
  },
  {
    "page_number": 157,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nJoin Performance\nJoins are an expensive  operation  to process,  and are often bottlenecks  in query runtimes.  As such, to\nwrite efficient  SQL, you want to be working  with the fewest  rows and columns  before  joining  two\ntables together.  Some general  tips to improve  join performance  include  the following:\n* Select specific  fields instead  of using SELECT  *\n¢ Use LIMIT  in your queries\n* Filter and aggregate  data before  joining\n* Avoid  multiple  joins in a single  query\nAdvanced  SQL Commands\nAggregation\nFor interviews,  you need to know how to use the most common  aggregation  functions  like COUNT,\nSUM,  AVG, or MAX:\nSELECT  COUNT  (*) FROM users ...\nFiltering\nSQL contains  various  ways to compare  rows, the most common  of which use = and <> (not equal),\n>, and <, along with regex and other types of logical  and filtering  clauses  such as OR and AND. For\nexample,  below  we filter to active Reddit  users from outside  the U:S.:\nSELECT\n*\nFROM\nusers\nWHERE\nactive  status = True\nAND country  <> ‘US’\nCommon  Table Expressions  and Subqueries\nCommon  Table Expressions  (CTEs)  define a query and then allow it to be referenced  later using an\nalias. They provide  a handy way of breaking  up large queries  into more manageable  subsets  of data.\nFor example,  below is a CTF. which gets the number  of posts made by each user, which is then used\nto get the distribution  of posts made by users (1.¢., 100 users posted 5 times. 80 users posted 6 times,\nand so on):\nWITH user post_count  AS (\nSELECT\nusers.user  id,\nCOUNT (post _id) AS num posts\nFROM\nAce the Data Science  Interview 145"
  },
  {
    "page_number": 158,
    "content": "CHAPTER  8: SQL & DB DESIGN\nusers\nLEFT JOIN posts on users.user_id  = posts.user  id\nGROUP BY\n1\nSELECT\nnum posts,\nCOUNT  (*) as num_users\nFROM\nuser post_count\nGROUP BY\n1\nSubqueries  serve a similar function  to CTEs, but are inline in the query itself and must have a unique\nalias for the given scope.\nSELECT\nnum posts,\nCOUNT  (*) AS num_users\nFROM\n(\nSELECT\nusers.user  id,\nCOUNT (post _id) AS num_posts\nFROM\nusers\nLEFT JOIN posts on users.user  id = posts.user_id\nGROUP BY\n1\nCTEs and subqueries  are mostly similar, with the exception  that CTEs can be used recursively.  Both\nconcepts  are incredibly  important  to know and practice, since most of the harder SQL interview\nquestions  essentially  boil down to breaking  the problem  into smaller  chunks of CTEs and subqueries.\nWindow  Functions\nWindow  functions  perform  calculations  across a set of rows, much like aggregation  functions,  but do\nnot group those rows as aggregation  functions  do. Therefore,  rows retain their separate  identities  even\nwith aggregated  columns.  Thus, window  functions  are particularly  convenient  when we want to use\nboth aggregated  and non-aggregated  values at once. Additionally,  the code is often easier to manage\nthan the alternative:  using group by statements  and then performing  joins on the original  input table.\nSyntax-wise,  window functions  require the OVER clause to specify a particular  window. This\nwindow  has three components:\n146 Ace the Data Science  Interview  | SQL & DB Design"
  },
  {
    "page_number": 159,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\n* Partition  Specification:  separates  rows into different  partitions,  analogous  to how GROUP  BY\noperates.  This specification  is denoted  by the clause  PARTITION  BY\n* Ordering  Specification:  determines  the order in which  rows are processed,  given by the clauseORDER  BY\n* Window  Frame Size Specification:  determines  which Sliding window  of rows should be\nprocessed  for any given row. The window  frame  defaults  to all rows within  a partition  but can be\nspecified  by the clause ROWS  BETWEEN  (start, end)\nFor instance,  below we use a window  function  to sum up the total Reddit  posts per user, and then add\neach post_count  to each row of the users table:\nSELECT\n*\nt\nSUM(posts)  OVER (PARTITION  BY user _id) AS post count\nFROM\nusers u\nLEFT JOIN posts p ON u.user_  id = p.user  id\nNote that a comparable  version  without  using window  functions  looks like the following:\nSELECT\na.*,\nb.post  count\nFROM\nusers a\nJOIN (\nSELECT\nuser id,\nSUM(posts)  AS post count\nFROM\nusers u\nLEFT JOIN posts p ON u.user_  id = p.user  id\nGROUP BY\n1\n) b ON a.user  id = b.user  id\nAs you can see, window  functions  tend to lead to simpler  and more expressive  SQL.\nLAG and LEAD\nTwo popular  window  functions  are (LAG)  and (LEAD).  These are both positional  window  functions,\nmeaning  they allow you to refer to rows after the current  row (LAG),  or rows before the current  row\n(LEAD).  The below example  uses LAG so that for every post, it finds the time difference  between\nthe post at hand, and the post made right before it in the same subreddit:\nAce the Data Science  Interview"
  },
  {
    "page_number": 160,
    "content": "CHAPTER  8: SQL & DB DESIGN\nSELECT\np.*,\nLAG(post  time, 1) OVER (\nPARTITION  BY user_id,\nsubreddit_  id\nORDER BY\npost time ASC\n) AS prev_subreddit_post_time\nFROM\nposts p\nRANK\nSay that for each user, we wanted to rank posts by their length. We can use the window function\nRANK()  to rank the posts by length for each user:\nSELECT\n*\nU\nRANK () OVER ({\nPARTITION  BY user id\nORDER BY\nLENGTH  (body) DESC\n) AS rank\nKF’ ROM\nusers u\nLEFT JOIN posts p ON u.user_  id = p.user_id\nDatabases  and Systems\nAjthough  knowing  all of the database’s  inner workings  isn’t strictly necessary,  having a high-level\nunderstanding  of basic database  and system design concepts is very helptul. Database  interview\nquestions  typically  do not involve minutiae  about specific databases  but, instead, focus on how\ndatabases  generally  operate  and what trade-offs  are made during schema  design. For example,  you\nmight be asked how you'd set up tables to represent  a real-world  situation,  like storing data if you\nworked at Reddit. You'd need to detine the core tables (users, posts, subreddits)  and then define\nthe relationships  between. For the Reddit example.  posts would have a user id column for the\ncorresponding  user that made the post.\nYou also may be asked to choose which columns  should be indexed,  which allows for more rapid\nlookup  of data. For the Reddit example.  you would want to index the user _id column,  since it’s likely\nheavily  used as a join key across many important  queries.\nWhile data science  interviews  don’t go into system  design concepts  as deeply  or as often as software\nengineering  and data engineering  interviews,  it can still show up from time to time. This ts particularly\nthe case 1f you are joming a smaller  company,  where your data science job might involve  creating  and\n148 Ace the Data Science  Interview  | SQL & DB Design"
  },
  {
    "page_number": 161,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nmanaging  data pipelines.  Besides  generic  questions  about scaling  up data infrastructure,  you might\nbe asked conceptual  questions  about popular  large-scale  processing  frameworks  (Hadoop,  Spark) or\norchestration  frameworks  (Airflow,  Luigi) —- especially  if you happen  to list these technologies  on\nyour resume.\nKeys & Normalization\nPrimary  keys ensure  that each entity has its own unique  identifier,  i.e., no rows ina table are duplicated\nwith respect  their primary  key. Foreign  keys, on the other hand, establish  mappings  between  entities.\nBy using a foreign key to link two related tables, we ensure that data is only stored once in the\ndatabase.  For the Reddit  example,  in the posts table, the post_id  column  is the primary  key, and each\npost has a user_id  which is a foreign  key to the users table.\nItem Primary  Key Foreign  Key\nConsists  of One or More Columns Yes Yes\nDuplicate  Values  Allowed No Yes\nNULLs  Allowed No Yes\nUniquely  Identify  Rows in a Table Yes Maybe\nNumber  Allowed  Per Table One Zero or More\nIndexed Automatically  Indexed No Index Automatically  created\nKeys allow us to split data efficiently  into separate  tables,  but still enforce  a logical  relationship  between\ntwo tables. rather  than having  everything  duplicated  into one table. This process  of generally  separating\nout data to prevent  redundancy  is called normalization.  Along with reducing  redundancy,  normalization\nhelps you enforce  database  constraints  and dependencies,  which improves  data integrity.\nThe disadvantage  to normalization  is that now we need an expensive  join operation  between  the two\nrelated tables. As such, in high-performance  systems.  denormalization  1s an optimization  technique\nwhere we keep redundant  data to prevent  expensive  join operations.  This speeds up read times, but\nat the cost of having to duplicate  data. At scale. this can be acceptable  since storage 1s cheap, but\ncompute  is expensive.\nWhen normalization  comes up in interviews,  it often concems the conceptual  setup of database\ntables: why a certain entity should have a foreign  key to another  entity. what the mapping  relationship\nis between  two types of records (one-to-one.  one-to-many,  or many-to-many).  and when it might be\nadvantageous  to denormalize  a database.\nProperties  of Distributed  Databases\nTwo concepts,  the CAP theorem and the ACID framework.  are commonly  used to assess theoretical\nguarantees  of databases  and are discussed  in detail below.\nThe CAP theorem provides  a framework  for assessing  properties  of a distributed  database,  although\nonly two of the theorem’s  three specifications  can be met simultaneously.  The name CAP 1s an acronym\nbased on the following  desirable  characteristics  of distributed  databases:\nConsistency:  All clients using the database  see the same data.\nAvailability:  The system is always available, and each request receives a non-error response, but\nthere’s no guarantee  that the response  contains  the latest data.\nPartition  tolerance:  The system functions  even if communication  between nodes ts lost or delayed.\nAce the Data Science  Interview 149"
  },
  {
    "page_number": 162,
    "content": "CHAPTER  8 : SQL & DB DESIGN\nCP Category:  MongoDB, Redis CA Category:  SQL Server, MySQL\nPick Two\nPartition A ) Availability\nTolerance\nAlthough  the CAP theorem  is a theoretical  framework,  one should consider  the real-life  trade-offs\nthat need to be made based on the needs of the business and those of the database’s  users. For\nexample,  the Instagram  feed focuses on availability  and less so on consistency,  since what matters\nis that you get a result instantly  when visiting the feed. The penalty for inconsistent  results isn’t\nhigh. It’s not going to crush users to see @ChampagnePapi’s  last post has 57,486 likes (instead  of\nthe correct 57,598 likes). In contrast,  when designing  the service to handle payments  on Whatsapp,\nyou'd favor consistency  over availability,  because  you'd want all servers  to have a consistent  view of\nhow much money  a user has to prevent  people from sending  money they didn’t have. The downside\nis that sometimes  sending  money takes a minute  or a payment  fails and you are asked to re-try. Both\nare reasonable  trade-offs  in order to prevent  double-spend  issues.\nThe second principle  for measuring  the correctness  and completeness  of a database  transaction  ts\ncalled the ACID  framework.  ACID is an acronym  derived  from the following  desirable  characteristics:\ne Atomicity:  an entire transaction  occurs as a whole or it does not occur at all (i.e., no partial\ntransactions  are allowed).  If a transaction  aborts before completing,  the database  does a\n“rollback”  on all such incomplete  transactions.  This prevents  partial  updates  to a database,  which\ncause data integrity  issues.\n¢ Consistency:  integrity  constraints  ensure  that the database  is consistent  before  and after a given\nlransaction  1s completed.  Appropriate  checks handle any referential  integrity  for primary  and\nforeign  keys.\n¢ fsolation:  transactions  occur in isolation  so that multiple  transactions  can occur independently\nwithout  interference.  This characteristic  properly  maintains  concurrency.\n¢ Durability:  once a transaction  is completed,  the database  is properly  updated  with the data\nassociated  with that transaction,  so that even a system  failure  could not remove  that data from it.\nThe ACID properties  are particularly  important  for online transactional  processing  (OLTP)  systems,\nwhere databases  handle  large volumes  of transactions  conducted  by many users in real time.\nScaling  Databases\nTraditionally,  database  scaling  was done by using full-copy  clusters  where multiple  database  servers\n(cach referred  to as a node within the cluster)  contained  a full copy of the data, and a load balancer\nwould roundrobin  incoming  requests.  Since each database  server  had a full copy of the data, each node\nexperienced  the issues mentioned  in the CAP theorem  discussed  above (especially  during high-load\nperiods).  With the advent of the cloud, the approach  towards  scaling  databases  has evolved  rapidly.\nNowadays,  the cloud makes two main strategies  to scaling  feasible:  vertical  and horizontal  scaling.\n150 Ace the Data Science  Interview  | SQL & DB Design"
  },
  {
    "page_number": 163,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nVertical  scaling,  also known as scaling  up, involves  adding CPU and RAM to existing  machines.\nThis approach  is easy to administer  and does not require  changing  the way a system is architected.\nHowever,  vertical scaling can quickly become prohibitively  expensive,  eventually  limiting the\nscope for upgrades.  This is because  certain  machines  may be close to their physical  limits, making  it\npractically  impossible  to replace  them with more performant  servers.\nIn horizontal  scaling,  also known  as scaling  out, more commodity  machines  (nodes)  are added to the\nresource  pool. In comparison  to vertical  scaling,  horizontal  scaling  has a much cheaper  cost structure\nand has better fault tolerance  than vertical  scaling.  However,  as expected,  there are trade-offs  with\nthis approach.  With many more nodes, you have to deal with issues that arise in any distributed\nsystem,  like handling  data consistency  between  nodes. Therefore,  horizontal  scaling  offers a greater\nset of challenges  in infrastructure  management  compared  to vertical  scaling.\nSharding,  in which  database  rows themselves  are split across nodes in a cluster,  is acommon  example\nof horizontal  scaling.  For all tables,  each node has the same schema  and columns  as the original  table,\nbut the data are stored independently  of other shards.  To split the rows of data, a sharding  mechanism\ndetermines  which node (shard)  that data for a given key should exist on. This sharding  mechanism\ncan be a hash function,  a range, or a lookup  table. The same operations  apply for reading  data as well,\nand so, in this way, each row of data is uniquely  mapped  to one particular  shard.\nRelational  Databases  vs. NoSQL  Databases\nRelational  databases,  like MySQL  and Postgres,  have a table-based  structure  with a fixed, pre-defined\nschema. In contrast,  NoSQL  databases  (named  because  they are “non-SQL”  and “non-relational”)\nstore data in a variety  of forms rather than in a strict table-based  structure.\nSQL Database NoSQL Databa\nee\nColumn Graph\nRelational Key-Value Document\nOne type of NoSQL database is the document  database. MongoDB,  the most popular document\ndatabase, associates  each record with a document.  The document  allows for arbitrarily  complex,\nnested, and varied schemas inside it. This flexibility allows for new fields to be trivially added\ncompared  to a relational  database,  which has to adhere to a pre-defined  schema.\nAnother type of NoSQL database is the graph database. Neo4J is a well-known  graph database,\nwhich stores each data record along with direct pointers  to all the other data records it 1s connected  to.\nAce the Data Science  Interview 151"
  },
  {
    "page_number": 164,
    "content": "CHAPTER  8: SQL & DB DESIGN\nBy making  the relationships  between  the data as important  as storing the data itself, graph databases\nallow for a more natural representation  of nodes and edges, when compared  to relational  databases.\nBASE Consistency  Model\nAnalogous  to the ACID consistency  model for relational  databases,  the BASE model applies to\nNoSQL  databases:\n¢ Basically  Available:  data is guaranteed  to be available;  there will be a response  to any request.\nThis occurs due to the highly distributed  approach  of NoSQL  databases.  However,  the requested\ndata may be inconsistent  and inaccurate.\n¢ Soft State: system’s  state may change over time, even without input. These passive changes\noccur due to the eventual  consistency  property.\n¢ Eventual  Consistency:  data will eventually  converge  to a consistent  state, although  no guarantees\nare made on when that will occur.\nIf you compare  and contrast  ACID and BASE,  you will see that the BASE model puts a stronger  focus\non availability  and scalability  but less of an emphasis  on data correctness.\nMapReduce\nMapReduce  1s a popular  data processing  framework  that allows  for the concurrent  processing  of large\nvolumes  of data. MapReduce  involves  four main steps:\nThe overall  MapReduce  word count process\nNick Likes\nData Science\nNick Likes Data Science\nNick Likes Big Data\nNick Loves Big SeanaNick Likes\nBig Datav1\nNick Loves\nBig SeanNick, 3\nLikes, 2\nData, 2\nScience,  1\nBig, 2\nLoves,  1\nSean, 1\nAASNick, 1\nNick, 1\nLikes, 1\nData, 1\nScience,  1\n‘~——- 31 Likes 1\n~{1 Likes, 1\n--% Data, 1\nNick, 1 -—p{ Data, 1\nLikes, 1\nBig, 1\nData, 1\n5\n{ ,\n‘ }\n| -»| Big, 1\nNick, 1 |\nLoves,  1\nBig, 1\nSean,  1\n1§2 Ace the Data Science  Interview  | SQL & DB Design"
  },
  {
    "page_number": 165,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\n|) Split step: splits up the input data and distributes  it across different  nodes\n2) Map step: takes the input data and outputs  <key, value>  pairs\n3) Shuffle  step: moves  all the <key, value>  pairs with the same key to the same node\n4) Reduce  step: processes  the <key, value>  pairs and aggregates  them into a final output\nThe secret sauce behind  MapReduce’s  efficiency  is the shuffle  step; by grouping  related  data onto the\nsame node, we can take advantage  of the locality  of data. Said another  way, by shuffling  the related\n<key, value>  pairs needed  by the reduce  step to the same node rather  than sending  them to a different\nnode for reducing,  we minimize  node-to-node  communication,  which is often the bottleneck  for\ndistributed  computing.\nFor a concrete  example  of how MapReduce  works,  assume  you want to count the frequency  of words\nin a multi-petabyte  corpus  of text data. The MapReduce  steps are visualized  on left:\nHere’s  how each MapReduce  step operates  in more detail:\n1. Split step: We split the large corpus of text into smaller  chunks and distribute  the pieces to\ndifferent  machines.\n2. Map step: Each worker  node applies  a specific  mapping  function  to the input data and writes\nthe output <key, value> pairs to a memory  buffer. In this case, our mapping  function  simply\nconverts  each word into a tuple of the word and its frequency  (which  is always |). For example,\nsay we had the phrase  “hello  world”  on a single machine.  The map step would  convert  that input\ninto two key value pairs: <“hello”,  1> and <’world”’,  1>. We do this for the entire corpus,  so that\nif our corpus  is words big, we end up with key-value  pairs in the map step.\n3. Shuffle  step: Data is redistributed  based on the output keys from the prior step’s map function,\nsuch that tuples with the same key are located  on the same worker  node. In this case, it means\nthat all tuples of <\"hello”,  1> will be located on the same worker node, as will all tuples of\n<”world”,  1>, and so on.\n4. Reduce  step: Each worker  node processes  each key in parallel]  using a specified  reducer  operation\nto obtain the required  output result. In this case, we just sum up the tuple counts for each key, so\nif there are 5 tuples for <“hello”,  1> then the final output will be <\"hello”,  5>, meaning  that the\nword “hello”  occurred  5 times.\nBecause  the shuffle step moved all the “hello” key-value  pairs to the same node, the reducer can\noperate locally and, hence, efficiently.  The reducer  doesn’t need to communicate  with other nodes\nto ask for their “hello” key-value  pairs, which minimizes  the amount of precious node-to-node\nbandwidth  consumed.\nIn practice,  since MapReduce  is just the processing  technique,  people rely on Hadoop  to manage  the\nsteps of the MapReduce  algorithm.  Hadoop  involves:\n1) Hadoop  File System  (HDFS):  manages  data storage,  backup,  and replication\n2) MapReduce:  as discussed  above\n3) YARN: a resource  manager  which manages  job scheduling  and worker node orchestration\nSpark is another  popular  open-source  tool that provides  batch processing  similar to Hadoop,  with a\nfocus speed and reduced disk operations.  Unlike Hadoop, it uses RAM for computation,  enabling\nfaster inmemory  performance  but higher running  costs. Additionally,  unlike Hadoop,  Spark has built-\nin resource  scheduling  and monitoring,  whereas MapReduce  relies on external resource managers\nlike YARN.\nAce the Data Science  Interview 153"
  },
  {
    "page_number": 166,
    "content": "CHAPTER  8: SQL & DB DESIGN\nSQL & Database  Design  Questions\nEasy Problems\n8.1. Facebook:  Assume  you have the below events table on app analytics.  Write a query to get the\nclickthrough  rate per app in 2019.\nevents\nAa _ column_name  |= type\napp_id integer\nevent_id string (“impression’,  “click”)\ntimestamp datetime\n8.2. Robinhood:  Assume you are given the tables below containing  information  on trades and\nusers. Write a query to list the top three cities that had the most number  of completed  orders.\ntrades users\nAa _ column_name  |= type Aa _ column_name  |= _ type\norder_id integer user_id integer\nuser_id integer city string\nprice float email string\nquantity integer signup_date datetime\nstatus string (“complete”,  “cancelled”)\ntimestamp datetime\n8.3. New York Times: Assume that you are given the table below containing  information  on\nviewership  by device type (where the three types are laptop, tablet, and phone). Define\n“mobile”  as the sum of tablet and phone viewership  numbers.  Write a query to compare  the\nviewership  on laptops  versus mobile  devices.\nviewership\nAa column_name  |= type\nuser_id integer\ndevice type string\nview_time datetime\n8.4. Amazon:  Assume you are given the table below for spending  activity by product type.\nWrite a query to calculate  the cumulative  spend so far by date for each product  over time in\nchronological  order.\n154 Ace the Data Science  Interview | SQL & DB Design"
  },
  {
    "page_number": 167,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\ntotal_trans\nAa column_name  |= type\norder_id Integer\nuser  _ id integer\nproduct_id string\nspend float\ntrans_date datetime\n8.5. eBay: Assume  that you are given the table below containing  information  on various orders\nmade by customers.  Write a query to obtain  the names  of the ten customers  who have ordered\nthe highest  number  of products  among  those customers  who have spent at least $1000 total.\nuser_transactions\nAa_ column_name  |= type\ntransaction_id integer\nproduct_id integer\nuser_id integer\nspend float\ntrans_date datetime\n8.6. Twitter:  Assume  you are given the table below  containing  information  on tweets.  Write a query\nto obtain  a histogram  of tweets posted  per user in 2020.\ntweets\nAa_ column_name  |= _ type\ntweet_id integer\nuser_id integer\nmsg string\ntweet_date datetime\n8.7. Stitch Fix: Assume  you are given the table below containing  information  on user purchases.\nWrite a query to obtain the number  of people who purchased  at least one or more of the same\nproduct  on multiple  days.\npurchases\nAa column_name  |= type\npurchase_id integer\nuser_id integer\nproduct_id integer\nquantity integer\nprice float\npurchase_time datetime\nAce the Data Science  Interview"
  },
  {
    "page_number": 168,
    "content": "CHAPTER  8: SQL & DB DESIGN\n8.8. Linkedin:  Assume  you are given the table below that shows the job postings  for all companies\non the platform.  Write a query to get the total number  of companies  that have posted duplicate\njob listings (two jobs at the same company  with the same title and description).\njob_listings\nAa _ column_name  |= type\njob_id integer\ncompany_id integer\ntitle string\ndescription string\npost_date datetime\n8.9. Etsy: Assume  you are given the table below on user transactions.  Write a query to obtain the\nlist of customers  whose first transaction  was valued  at $50 or more.\nuser_transactions\nAa column_name  |= type\ntransaction_id integer\nproduct_id integer\nuser_id integer\nspend float\ntransaction  date datetime\n8.10. Twitter:  Assume  you are given the table below containing  information  on each user’s tweets\nover a period  of time. Calculate  the 7-day rolling  average  of tweets  by each user for every date.\ntweets\nAa _ column_name  |= type\ntweet_id integer\nmsg string\nuser_id integer\ntweet_date datetime\n8.11. Uber: Assume  you are given the table below on transactions  made by users. Write a query to\nobtain the third transaction  of every user.\ntransactions\nAa _ column_name  |= type\nuser_id integer\nspend float\ntransaction  date datetime\n156 Ace the Data Science  Interview  | SQL & DB Design"
  },
  {
    "page_number": 169,
    "content": "8.12.\n8.13.\n8.14.\n8.15.\n8.16.\n8.17.\n8.18.\n8.19.\n8.20.\n8.21.ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nAmazon:  Assume  you are given the table below containing  information  on customer  spend  on\nproducts  belonging  to various  categories.  Identify  the top three highest-grossing  items within\neach category  in 2020.\nproduct_spend\nAa _ column_name  |= type\ntransaction  _id integer\ncategory  _id integer\nproduct_id integer\nuser_id integer\nspend float\ntransaction  date datetime\nWalmart:  Assume  you are given the below table on transactions  from users. Bucketing  users\nbased on their latest transaction  date, write a query to obtain the number  of users who made a\npurchase  and the total number  of products  bought  for each transaction  date.\nuser_transactions\nAa column_name  |= type\ntransaction  id integer\nproduct_id integer\nuser_id integer\nspend float\ntransaction  date datetime\nFacebook:  What is a database  view? What are some advantages  views have over tables?\nExpedia:  Say you have a database  system where most of the queries made were UPDATEs/\nINSERTs/DELETEs.  How would this affect your decision to create indices? What if the\nqueries  made were mostly SELECTs  and JOINs instead?\nMicrosoft:  What is a primary  key? What characteristics  does a good primary  key have?\nAmazon:  Describe  some advantages  and disadvantages  of relational  databases  vs. NoSQL\ndatabases.\nCapital  One: Say you want to set up a MapReduce  job to implement  a shuffle operator,  whose\ninput is a dataset and whose output is a randomly  ordered version of that same dataset. At a\nhigh level, describe  the steps in the shuffle  operator’s  algorithm.\nAmazon:  Name one major similarity  and difference  between a WHERE  clause and a HAVING\nclause  in SQL.\nKPMG:  Describe  what a foreign key is and how it relates to a primary  key.\nMicrosoft:  Describe what a clustered index and a non-clustered  index are. Compare and\ncontrast  the two.\nAce the Data Science  Interview 157"
  },
  {
    "page_number": 170,
    "content": "CHAPTER  8: SQL & DB DESIGN\nMedium  Problems\n8.22. Twitter:  Assume  you are given the two tables below containing  information  on the topics that\neach Twitter user follows and the ranks of each of these topics. Write a query to obtain all\nexisting  users on 2021-01-01  that did not follow any topic in the 100 most popular  topics for\nthat day.\nuser_ topics topic_rankings\nAa column_name  |= type Aa column_name  |= type\nuser_id integer topic_id integer\ntopic_id integer ranking integer\nfollow_date datetime ranking_date datetime\n8.23. Facebook:  Assume  you have the tables below containing  information  on user actions.  Write a\nquery to obtain active user retention  by month.  An active user is defined  as someone  who took\nan action (sign-in,  like, or comment)  in the current  month.\nuser_actions\nAa _ column_name  |= type\nuser_id integer\nevent_id string (“sign-in’,  “like”, “comment)\ntimestamp datetime\n8.24. Twitter:  Assume  you are given the tables  below  containing  information  on user session  activity.\nWrite a query that ranks users according  to their total session  durations  for each session  type\nbetween  the start date (2021-01-01)  and the end date (2021-02-01).\nsessions\nAa_ column_name  |= type\nsession_id integer\nuser_id integer\nsession_type string\nduration integer\nstart_time datetime\nxtON. Snapchat:  Assume  you are given the tables below containing  information  on Snapchat  users\nand their time spent sending  and opening  snaps. Write a query to obtain a breakdown  of the\nlime spent sending  vs. opening  snaps (as a percentage  of total time spent) for each of the\ndifferent  age groups.\nactivities age_ breakdown\nAa_ column_name  |= type Aa column_name  |= type\nactivity _id integer user_id integer\n158 Ace the Data Science  Interview  | SQL & DB Design"
  },
  {
    "page_number": 171,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nuser_id integer | age bucket _| string\ntype String (‘send’,  ‘open’)\ntime_spent float\nactivity_date datetime\n8.26. Pinterest:  Assume  you are given the table below containing  information  on user sessions,\nincluding  their start and end times.  A session  is considered  to be concurrent  with another  user’s\nsession if they overlap.  Write a query to obtain the user session  that is concurrent  with the\nlargest  number  of other user sessions.\nsessions\nAa column_name  |= type\nsession_id integer\nstart_time datetime\nend_time datetime\n8.27. Yelp: Assume  you are given the table below containing  information  on user reviews.  Define  a\ntop-rated  business  as one whose reviews  contain  only 4 or 5 stars. Write a query to obtain  the\nnumber  and percentage  of businesses  that are top rated.\nreviews\nAa _ column_name  |= type\nbusiness  _id integer\nuser_id integer\nreview_text string\nreview_  stars integer\nreview_date datetime\n8.28. Google:  Assume  you are given the table below containing  measurement  values obtained  from\na sensor over several  days. Measurements  are taken several  times within a given day. Write a\nquery to obtain  the sum of the odd-numbered  measurements  and the sum of the even-numbered\nmeasurements  by date.\nmeasurements\nAa _ column_name = type\nmeasurement_id integer\nmeasurement  value | float\nmeasurement_time  datetime\n§.29, Etsy: Assume you are given the two tables below containing  information  on user signups\nand user purchases.  Of the users who joined within the past week, write a query to obtain the\npercentage  of users that also purchased  at least one item.\nAce the Data Science  Interview 159"
  },
  {
    "page_number": 172,
    "content": "CHAPTER  8: SQL & DB DESIGN\nsignups user_ purchases\nAa _ column_name  |= _ type Aa_ column_name  |= type\nuser_id integer user_id integer\nsignup_date datetime product_id integer\npurchase_amount  | float\npurchase_date datetime\n8.30. Walmart:  Assume  you are given the following  tables on customer  transactions  and products.\nFind the top 10 products  that are most frequently  bought together  (purchased  in the same\ntransaction).\ntransactions products\nAa column_name  |= type Aa _ column_name  |= type\ntransaction_id integer product_id integer\nproduct_id integer product_name string\nuser_id integer price float\nquantity integer\ntransaction_time  datetime\n8.31. Facebook:  Assume  you have the table given below containing  information  on user logins.\nWrite a query to obtain the number  of reactivated  users (1.e., those who didn’t log in the\nprevious  month,  who then logged  in during  the current  month).\nuser_logins\nAa _ column_name  |= type\nuser_id integer\nlogin _date datetime\n8.32. Wayfair:  Assume  you are given the table below containing  information  on user transactions\ntor particular  products.  Write a query to obtain the year-on-year  growth  rate for the total spend\nof each product,  for each week (assume  there is data each week).\nuser_transactions\nAa _ column_name  | = type\ntransaction_id integer\nproduct_id integer\nuser_id integer\nspend float\ntransaction  date datetime\n160 Ace the Data Science  Interview  | SQL & DB Design"
  },
  {
    "page_number": 173,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\n8.33. Stripe:  Assume  you are given the table below containing  information  on user transactions  for\na particular  business  using Stripe.  Write a query  to obtain  the account’s  rolling  7-day earnings.\nuser_transactions\nAa _ column_name  | = type\ntransaction_id integer\nuser_id integer\namount float\ntransaction_date  datetime\n8.34. Facebook:  Say you had the entire Facebook  social graph (users and their friendships).  How\nwould you use MapReduce  to find the number  of mutual friends for every pair of Facebook\nusers?\n8.35. Google:  Assume  you are tasked with designing  a large-scale  system that tracks a variety of\nsearch query strings and their frequencies.  How would you design this, and what trade-offs\nwould  you need to consider?\nSQL & Database  Design  Solutions\nNote: Due to the variety of SQL flavors,  don’t be alarmed  by minor variations  in syntax. We’ve\nwritten  the SQL snippets  in this book 1n PostgreSQL.\nSolution  #8.1\nTo get the click-through  rate, we use the following  query, which includes  a SUM along with a IF\nto obtain the total number  of clicks and impressions,  respectively.  Lastly, we filter the timestamp  to\nobtain the click-through  rate for just the year 2019.\nSELECT\napp id,\nSUM(IF  (event id = ‘click’, 1, 0)) / SUM(IF(event_id  = ‘impression’,  1, 0))\nAS ctr\nFROM\nevents\nWHERE\ntimestamp  >= ‘2019-01-01’\nAND timestamp  <= ‘2020-01-01’\nGROUP BY\n1\nSolution  #8.2\nTo find the cities with the top three highest number of completed  orders, we first write an inner\nquery to join the trades and user table based on the user_id column and then filter for complete\norders. Using COUNT  DISTINCT,  we obtain the number  of orders per city. With that result, we then\nperform  a simple GROUP  BY on city and order by the resulting  number  of orders, as shown below:\nAce the Data Science  Interview 161"
  },
  {
    "page_number": 174,
    "content": "CHAPTER  8: SQL & DB DESIGN\nSELECT\nu.city,\nCOUNT (DISTINCT  t.order_id)  AS num_orders\nFROM\ntrades t\nJOIN users u ON t.user  id = u.user id\nWHERE\nt.status  = ‘complete’\nGROUP BY\ncity\nORDER BY\nnum orders DESC\nLIMIT\n3\nSolution  #8.3\nTo compare  the viewership  on laptops versus mobile devices,  we first can use a IF statement  to\ndefine the device type according  to the specifications.  Since the tablet and phone categories  form the\n‘‘mobile”  device  type, we can Set laptop to be its own device  type (1.e., “laptop’’).  We can then simply\nSUM the counts for each device  type:\nSELECT\nSUM(IF  (device  type = ‘laptop’,  1, 0)) AS laptop  views,\nSUM(IF  (device  type IN (‘phone’,  ‘tablet’),  1, 0)) AS mobile  views\nFROM\nviewership\nSolution  #8.4\nSince we don’t care about the particular  order _id or user id, we can use a window  function  to\npartition  by product  and order by transaction  date. Spending  is then summed  over every date and\nproduct  as follows:\nSELECT\ntrans date,\nproduct  id,\nSUM(spend)  OVER (\nPARTITION  BY product  id\nORDER BY\ntrans date\n) AS cum spend\nFROM\ntotal trans\nORDER BY\n162 Ace the Data Science  Interview  | SQL & DB Design"
  },
  {
    "page_number": 175,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nproduct  id,\ntrans date ASC\nSolution  #8.5\nIn order to obtain a count of products  by user, we employ  COUNT  product_id  for each user: hence,\nthe GROUP  BY is performed  over user_id.  To filter on having  spent at least $1000, we use a HAVING\nSUM(spend)  > 1000 clause. Lastly,  we order user_ids  by product_id  count and take the top 10.\nSELECT\nuser id,\nCOUNT  (product_id)  AS num  products\nFROM\nuser transactions\nGROUP BY\nuser id\nHAVING\nSUM(spend)  > 1000\nORDER BY\nnum products  DESC\nLIMIT\n10\nSolution  #8.6\nFirst, we obtain the number  of tweets per user in 2020 by using a simple  COUNT  within an initial\nsubquery.  Then, we use that tweet column  as the bucket  within a new GROUP  BY and COUNT  as\nshown  below:\nSELECT\nnum tweets  AS tweet bucket,\nCOUNT  (*) AS num _ users\nFROM\n(\nSELECT\nuser id,\nCOUNT  (*) AS num_tweets\nFROM\ntweets\nWHERE\ntweet date BETWEEN  ‘2020-01-01’\nAND ‘2020-12-31’\nGROUP BY\nuser id\n) total tweets\nAce the Data Science  Interview 163"
  },
  {
    "page_number": 176,
    "content": "CHAPTER  8: SQL & DB DESIGN\nGROUP BY\nnum tweets\nORDER BY\nnum_tweets  ASC\nSolution  #8.7\nWe can’t simply perform a count since, by definition,  the purchases  must have been made on\ndifferent  days (and for the same products).  To address  this issue, we use the window  function  RANK\nwhile partitioning  by user id and product_id  and then order the result by purchase  time in order to\ndetermine  the purchase  number.  From this inner subquery,  we then obtain the count of user_ids  for\nwhich purchase  number  was 2 (note that we don’t need above 2 since any purchase  number  above 2\ndenotes  multiple  products).\nSELECT\nCOUNT  (DISTINCT  user id)\nFROM\n(\nSELECT\nuser id,\nRANK() OVER (\nPARTITION  BY user id,\nproduct  id\nORDER BY\nCAST (purchase  time AS DATE)\n) AS purchase  no\nFROM\npurchases\n) t\nWHERE\npurchase  no = 2\nSolution  #8.8\nTo find all companies  with duplicate  listings  based on title and description,  we can use a RANK()\nwindow  function  partitioning  on company_id,  job tle, and job description.  Then, we can filter\nfor companies  where the largest row number  based on those partition  fields is greater  than 1, which\nindicates  duplicated  jobs, and then take a count of the number  of companies:\nWITH job listing  ranks AS (\nSELECT\ncompany  id,\ntitle,\ndescription,\nROW NUMBER()  OVER (\n164 Ace the Data Science  Interview  | SQL & DB Design"
  },
  {
    "page_number": 177,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nPARTITION  BY company  id,\ntitle,\ndescription\nORDER BY\npost date\n) AS rank\nFROM\njob listings\n)\nSELECT\nCOUNT  (DISTINCT  company  id)\nFROM\n(\nSELECT\ncompany  id\nFROM\njob listing  ranks\nWHERE\nMAX (rank) > 1\nSolution  #8.9\nAlthough  we could use a self join on fransaction_date  = MIN(transaction_date)  for each user, we\ncould also use the ROW  NUMBER  window  function  to get the ordering  of customer  purchases.\nWe could then use that subquery  to filter on customers  whose  first purchase  (shown  1n row one) was\nvalued  at 50 dollars  or more. Note that this would  require  the subquery  to include  spend also:\nWITH purchase  num AS (\nSELECT\nuser id,\nspend,\nROW NUMBER()  OVER (\nPARTITION  BY user id\nORDER BY\ntransaction  date ASC\n) as rownum\nFROM\nuser  transactions  u\n)\nSELECT\nuser id\nFROM\npurchase  num\nWHERE\nrownum = 1\nAND spend >= 50.00\nAce the Data Science  Interview 165"
  },
  {
    "page_number": 178,
    "content": "CHAPTER  8: SQL & DB DESIGN\nSolution  #8.10\nFirst, we need to obtain the total number  of tweets made by each user on each day, which can be gotten\nin a CTE using GROUP  BY with user_id  and tweet_date,  while also applying a COUNT  DISTINCT\nto tweet_id. Then, we use a window function on the resulting subquery to take an AVG number of\ntweets over the six prior rows and the current row (thus giving us the 7-day rolling average),  while\nordering  by user_id  and tweet_date:\nWITH tweet counts AS (\nSELECT\nuser id,\nCAST (tweet date AS date) AS tweet  _date,\nCOUNT  (*) AS num_tweets\nFROM\ntweets\nGROUP BY\nuser id,\nCAST (tweet date AS date)\n)\nSELECT\nuser id,\ntweet date,\nAVG(num  tweets) OVER (\nPARTITION  BY user_id\nORDER BY\nuser id,\ntweet date ROWS BETWEEN 6 preceding\nAND CURRENT  ROW\n) AS rolling  avg_7d\nFROM\ntweet counts\nSolution  #8.11\nFirst, we obtain the transaction  numbers  for each user. We can do this by using the ROW_NUMBER\nwindow  function,  where we PARTITION  by the user_id  and ORDER  by the transaction_date  fields,\ncalling the resulting  field a transaction  number. From there, we can simply take all transactions\nhaving  a transaction  number  equal to 3.\nWITH nums AS (\nSELECT\nx\n’\nROW NUMBER()  OVER (\nPARTITION  BY user id\nORDER BY\ntransaction  date\n166 Ace the Data Science  Interview  | SQL & DB Design"
  },
  {
    "page_number": 179,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\n) AS trans_num\nFROM\ntransactions\n)\nSELECT\nuser id,\nspend,\ntransaction  date\nFROM\nnums\nWHERE\ntrans num = 3\nSolution  #8.12\nFirst, we calculate  a subquery  with total spend by product  and category  using SUM and GROUP  BY.\nNote that we must filter by a 2020 transaction  date. Then, using this subquery,  we utilize a window\nfunction  to calculate  the rankings  (by spend) for each product  category  using the RANK window\nfunction  over the existing  sums in the previous  subquery.  For the window  function,  we PARTITION\nby category  and ORDER  by product  spend. Finally,  we use this result and then filter for a rank less\nthan or equal to 3 as shown  below.\nWITH product  category  spend AS (\nSELECT\nproduct  id,\ncategory  id,\nSUM(spend)  AS total product_spend\nFROM\nproduct_spend\nWERERE\ntransaction  date BETWEEN ‘2020-01-01’\nAND ‘2020-12-31’\nGROUP BY\nproduct  id,\ncategory  id\n),\ntop spend AS (\nSELECT\np-.*,\nRANK() OVER (\nPARTITION  BY category  id\nORDER BY\ntotal product  spend DESC\n) AS rnk\nF ROS\nAce the Data Science  Interview 167"
  },
  {
    "page_number": 180,
    "content": "CHAPTER  8: SQL & DB DESIGN\nproduct  category  spend p\n)\nSELECT\n*\nFROM\ntop spend\nWHERE\nrnk <= 3\nORDER BY\ncategory  id,\nrnk DESC\nSolution  #8.13\nFirst, we obtain the latest transaction  date for each user. This can be done ina CTE using the RANK\nwindow function to get rankings  of products purchased  per user based on the purchase  transaction\ndate. Then, using this CTE, we simply COUNT  both the user ids and product  ids where the latest rank\nis | while grouping  by each transaction  date.\nWITH latest date AS (\nSELECT\ntransaction  date,\nuser id,\nproduct  id,\nRANK() OVER (\nPARTITION  BY user id\nORDER BY\nCAST(transaction  date AS DATE) DESC\n) AS days rank\nFROM\nuser  transactions\n)\nSELECT\ntransaction  date,\nCOUNT  (DISTINCT  user id) AS num_users,\nCOUNT  (product  id) AS total products\nFROM\nlatest  date\nWHERE\ndays rank = l\nGROUP BY\ntransaction  date\nORDER BY\ntransaction  date desc\n168 Ace the Data Science  Interview | SQL & DB Design"
  },
  {
    "page_number": 181,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nSolution  #8.14\nA database  view is the result of a particular  query within  a set of tables. Unlike  a normal  table, a view\ndoes not have a physical  schema.  Instead,  a view is computed  dynamically  whenever  it is requested.\nIf the underlying  tables that the views reference  are changed,  then the views will change  accordingly.\nViews  have several  advantages  over tables:\n|. Views  can simplify  workflows  by aggregating  multiple  tables,  thus abstracting  the complexity  of\nunderlying  data or operations.\n2. Since views can represent  only a subset  of the data, they provide  limited  exposure  of the table’s\nunderlying  data and hence increase  data security.\n3. Since views do not store actual data, there is significantly  less memory  overhead.\nSolution  #8.15\nSQL statements  that modify the database,  like UPDATE,  INSERT,  and DELETE,  need to change\nnot only the rows of the table but also the underlying  indexes.  Therefore,  the performance  of those\nstatements  depends  on the number  of indexes  that need to be updated.  The larger the number  of\nindexes,  the longer it takes those statements  to execute.  On the flip side, indexing  can dramatically\nspeed up row retrieval  since no underlying  indexes need to be modified.  This is important  for\nstatements  performing  full table scans, like SELECTs  and JOINs.\nTherefore,  for databases  used in online transaction  processing  (OLTP)  workloads,  where database\nupdates  and inserts are common,  indexes  generally  lead to slower  performance.  In situations  where\ndatabases  are used for online analytical  processing  (OLAP),  where database  modifications  are\ninfrequent  but searching  and joining  the data 1s common,  indexes  generally  lead to faster  performance.\nSolution  #8.16\nA primary  key uniquely  identifies  an entity. It can consist  of multiple  columns  (known  as a composite\nkey) and cannot  be NULL.\nCharacteristics  of a good primary  key are:\ne Stability:  a primary  key should  not change  over time.\n¢ Uniqueness:  having duplicate  (non-unique)  values for the primary  key defeats the purpose  of\nthe primary  key.\n¢ frreducibility:  no subset of columns  in a primary  key is itself a primary  key. Said another  way,\nremoving  any column  from a good primary  key means that the key’s uniqueness  property  would\nbe violated.\nSolution  #8.17\nAdvantages  of Relational  Databases:  Ensure data integrity  through  a defined  schema  and the ACID\nproperties.  Easy to get started with and use for small-scale  applications.  Lends itself well to vertical\nscaling. Uses an almost standard query language,  making leaming or switching  between different\ntypes of relational  databases  easy.\nAdvantages  of NoSQL  Databases:  Offers more flexibility  in data format and representations,  which\nmakes working  with unstructured  or semistructured  data easier. Hence, useful when still iterating  on\nthe data schema  or adding new features/functionality  rapidly like in a startup  environment.  Convenient\nto scale with horizontal  scaling. Lends itself better to applications  that need to be highly available.\nAce the Data Science  Interview 169"
  },
  {
    "page_number": 182,
    "content": "CHAPTER  8: SQL & DB DESIGN\nDisadvantages  of Relational Databases:  Data schema needs to be known in advance. Altering\nschemas is possible, but frequent changes to the schema for large tables can cause performance\nissues. Horizontal  scaling is relatively  difficult, leading to eventual  performance  bottlenecks.\nDisadvantages  of NoSQL Databases:  As outlined  by the BASE framework,  weaker guarantees  on\ndata correctness  are made due to the soft-state and eventual consistency  property. Managing  data\nconsistency  can also be difficult due to the lack of a predefined  schema that’s strictly adhered to.\nDepending  on the type of NoSQL database, it can be challenging  for the database to handle some\ntypes of complex  queries  or access patterns.\nSolution  #8.18\nAta high level, to shuffle the data randomly,  we need to map each row of the input data to a random\nkey. This ensures that the row of input data is randomly  sent to a reducer, where it’s simply outputted.\nMore concretely,  the steps of the MapReduce  algorithm  are:\n1. Map step: Each row is assigned  a random value from 1.,...,.k, where k is the number of reducer\nnodes available.  Therefore,  for every key, the output is the tuple (key, row).\n2. Shuffle step: Rows with the same input key go to the same reducer.\n3. Reduce  step: For each record, the row is simply outputted.\nSince the reducer only has rows that were filtered randomly  for a given value of i, where  i is from\n1,...,4, the resulting  output will be ordered  randomly.\nSolution  #8.19\nA couple of answers  are possible,  but here are some examples:\nSimilarities:\n1. Both clauses  are used to limit/filter  a given query’s  results.\n2. Both clauses  are optional  within a query.\n3. Usually,  queries  utilizing  one of the two can be transformed  to use the other.\nDifferences:\n1. AHAVING  clause can follow a GROUP  BY statement,  but WHERE  cannot.\n2. A WHERE  clause evaluates  per row, whereas a HAVING  clause evaluates  per group.\n3, Aggregate  functions  can be referred  to in a logical expression  if  a HAVING  clause is used.\nSolution  #8.20\nForeign  keys are a set of attributes  that aid in joining  tables by referencing  primary  keys (although\njoins can occur without  them). Primarily,  they exist to ensure  data integrity.  The table with the primary\nkey is called the parent table, whereas  the table with the foreign key is called the child table. Since\nforeign keys create a link between  the two tables, having foreign keys ensures  that these links are\nvalid and prevents  data from being inserted  that would otherwise  violate these conditions.  Foreign\nkeys can be created  during  CREATE  commands,  and it is possible  to DROP  or ALTER  foreign  keys.\nWhen designating  foreign keys, it is important  to think about the cardinality  - - the relationship\nbetween  parent and child tables. Cardinality  can take on four forms: one-to-one  (one row in the\nparent table maps to one row in the child table), one-to-many  (one row in the parent table maps to\nmany rows in the child table), many-to-one  (many rows in the parent table map to one row in the\n170 Ace the Data Science  Interview | SQL & DB Design"
  },
  {
    "page_number": 183,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nchild table), and many-to-many  (many rows in the parent  table map to many rows in the child table).\nThe particular  type of relationship  between  the parent  and child table determines  the specific  syntax\nused when setting  up foreign  keys.\nSolution  #8.21\nBoth clustered  indexes  and non-clustered  indexes  help speed up queries  in a database.  With a clustered\nindex, database  rows are stored physically  on the disk in the same exact order as the index. This\narrangement  allows you to rapidly  retrieve  all rows that fall into a range of clustered  index values.\nHowever,  there can only be one clustered  index per table since data can only be sorted physically  on\nthe disk in one particular  way at a time.\nIn contrast,  a non-clustered  index does not match the physical  layout of the rows on the disk on\nwhich the data are stored. Instead, it duplicates  data from the indexed  column(s)  and contains  a\npointer  to the rest of data. A non-clustered  index is stored separately  from the table data, and hence,\nunlike a clustered  index, multiple  non-clustered  indexes  can exist per table. Therefore,  insert and\nupdate operations  on a non-clustered  index are faster since data on the disk doesn’t  need to match\nthe physical  layout as in the case of a clustered  index. However,  this makes the storage  requirement\nfor a non-clustered  index higher than for a clustered  index. Additionally,  lookup operations  for a\nnon-clustered  index may be slower  than that of a clustered  index since all queries  must go through\nan additional  layer of indirection.\nSolution  #8.22\nFirst, we need to obtain the top 100 most popular  topics for the given date by employing  a simple\nsubquery.  Then, we need to identify  all users who followed  no topic included  within these top 100 for\nthe date specified.  Equivalently,  we could identify  those that did follow one of these topics and then\nfilter them out of this list of users that existed  on 2021-01-01.\nTwo approaches  are as follows:\n1. use the MINUS  (or EXCEPT)  operator  and subtract  those following  a top 100 topic (via an inner\njoin) from the entire user universe\n2. use a WHERE  NOT EXISTS  clause in a similar  fashion.\nFor simplicity,  the solution  below uses the MINUS  operator.  Note that we need to filter for date in the\nuser_topics  table so that we capture  only existing  users as of 2020-01-01:\nWITH top_topics  AS (\nSELECT\n*\nFROM\ntopic rankings\nWHERE\nranking  date = ‘2021-01-01’\nAND rank <= 100\n)\nSELECT\nDISTINCT  user _id\nFROM\nAce the Data Science  Interview 171"
  },
  {
    "page_number": 184,
    "content": "CHAPTER  8: SQL & DB DESIGN\nuser topics\nWHERE\nfollow date <= ‘2021-01-01’\nMINUS\nSELECT\nu.user_  id\nFROM\nuser topics u\nJOIN top topics t ON u.topic_id  = t.topic_id\nSolution  #8.23\nIn order to calculate  user retention,  we need to check for each user whether  they were active this\nmonth versus last month. To bucket days into each month, we need to obtain the first day of the\nmonth for the specified  date by using DATE _TRUNC.  We use a COUNT  DISTINCT  over user_id\nto obtain the monthly  active user (MAU)  count for the month.  This can be put into a subquery  called\ncurr_month,  and then EXISTS  can be used to check it against another  subquery  for the previous\nmonth, /ast_ month. In that subquery, ADD  MONTHS  can be used with an argument  of | to get the\nprevious  month, thereby  allowing  us to check for user actions from the previous  month (since that\nwould  mean they were logged  in), as shown  below:\nSELECT\nDATE  TRUNC(‘month’,  curr  _month.timestamp)  AS month,\nCOUNT (DISTINCT  curr  month.user  id) AS mau\nFROM\nuser actions  curr month\nWHERE\nEXISTS ({\nSELECT\nFROM\nuser actions  last month\nWHERE\nadd _ months  (DATE  TRUNC(‘month’,  last  _month.timestamp),  -1) =\nDATE TRUNC(  ‘month’,  curr  month.timestamp)\n)\nGROUP BY\nDATE TRUNC  (‘month’,  curr  month.timestamp)\nORDER BY\nmonth ASC;\nSolution  #8.24\nFirst, we can perform  a CTE to obtain the total session  duration  by user and session  type between  the\nstart and end dates. Then, we can use RANK to obtain the rank, making  sure to partition  by session\ntype and then order by duration  as in the query below:\n172 Ace the Data Science  Interview | SQL & DB Design"
  },
  {
    "page_number": 185,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nWITH user duration  AS (\nSELECT\nuser id,\nsession  type,\nSUM(duration)  AS duration\nFROM\nsessions\nWHERE\nstart_time  BETWEEN  ‘2021-01-01’\nAND ‘2021-02-01’\nGROUP BY\nuser id,\nsession  type\n)\nSELECT\nuser id,\nsession  type,\nRANK() OVER (\npartition  by user id\nsession  type\nORDER BY\nuser id\nduration  DESC\n) AS rank\nFROM\nuser duration\nORDER BY\nsession  type,\nrank DESC\nSolution  #8.25\nWe can obtain the total time spent on sending  and opening  using conditional  IF statements  for each\nactivity  type while getting  the amount  of time spent ina CTE. We can also obtain the total time spent\nin the same CTE. Next, we take that result and JOIN by the corresponding  user_id  with activities.\nWe filter for just send and open activity  types and group by age bucket.  Then, using this CTE, we can\ncalculate  the percentages  of send and open time spent versus overall time spent as follows:\nWITH time stats AS (\nSELECT\nage breakdown.age_  bucket,\nSUM(IF  (type = ‘send’, time spent, 0)) AS send _timespent,\nSUM(IF  (type = ‘open’, time spent, 0)) AS open _timespent,\nSUM(time  spent) AS total timespent\nFROM\nAce the Data Science  Interview 173"
  },
  {
    "page_number": 186,
    "content": "CHAPTER  8: SQL & DB DESIGN\nage breakdown oo\nJOIN activities  on age  breakdown.user_id  = activities.user_id\nWHERE\nactivities.type  IN (‘send’, ‘open’  )\nGROUP BY\nage  breakdown.age  bucket\n)\nSELECT\nage bucket,\nsend timespent  / total timespent  AS pct_send,\nopen timespent  / total timespent  AS pct_open\nFROM\ntime stats\nSolution  #8.26\nThe first step is to determine  the query logic for when two sessions  are concurrent.  Say we have two\nsessions,  session | and session  2. Note that there are two cases in which they overlap:\n1. Ifsession  | starts first, then the start time for session  2 1s less than or equal to session  1’s end time\n2. If session  2 starts first, then session 1’s end time for session | is greater  than or equal to session\n2’s start time\nIn total, this simplifies  to session  2’s start time falling  between  session 1’s start time and session 1’s\nend time.\nWith this in mind, we can calculate  the number  of sessions  that started  during  the time another  session\nwas running  by using an inner join and using BETWEEN  to check the concurrency  case as follows:\nSELECT\nsl.session  la,\nCOUNT (s2.session_id)  AS concurrents\nFROM\nsessions  s]l\nJOIN sessions  s2 ON sl.session  id != s2.session  id\nAND s2.start  time BETWEEN  sl.start  time\nAND sl.end  time\nGROUP BY\nsl.session  ia\nORDER BY\nconcurrents  DESC\nLIMIT\n1\nSolution  #8.27\nFirst. we need to idenufy  businesses  having  reviews  consisting  of  only 4 or 5 stars. We can do so by\nusing a CTE to find the lowest number  of stars given to a business  across all its reviews.  Then, we\n174 Ace the Data Science  Interview  | SQL & DB Design"
  },
  {
    "page_number": 187,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\ncan use a SUM and IF statement  to filter across  businesses  with a minimum  review  of 4 or 5 stars to\nget the total number  of top-rated  businesses,  and then divide  this by the total number  of businesses  to\nfind the percentage  of top-rated  businesses.\nWITH min_review  AS (\nSELECT\nbusiness  id,\nmin(review_stars)  AS min_stars\nFROM\nreviews\nGROUP BY\nbusiness  id\n)\nSELECT\n(1.0 * SUM(IF(min_stars  >= 4, 1, 0)) / COUNT(*))  * 100.0 AS top places  pct\nFROM\nmin review\nSolution  #8.28\nFirst, we need to establish  which measurements  are odd numbered  and which  are even numbered.  We\ncan do so by using the ROW_NUMBER  window  function  over the measurement  time 1o obtain the\nmeasurement  number  during  a day. Then, we filter for odd numbers  by checking  if a measurement’s\nmod 2 is | for odds or is O for evens. Finally, we sum by date using a conditional  IF statement,\nsumming  over the corresponding  measurement_value:\nWITH measurements  by count AS (\nSELECT\nCAST (measurement  time AS date) measurement  day,\nmeasurement  value,\nROW NUMBER()  OVER (\nPARTITION  BY CAST (measurement  time AS date)\nORDER BY\nmeasurement  time ASC\n) AS measurement  _count\nFROM\nmeasurements\n)\nSELECT\nmeasurement  day,\nSUM (\nIF (measurement  count % 2 != 0, measurement  value, 0)\n) AS odd sum,\nSUM (\nIF (measurement  count % 2 = 0, measurement  value, Q)\n) AS even sum\nAce the Data Science  Interview 175"
  },
  {
    "page_number": 188,
    "content": "CHAPTER  8: SQL & DB DESIGN\nFROM\nmeasurements  by count\nGROUP BY\nmeasurement  day\nORDER BY\nmeasurement  day ASC\nSolution  #8.29\nFirst, we obtain the latest week’s users. To do this, we use NOW for the current time and subtract\nan INTERVAL  of 7 days, thus providing  the relevant  user IDs to look at. By using LEFT JOIN, we\nhave all signed-in  users, and whether they made a purchase  or not. Now we take the COUNT  of\nDISTINCT  users from the purchase  table, divide it by the COUNT  of DISTINCT  users trom the\nsignup  table, and then multiply  the results by 100 to obtain a percentage:\nSELECT\nCOUNT (DISTINCT  p.user id) / COUNT(DISTINCT  s.user_id)  * 100 AS\nlast week pct\nFROM\nSignups  s\nLEFT JOIN user purchases  p ON p.user_  id = s.user_id\nWHERE\ns.signup  date > NOW() - INTERVAL  7 DAY\nSolution  #8.30\nFirst, we can join the transactions  and product  tables together  based on product_id  to get the user_id,\nproduct_  name, and transaction  _time for the transactions.  With the CTE at hand, we can do a self  join\nto fetch products  thal were purchased  together  by a single user by joining  on transaction_id.  Note\nthat we want all pairs of products,  but we don’t want to overcount,  1.e., if user A purchased  products\nX and Y in the same transaction,  then we only want to count the (X, Y) transaction  once, and not also\n(Y, X). To handle this, we can use a condition  within the inner join that the product_id  of A is less\nthan that of B (where  A and B are the CTE results from before).  Lastly,  we use a GROUP  BY clause\nfor each pair of products  and sort by the resulting  count, taking the top 10:\nWITH (\nSELECT\nt.user  id,\np.product  name,\nt.transaction  id\nFROM\ntransactions  t\nJOIN product  p ON t.product_id  = p.product_id\n) AS purchase  info\n176 Ace the Data Science  Interview | SQL & DB Design"
  },
  {
    "page_number": 189,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nSELECT\npl.product  name AS productl,\np2.product_name  AS product2,\nCOUNT  (*) AS count\nFROM\npurchase  info pl\nJOIN purchase  info p2 ON pl.transaction_  id = p2.transaction  id\nAND pl.product  id < p2.product_id , |\nGROUP BY ,\n1,\n2\nORDER BY\n3 DESC\nLIMIT\nLO\nSolution  #8.31\nFirst, we look at all users who did not log in during the previous  month. To obtain the last month’s\ndata, we subtract  an INTERVAL  of 1 month from the current month’s  login date. Then, we use a\nWHERE  EXISTS  against  the previous  month’s  interval  to check whether  there was a login in the\nprevious  month.  Finally,  we COUNT  the number  of users satisfying  this condition.\nSELECT\nDATE_TRUNC  (*month’,  current_month.login_  date) AS current  month,\nCOUNT(*)  AS num_reactivated_users\nFROM\nuser_logins  current  month\nWHERE\nNOT EXISTS (\nSELECT\n*\nFROM\nuser logins last month\nWHERE\nDATE TRUNC(‘month’,  last month.login  date) BETWEEN DATE  _\nTRUNC(‘month’,  current  _month.login  date) AND DATE  TRUNC(‘month’,\ncurrent  month.login  date) - INTERVAL  ‘i month’\n)\nSolution  #8.32\nFirst, we need to obtain the total weekly  spend by product  using SUM and GROUP  BY operations\nand use DATE TRUNC on the transaction  date to specify  a particular  week. Using this information,\nwe then calculate  the pnor year’s weekly spend for each product.  In particular,  we want to take a\nLAG for 52 weeks, and PARTITION  BY product,  to calculate  that week’s prior year spend for the\nAce the Data Science  Interview 177"
  },
  {
    "page_number": 190,
    "content": "CHAPTER  8: SQL & DB DESIGN\ngiven product. Lastly, we divide the current total spend by the corresponding  previous 52-week lag\nvalue:\nWITH weekly spend AS (\nSELECT\nDATE  TRUNC(‘*week’,  transaction_date  :: DATE) AS week,\nproduct  id,\nSUM(spend)  AS total_spend\nFROM\nuser  transactions\nGROUP BY\nweek,\nproduct_id\n),\ntotal weekly  spend AS (\nSELECT\nw.*,\nLAG (total spend, 52) OVER (\nPARTITION  BY product  id\nORDER BY\nweek ASC\n) as prev_total  spend\nIF'ROM\nweekly spend w\n)\nSELECT\nproauct  1a,\ntotal spend,\nprev_ total spend,\ntotal spend / prev total spend AS spend yoy\nFROM\ntotal weekly  spend\nSolution  #8.33\nFirst, we need to obtain the total daily transactions  using a simple SUM and GROUP  BY operation.\nHaving  the daily transactions,  we then perform  a self join on the table using the condition  that the\ntransaction  date for one transaction  occurs within 7 days of the other, which we can check by using\nthe DATE  ADD function  along with the condition  that the earlier  date doesn’t  precede  the later date:\nWITH daily transactions  AS (\nSELECT\nCAST (transaction  date AS DATE),\nSUM(amount)  AS total amount\nFROM\nuser  transactions\n178 Ace the Data Science  Interview  | SQL & DB Design"
  },
  {
    "page_number": 191,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nGROUP BY |\ntransaction  date\nSELECT\nt2. transaction _date,\nSUM (tl. amount  ) AS weekly  rolling  total\nFROM\ndaily  transactions  tl\nINNER JOIN daily  transactions  t2 ON ti. transaction date > DATE  ADD(*‘DAY’,\n--7, t2.transaction  _date) AND tl. transaction  date <= t2. transaction_  date\nGROUP BY\nt2.transaction  date\nORDER BY\nt2.transaction  date ASC\nSolution  #8.34\nTo use MapReduce  to find the number  of mutual  friends  for all pairs of Facebook  users, we can think\nabout what the end output needs to be and then work backward.  Concretely,  for all given pairs of\nusers X and Y, we want to identify  which friends  they have in common,  from which we’! derive the\nmutual  friend count. The core of this algorithm  1s finding  the intersection  between  the friends  list for\nX and the friends list for Y. This operation  can be delegated  to the reducer.  Therefore,  it is sensible\nthat the key for our reduce step should be the tuple (X, Y) and that the value to be reduced  is the\ncombination  of the friends  list of X and the friends  list of Y. Thus, in the map step, we want to output\nthe tuple (X, Z) for each friend Z that X has.\nAs an example,  assume  that X is friends  with [W, Y, Z] and Y is friends  with [X, Z].\n1. Map step: For X, we want to output the following  tuples: 1) ((X, W), [W, Y, Z]), 2) ((X, Y), [W,\nY, Z]), and 3) ((X, Z), [W, Y, Z]). For Y we want to output the following  tuples: 1) ((X, Y), [X,\nZ|), and 2) ((Y, Z), [X, Z]). Note that the key is sorted, so that (Y, X) — (X, Y).\n2. Shuffle  step: Each machine  is delegated  data based on the keys from the map step, t.e., each tuple\n(X, Y). So, in the previous  example,  note that the map step outputs  the key (X, Y) for both X and\nY, and therefore  both of the keys are on the same machine.  That machine  will therefore  have the\ntuple (X, Y) as the key, and will store [W, Y, Z] and [X, Z] to be used in the reduce step.\n3. Reduce  step: We group by keys and take the intersection  of the resulting  lists. For the example  of\n(X, Y) — [W, Y, Z], [X, Z], we take the intersection  of [W, Y, Z] and [X, Z], which is [Z]. Thus,\nwe return the length of the set (1) for the input (X, Y).\nTherefore,  we are able to identify  Z as the common  friend of X and Y, and can return | as the number\nof mutual friends.  The process  outlined  above is repeated  in parallel for every pair of Facebook  users\nin order to find the final mutual friend counts between  each pair of users.\nSolution  #8.35\nTo design a system that tracks search query strings and their frequencies,  we can start with a basic\nkeyvalue  store. For each search query string, we store the corresponding  frequency  in a database\ntable containing  only those two fields. To build the system at scale, we have two options: vertical\nscaling or horizontal  scaling. For vertical scaling, we would add more CPU and RAM to existing\nAce the Data Science  Interview 179"
  },
  {
    "page_number": 192,
    "content": "CHAPTER  8: SQL & DB DESIGN\nmachines,  an action that is not likely to work well at Google’s  scale. Instead, we should consider\nhorizontal  scaling, in which more machines  (nodes) are added to a cluster. We would then store\nsearch query strings across a large set of nodes and be able to quickly find which node contains  a\ngiven search  query string.\nFor the actual sharding logic, consisting  of mapping  query strings to particular  shards, several\napproaches  are possible.  One way is to use a range of values; for example,  we could have 26 shards\nand map query strings beginning  with A to shard 1, B to shard 2, and so on. While this approach  is\nsimple to implement,  its primary  drawback  is that the data obtained  could be unevenly  distributed,\nmeaning  that certain shards would need to deal with much more data than others. For example,  the\nshard containing  strings  starting  with the letter ‘x’ will have much less load than the shard containing\nstrings starting  with the letter ‘a.’\nAn alternative  sharding  scheme could be to use a hash function  that maps the query string to a\nparticular  shard number.  This is another  simple solution  and would help reduce the problem  of all\ndata being mapped  to the same shard. However,  adding new nodes 1s troublesome  since the hash\nfunction  must be re-run across all nodes and the data rebalanced.  However,  those problems  can be\naddressed  through  a method  called “consistent  hashing,”  which aids in data rebalancing  when new\nservers  are added.\n180 Ace the Data Science  Interview  | SQL & DB Design"
  },
  {
    "page_number": 193,
    "content": "Coding\nCHAPTER  9\nEvery  Superman  has his kryptonite,  but as a Data Scientist,  coding  cant be yours. Between\ndata munging,  pulling  in data  from APIs, and setting  up data processing  pipelines,  writing\ncode is a near-universal  part of a Data Scientist’  job. This is especially  true at smaller\ncompanies,  where data scientists  tend to wear multiple hats and are responsible  for\nproductionizing  their analyses  and models. Even if you are the rare Data Scientist  that\nnever has to write production  code, consider  the collaborative  nature  of the  field — having\nstrong computer  science  fundamentals  will give you a leg up when working  with software\nand data engineers.\nTo test your programming  foundation,  Data Science  interviews  often take you ona stroll\ndown memory lane back to your Data Structures  and Algorithms  class (you did take\none, right?). These coding questions  test your ability to manipulate  data structures  like\nlists, trees, and graphs, along with your ability to implement  algorithmic  concepts  such\nas recursion  and dynamic  programming.  You're also expected  to assess vour solution’\nruntime  and space efficiency  using Big O notation.\nApproaching  Coding  Questions\nCoding interviews  typically last 30 to 45 minutes and come in a variety of formats. Early in the\ninterview  process, coding interviews  are often conducted  via remote coding assessment  tools like\nHackerRank,  Codility,  or CoderPad.  During final-round  onsite interviews,  it’s typical to write code\non a whiteboard.  Regardless  of the format, the approach  outlined below to solve coding interview\nproblems  applies.\nAce the Data Science  Interview 181"
  },
  {
    "page_number": 194,
    "content": "CHAPTER  9: CODING\nAfter receiving  the problem:  Don’t jump right into coding. It’s crucial first to make sure you are\nsolving the correct problem. Due to language  barriers, misplaced  assumptions,  and subtle nuances\nthat are easy to miss, misunderstanding  the problem  is a frequent  occurrence.  To prevent this, make\nsure to repeat the question  back to the interviewer  so that the two of you are on the same page. Clanfy\nany assumptions  made, like the input format and range, and be sure to ask if the input can be assumed\nto be non-null  or well formed.  As a final test to see if you’ve understood  the problem,  work through\nan example  input and see if you get the expected  output. Only after you’ve done these steps are you\nready to begin solving  the problem.\nWhen brainstorming  a solution:  First, explain at a high level how you could tackle the question.\nThis usually means discussing  the brute-force  solution.  Then, try to gain an intuition  for why this\nbrute-force  solution  might be inefficient,  and how you could improve  upon it. If you’re able to land\non a more optimal approach,  articulate  how and why this new solution  is better than the first brute-\nforce solution  provided.  Only after you've settled on a solution  is it time to begin coding.\nWhen coding  the solution:  Explain  what you are coding. Don’t just sit there typing away, leaving\nyour interviewer  in the dark. Because  coding interviews  often let you pick the language  you write\ncode in, you’re expected  to be proficient  in the programming  language  you chose. As such, avoid\npseudocode  in favor of proper compilable  code. While there is time pressure,  don’t take many\nshortcuts  when coding.  Use clear variable  names and follow good code organization  principles.  Write\nwell-styled  code — for example,  following  PEP 8 guidelines  when coding in Python.  While you are\nallowed  to cut some corners,  like assuming  a helper method  exists, be explicit  about it and offer to\nfix this later on.\nAfter  you’re  done coding:  Make sure there are no mistakes  or edge cases you didn’t handle.  Then\nwrite and execute  test cases to prove you solved  the problem.\nAt this point, the interviewer  should dictate which direction  the interview  heads. They may ask\nabout the time and space complexity  of your code. Sometimes  they may ask you to refactor  and\nclean the code, especially  1f you cut some corners  while coding the solution.  They may also extend\nthe problem,  often with a new constraint.  For example,  they may ask you not to use recursion  and\ninstead tell you to solve the problem  recursively.  Or, they might ask you to not use surplus  memory\nand instead  solve the problem  in place. Sometimes,  they may pose a tougher  variant  of the problem\nas a follow-up,  which might require  starting  the problem-solving  process  all over again.\nSpace  & Time Complexity\nDetermining  the runtime  and space usage (how much memory  is utilized)  of an algorithm  is essential\nfor coding interviews  and real-world  data science.  Because  compute  and storage  resources  can be\nbottlenecks  to machine  learning  model  training  and deployment,  analyzing  an algorithm’s  performance\ncan affect what techniques  you choose to implement.  Consider  OpenAl’s  GPT-3 language  model,\nwhich contains  over 175 billion  parameters  and took $12 million  in compute  resources  to train. Much\nof the work bringing  GPT-3 to the world involved  optimizing  resource  usage to efficiently  train such\na large model.\nComputer  scientists  analyze  and classify  the behavior  of an algorithm’s  time and space usage via\nasymptotic  complexity  analysis.  This technique  considers  how an algorithm  performs  when the input\nsize goes toward infinity  and characterizes  the behavior  of the runtime  and space used as a function\nof #7. In academic  settings,  we establish  tight bounds  on performance  in terms of 7 using Big 0 (big\ntheta) notation,  However.  in industry,  the technical  definitions  have been muddled,  and we tend to\ndenote  these tight bounds  on performance  using Big O notation.\n182 Ace the Data Science  Interview  | Coding"
  },
  {
    "page_number": 195,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nIn the context  of companies  asking interview  questions,  we care about not just establishing  tight\nbounds  on performance  but thinking  about the worst-case  scenario  for this performance.  As such,\nBig O notation  often describes  the “worst-case  upper  bound,”  or the longest  an algorithm  would run\nor the maximal  amount  of space it would  need in the worst case.\nFor instance,  consider  an array of size N. Here are the following  classes of runtime  complexities,\nfrom fastest  to slowest,  using Big O notation:\n© O(1): Constant  time. Example:  getting  a value at a particular  index from an array\n* O(log  NV): — Logarithmic  time. Example:  binary  search  on a sorted array\n° OW): Linear  time. Example:  using a for-loop  to traverse  through  an array\n* O(N log NY): Log-linear  time. Example:  running  mergesort  on an array\n© O(WN2): Quadratic  time. Example:  iterating  over every pair of elements  in an alTay using a\ndouble  for-loop\n© O(24N): Exponential  time. Example:  recursively  generating  all binary numbers  that are N\ndigits long\n© O(N): Factorial  time. Example:  generating  all permutations  of an array\nO(N!) O(2\") O(N?)\nO(N log N)\nO(N)\nTime to complete (in operations)\nO(log N)\nO(1)\nSize of input data\nThe same Big-O runtime  analysis  concepts  apply analogously  to space complexity.  For example,  if\nwe need to store a copy of an input array with N elements,  that would be an additional  O(N) space.\nIf we wanted  to store an adjacency  matrix among  N nodes, we would need O(N“2)  space to keep the\nN-by-N  sized matrix.\nFor a basic example  for both runtime  and space complexity  analysis,  we can look at binary search,\nwhere we are searching  for a particular  value within a sorted array. The code that implements  this\nalgorithm  is below (with an extra set of conditions  that returns the closest if the exact value is not\nfound):\nAce the Data Science  Interview 183"
  },
  {
    "page_number": 196,
    "content": "CHAPTER  9 : CODING\ndef binary search(a,  k):\nlo, hi = 0, len(a) ~ 1\nbest = lo\nwhile lo <= hi:\nmid = lo + (hi - lo) // 2\nif a({mid} < k:\nlo = mid +1\nelif a{mid] > k:\nhi = mid - ]\nelse:\nbest = mid\nbreak\nif abs({a{mid]  - k) < abs(a[best]  - k):\nbest = mid\nreturn best\nIf we start the binary search with an input of N elements,  then at the next iteration,  we would only\nneed to search through  N/2 elements,  and so on. The runtime  complexity  for binary search 1s O(log N)\nsince at each iteration  we cut the remaining  search space in half. The space complexity  would simply\nbe O(N) since the array is size N, and we do not need auxiliary  space.\nComplexity  Analysis  Applied  to ML Algorithms\nFor an example  of complexity  analysis  for a machine  learning  technique,  consider  the Naive Bayes\nclassifier.  Recall that the algorithm  aims to calculate\nB|A)P(A)\nP(B)\nfor each of the 77 training  data points (ofdimension  @) for each of the classes,  and that P(B{A)  is the\nlikelihood  probability,  and P(A) is the prior probability.  In sumpler  terms, Naive Bayes is counting\nhow many times each of the d features  co-occurs  within  each class.P(A|B) =~!\nNow, consider  the training  runtime.  For all 7 training  points, Naive Bayes will look at the posterior\nand prior probabilities  over all d features,  for all & classes. This will take O(nkd) total runtime\nsince the operations  boil down to a series of counts.  The space complexity  is just O(kd) to store the\nprobabilities  needed  to compute  results for new data points.\nAs another  example,  consider  logistic  regression.  Recall that we need to calculate  the following:\nS(x) = a\nfor any given x. There are 7 training  data points (each with dimension  d); hence, 8 is a d-by-! vector\nof weights.  Recall that the goal of  logistic  regression  ts to find the optimal  decision  boundary  to split\nthe data into two classes. This involves  multiplying  each of the # training  points with 6, which is\nd-by-1 vector, so the training  runtime  complexity  is O(d). The space complexity  is just O(d) to store\nthe weights  (B) to classify  new data points.\n184 Ace the Data Science  Interview | Coding"
  },
  {
    "page_number": 197,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nData  Structures\nBelow is a brief  overview  of the most common  data structures  used for coding  interviews.  The best\nway to become  familiar  with each data structure  is by implementing  a basic version  of it in your\nfavorite  language.  Knowing  the Big-O for common  operations,  like inserting  an element  or finding\nan element  within  the structure,  is also essential.  The table below can be used for reference:\nData Time Complexity Space\nStructure Complexity\nAverage Worst Worst\nAccess Search | Insertion |} Deletion | Access Search j Insertion  | Deletion\nArray 0(1) O(n) O(n) O(n) 0(1) O(n) O(n) O(n) O(n)\nStack O(n) O(n) 0(1) 0(1) O(n) O(n) 0(1) 0(1) O(n)\nQueue O(n) O(n) 0(1) 0(1) O(n) O(n) 0(1) 0(1) O(n)\nLinked List O(n) O(n) 0(1) 0(1) O(n) O(n) 0(1) 0(1) O(n)\nHash Map N/A 0(1) 0(1) 0(1) N/A O(n) O(n) O(n) O(n)\nBinary  Search | O(log (n)) | O(log (n)) | O(log (n)) | O(log (n)) O(n) O(n) O(n) O(n) O(n)\nree\nArrays\nAn array is a series of consecutive  elements  stored sequentially  in memory.  Arrays are optimal for\naccessing  elements  at particular  indices, with an O(1) access and index time. However,  they are\nslower for searching  and deleting  a specific  value, with an O(N) runtime,  unless sorted. An array’s\nsimplicity  makes it one of the most commonly  used data structures  during coding interviews.\nCommon  array interview  questions  include:\n¢ Moving  all the negative  elements  to one side of an array\n¢ Merging  two sorted arrays\n* Finding specific sub-sequences  of integers within the array, such as the longest consecutive\nsubsequence  or the consecutive  subsequence  with the largest sum\nA frequent  pattern for array interview  questions  is the existence  of a straightforward  brute-force  solution\nthat uses O(n) space, and a more clever solution  that uses the array itself to lower the space complexity\ndown to O(1). Another  pattern we’ve seen when dealing with arrays is the prevalence  of off-by-  1 errors\n— it’s easy to crash the program  by accidentally  reading  past the last element  of an array.\nFor jobs where Python knowledge  is important,  interviews  may cover list comprehensions,  due to\ntheir expressiveness  and ubiquity  in codebases.  As an example,  below, we use a list comprehension\nto create  a list of the first 10 positive  even numbers.  Then, we use another  list comprehension  to find\nthe cumulative  sum of the first list:\n[x*2 for x in range(1, 11)] # list creation\n(sum(a[:x])  for x in range (len(a)+1)]  # cumulative  sumn\nArrays are also at the core of linear algebra since vectors are represented  as |-D arrays, and matrices\nare represented  by 2-D arrays. For example, in machine learning, the feature matrix X can be\nrepresented  by a 2-D array, with one dimension  as the number of data points (7) and the other as the\nnumber  of features  (d).\nAce the Data Science  Interview 185"
  },
  {
    "page_number": 198,
    "content": "CHAPTER  9 : CODING\nLinked  Lists\nA linked list is composed  of nodes with data that have pointers  to other nodes. The first node is called\nthe head, and the last node is called the tail. Linked lists can be circular,  where the tail points to the\nhead. They can also be doubly linked, where each node has a reference  to both the previous  and next\nnodes. Linked  lists are optimal  for insertion  and deletion,  with O(1) insertion  time at the head or tail,\nbut are worse for indexing  and searching,  with a runtime  complexity  of O(N) for indexing  and O(N)\nfor search.\nHead\nSingle Linked  List\n4 m3 > 7 2 |+t——~ Null\nHead_ Double Linked List\nNut 4 3 ~ | 7 ane —7—> Null\nCommon  linked list questions  include:\n¢ Reversing  a linked  list\n¢ Detecting  a cycle in a linked list\n* Removing  duplicates  from a sorted  linked  list\n¢ Checking  if a linked list represents  a palindrome\nAs an example,  below we reverse  a linked list. Said another  way, given the input linked  list 4 > 1 3\n3 — 2, we want to write a function  which returns  2 > 3 + | — 4. To implement  this, we first start\nwith a basic node class:\nclass Node:\ndef init (self, val):\nself.val  = val\nself.next  = None\nThen we create the LinkedList  class, along with the method to reverse its elements.  The reverse\nfunction  iterates  through  each node  of the linked list. At each step, 1t does a series of swaps between\nthe pointers  of the current  node and its neighbors.\nClass LinkedList:\nde f init (self):\nself.head  = None\ndef reverse(self):\nprev = None\n186 Ace the Data Science  Interview  | Coding"
  },
  {
    "page_number": 199,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\ncurr = self.head\nwhile curr:\nnext = curr.next\ncurr.next  = prev\nprev = curr\ncurr = next\nself.head  = prev\nLike array interview  questions,  linked list problems  often have an obvious  brute-force  solution  that\nuses O(”) space, but then also a more clever solution  that utilizes  the existing  list nodes to reduce\nthe memory  usage to O(1). Another  commonality  between  array and linked list interview  solutions\nis the prevalence  of off-by-one  errors. In the linked list case, it’s easy to mishandle  pointers  for the\nhead or tail nodes.\nStacks  & Queues\nA stack is a data structure  that allows adding and removing  elements  in a last-in, first-out  (LIFO)\norder. This means  the element  that 1s added last is the first element  to be removed.  Another  name for\nadding  and removing  elements  from  a stack is pushing  and popping.  Stacks are often implemented\nusing an array or linked  list.\nA queue 1s a data structure  that allows adding  and removing  elements  in a first-in,  first-out  (FIFO)\norder. Queues  are also typically  implemented  using an array or linked list.\nStack Push\nPop\nQueue\nEnqueue ee ee ee ee ee ee ee  ee : Dequeue\nThe main difference  between  a stack and a queue is the removal  order: in the stack, there is a LIFO\norder, whereas  in a queue it’s a FIFO order. Stacks are generally  used in recursive  operations,  whereas\nqueues  are used in more iterative  processes.\nCommon  stacks and queues interview  questions  include:\n¢ Writing  a parser to evaluate  regular  expressions  (regex)\n¢ Evaluating  a math formula  using order of operations  rules\n¢ Running  a breadth-first  or depth-first  search through  a graph\nAn example interview problem that uses a stack is determining  whether a string has balanced\nparentheses.  Balanced,  in this case, means every type of left-side parentheses  is accompanied  by\nvalid right-side  parentheses.  For instance, the string “({}((){}))”  is correctly  balanced,  whereas the\nstring “{}(})” is not balanced,  due to the last character,  ‘)’. The algorithm  steps are as follows:\nAce the Data Science  Interview 187"
  },
  {
    "page_number": 200,
    "content": "CHAPTER  9 : CODING\n1) Starting  parentheses  (left-sided  ones) are pushed onto the stack.\n2) Ending parentheses  (right-sided  ones) are verified  to see if they are of the same type as the most\nrecently  seen left-side  parentheses  on the stack.\n3) If the parentheses  are of the same type, pop from the stack. If they don't match, return false\nsince the parentheses  are mismatched.\n4) Continue  parsing the input until it’s completely  processed,  and the stack is empty (every pair\nof parentheses  was correctly  accounted  for), in which case, return true. Or, if the stack is not\nempty, in which case, return false.\ndef check balance(s):\nleft side = [“(“%, “{TM, “(\"] # left parentheses\nright side = [“)”, “}”, “]”] # right parentheses\nstack = [] # stack\nfor iin s:\nif i in left side:\nstack.append(i)  # push onto stack\nelif i in right_side:\npos = right side.index(i)  # get right char\n# check match\nif len(stack)  == 0 or (left side[pos]  != stack[len(stack)-1]):\nreturn False\nelse:\nstack.pop()  # remove and continue\nif len(stack)  == 0: # balanced\nreturn True\nelse:\nreturn False\nHash Maps\nA hash map stores key-value  pairs. For every key, a hash map uses a hash function  to compute\nan index, which locates the bucket where that key’s corresponding  value is stored. In Python, a\ndictionary  offers support  for key-value  pairs and has the same functionality  as a hash map.\n[Drake | 521-8976\n521-1234\n| CardiB\n188 Ace the Data Science  Interview  | Coding"
  },
  {
    "page_number": 201,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nWhile a hash function  aims to map each key to a unique  index, there will sometimes  be “collisions”\nwhere  different  keys have the same index. In general,  when you use a good hash function,  expect  the\nelements  to be distributed  evenly throughout  the hash map. Hence,  lookups,  insertions,  or deletions\nfor a key take constant  time.\nDue to their optimal  runtime  properties,  hash maps make a frequent  appearance  in coding  interview\nquestions.\nCommon  hash map questions  center  around:\n¢ Finding  the unions  or intersection  of two lists\n¢ Finding  the frequency  of each word in a piece of text\n¢ Finding  four elements  a, b, c and dina list such thata+b=c¢+d\nAn example  interview  question  that uses a hash map is determining  whether  an array contains  two\nelements  that sum up to some value. For instance,  say we havea list [3, 1, 4, 2, 6, 9] and &. In this case,\nwe return  true since 2 and 9 sum up to 11.\nThe brute-force  method  to solving  this problem  is to use a double for-loop  and sum up every pair of\nnumbers  in the array, which provides  an O(N‘2)  solution.  But, by using a hash map, we only have\nto iterate through  the array with a single for-loop.  For each element  in loop, we’d check whether  the\ncomplement  of the number  (target  - that number)  exists in the hash map, achieving  an O(N) solution:\ndef check sum(a, target):\na= {} # create dictionary\nfor i in a:\nif (target  - i) in d: # check hashmap\nreturn True\nelise:\nad{ijJ = i # add to hashmap\nreturn  False\nDue to a hash function’s  ability to efficiently  index and map data, hashing  functions  are used in\nmany real-world  applications  (in particular,  with regards  to information  retrieval  and storage).  For\nexample,  say we need to spread  data across many databases  to allow for data to be stored and queried\nefficiently  while distributed.  Sharding,  covered  in depth in the databases  chapter,  is one way to split\nthe data. Sharding  is commonly  implemented  by taking  the given input data, and then applying  a hash\nfunction  to determine  which specific  database  shard the data should  reside on.\nTrees\nA tree is a basic data structure  with a root node and subtrees  of children  nodes. The most basic\ntype of tree is a binary tree, where each node has at most two children  nodes. Binary trees can be\nimplemented  with a left and nght child node, like below:\nclass TreeNode:\ndef init (self, val):\nself.val  = val\nself.left  = None\nself.right  = None\nAce the Data Science  Interview 189"
  },
  {
    "page_number": 202,
    "content": "CHAPTER  9 : CODING\nThere are various  types of traversals  and basic operations  that can be performed  on trees. For example,\nin an in-order traversal, we first process the left subtree of a node, then process the current node,\nand, finally,  process  the nght subtree:\ndef inorder  (node):\n| if node is None:\nreturn []\nelse:\nreturn inorder  (node.left)  +-[{node.val]  + inorder  (node.right)\nThe two other closely  related traversals  are post-order  traversal  and pre-order  traversal.  A simple  way\nto remember  how these three algorithms  work is by remembering  that the “‘post/pre/in”  refers to the\nplacement  of the processing  of the root value. Hence, a post-order  traversal  processes  the left child\nnode first, then the right child node and, in the end, the root node. A pre-order  traversal  processes  the\nroot node first, then the left child node, and then, the right child node.\nLevel-order  Tree Traversal:\n9,12, 5, 3,4, 11, 2,6, 7,8\nIn-order  Tree Traversal:\n3,12,6,4,  7,9, 11,5, 2,8\n(3) (4) (1) (2) Pre-order  Tree Traversal:\n9,12, 3,4, 6, 7, 5, 11, 2, 8\n(6) (7) (8) Post-order  Tree Traversal:\n3, 6,7, 4, 12, 11, 8, 2, 5,9\nFor searching,  insertion,  and deletion,  the worst-case  runtime  for a binary tree is O(4V), where N is\nthe number  of nodes in the tree.\nCommon  tree questions  involve  writing functions  to get various  properties  of a tree, like the depth\nof a tree or the number  of leaves in a tree. Oftentimes,  tree questions  boil down to traversing  through\nthe tree and recursively  passing  some data in a top-down  or a bottom-up  manner.  Coding  interview\nproblems  also often focus on two specific  types of trees: Binary Search  Trees and Heaps.\nBinary  Search  Trees\nA binary search tree (BST) is composed  of a senes of nodes, where any node in a left subtree is\nsmaller  than or equal to the root, and any node in the right subtree is larger than or equal to the\nroot. When BSTs are height balanced  so no one leaf is much deeper  than another  leaf from the root,\nsearching  for elements  becomes  efficient.  To demonstrate,  consider  searching  for the value 9 in the\nbalanced  BST below:\nExample  of a Binary  Search  Tree\n190 Ace the Data Science  Interview  | Coding"
  },
  {
    "page_number": 203,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nTo find 9, we first examine  the root value, 8. Since 9 is greater  than 8, the node containing  9, if it\nexists, would  have to be on the right side of the tree. Thus, we’ve cut the search space in half. Next,\nwe compare  against  the node 10. Since 9 is less than 10, the node, should  it exist, has to be on the left\nof 10. Again,  we’ve cut the search space in half. In conclusion,  since 10 doesn’t  have  a left child, we\nknow 9 doesn’t  occur in the tree. By cutting  the search space in half at each iteration,  BSTs support\nsearch,  insertion,  and deletion  in O(log N) runtime.\nBecause  of their lookup efficiency,  BSTs show up frequently  not just in coding interviews  but in\nreal-life  applications.  For instance,  B-trees,  which are used universally  in database  indexing,  are a\ngeneralized  version  of BSTs. That is, they allow for more than 2 nodes (up to M children),  but offer\na searching  and insertion  process similar to that of BST. These properties  allow B-trees to have\nO(log  NV) lookup  and insertion  runtimes  similar  to that of BSTs, where N is the total number  of nodes\nin the B-tree. Because  of the logarithmic  growth  of the tree depth, database  indexes  with millions  of\nrecords  often only have a B-tree  depth of four or five layers.\n97 | 157 | 232\n35 | 50 | 79 128 | 140 167 | 200 270 | 290\n104 | 117 120 145 250 | 264 269 300 | 330\nExample  of a B-Tree\nCommon  BST questions  cover:\n¢ ‘Testing  if a binary  tree has the BST property\n¢ Finding  the k-th largest  element  in a BST\n¢ Finding  the lowest common  ancestor  between  two nodes (the closest common  node to two input\nnodes such that both input nodes are descendants  of that node)\nAn example  implementation  of a BST using the TreeNode  class, with an insert function,  is as follows:\nclass TreeNode:\ndef init (self, val):\nself.val  = val\nself.left  =\nself.right  = NoneNone\nclass BST:\ndef init (self, val):\nself.root  = TreeNode  (val)\ndef insert(self,  node, val):\nif node is not None:\nif val < node.val:\nif node.left  is None:\nAce the Data Science  Interview 191"
  },
  {
    "page_number": 204,
    "content": "CHAPTER  9 : CODING\nnode.left  = TreeNode  (val)\nelse:\nself.insert  (node.left,  val)\nelse:\nif node.right  is None:\nnode.right  = TreeNode  (val)\nelse:\nself.insert(node.right,  val)\nelse:\nself.root  = TreeNode  (val)\nreturn\nHeaps\nAnother  common  tree data structure  is a heap. A max-heap  is a type of heap where each parent node\nis greater  than or equal to any child node. As such, the largest value in a max-heap  is the root value of\nthe tree, which can be looked up in O(1) time. Similarly,  for a min-heap,  each parent node is smaller\nthan or equal to any child node, and the smallest  value lies at the root of the tree and can be accessed\nin constant  time.\nHeap Data Structure\n©oO  \\ oO \\\n/ \\ \\ f-\\ IN [\noO Oo @M@ O&@ O© YO OM @\nMinimum  Heap Maximum  Heap\nTo maintain  the heap property,  there is a sequence  of  operations  known  as “heapify,”  whereby  values\nare “bubbled  up/down”  within the tree based on what value is being inserted  or deleted.  For example,\nsay we are inserling  a new value into a min-heap.  This value starts at the bottom  of the heap and then\nis swapped  with its parent node (“bubbled  up’’) until it is no longer  smaller  than its parent  (in the case\nof a min-heap).  The runtime  of this heapify  operation  is the height of the tree, O(log N).\nIn terms of runtime,  inserting  or deleting  1s O(log NV), because  the heapify  operation  runs to maintain\nthe heap property.  The search  runtime  is O(N) since every node may need to be checked  in the worst-\ncase scenario.  AS mentioned  earlier, heaps are optimal for accessing  the min or max value because\nthey are at the root, 1.¢., O(1) lookup  time. Thus, consider  using heaps when you care mostly  about\nfinding  the min or max value and don’t need fast lookups  or deletes  of arbitrary  elements.\nCommonly  asked heap interview  questions  include:\ne finding  the K largest  or smallest  elements  within  an array\n¢ finding  the current  median  value in a stream  of numbers\n* — sorting  an almost-sorted  array (where  clements  are just a few places off from their correct  spot)\nTo demonstrate  the use of heaps, below we find the k-largest  elements  in a list, using the heapq\npackage  in Python:\n192 Ace the Data Science  Interview  | Coding"
  },
  {
    "page_number": 205,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nimport heapg\nk = 5\na = [13, 5, 2, 6, 10, 9, 7, 4, 3]\nheapq.heapify(a)  # creates  heap\nheapq.nlargest  (k, a) # finds k-largest\nGraphs\nA B Cc OD Node | Adjacent  Node(s)\nA}O;} 1 {141 A BCD\nB;O;0O]0O};  1 B D\nC;}o0];1t]O07  0 C B\nD}O;O0O]1  } 0 D C\nTo Graph To Adjacency  Matrix To Adjacency  List\nThe lookup  time to check whether  two nodes are neighbors  for an adjacency  matrix is O(1). For an\nadjacency  list, it could be O() in the worst case (if a node has edges to every other node). What\nan adjacency  list lacks in time efficiency  compared  to an adjacency  matrix, it makes up for in space\nefficiency.  For a large graph with » nodes that are sparse  (the V nodes don’t have many connections  to\neach other),  an adjacency  list offers a compact  way to represent  the edges, versus  an adjacency  matrix\nwhich requires  you to store an N%2 sized matrix in memory.  For instance,  consider  the Facebook\nfriendship  graph, which has 2 billion users (nodes),  but a maximum  of only 5,000 connections  per\nnode (the 5,000 friend limit). A full adjacency  matrix would need to be 2 billion rows by 2 billion\ncolumns,  whereas  an adjacency  list would require  considerably  less memory.\nAn example  implementation  of a graph is below. The Vertex class uses an adjacency  list to keep track\nof its neighbors  with weights:\nclass Vertex:\ndef init (self, val):\nself.val  = val\nself.neighbors  = {}\ndef add_ to_neighbors(self,  neighbor,  w): # add to neighbor  dict with weight\nself.neighbors  [neighbor]  = w\ndef get neighbors  (self):\nreturn self.neighbors.keys  ()\nAce the Data Science  Interview 193"
  },
  {
    "page_number": 206,
    "content": "CHAPTER  9: CODING\nclass Graph:\ndef init (self):\nself.vertices  = {}\ndef add  vertex(self,  val):\nnew vertex = Vertex (val)\nself.vertices[(val]  = new_vertex  _\nreturn new_vertex\ndef add  edge(self,  u, v, weight): # add edge from u to v\nif u not in self.vertices:\nself.add  vertex  (Vertex  (u) )\nif v not in self.vertices:\nself.add  vertex  (Vertex  (v) )\nself.vertices[u].add_to  neighbors  (self.vertices[v],  weight)\ndef get  vertex(self,  val):\nreturn self.vertices[val]\ndef get vertices(self):\nreturn self.vertices.keys  ()\ndef iter (self):\nreturn iter(self.vertices)\nReal-World  Applications  of Graphs\nGraphs  serve as a prevalent  data structure  for representing  many real-world  problems.  For example,\nPageRank,  developed  by Google co-founders  Larry Page and Sergey Brin, is an algorithm  that\nmeasures  how important  various web pages are. PageRank  represents  each web page as a node,\nand each hyperlink  from one page to another  represents  an edge. The underlying  assumption  1s that\nimportant  web pages are more likely to have been linked to by other influential  pages on the web.\nIn the context  of graphs,  this means that nodes with a high number  of edges from other high-quality\nnodes are likely to have a better PageRank  score.\nAnother  real-world  use of graphs  is for computational  graphs  used within  scheduling  tools like Airflow\nor large-scale  data processing  frameworks  like Spark or Tensortlow.  These frameworks  represent  the\nseries of computations  and data flows that need to be performed  as a directed  acyclic  graph (DAG),\nwhich ts a directed  graph (meaning  edges are unidirectional)  that has no cycles. Nodes represent\noperations  that create or manipulate  data, and edges represent  how data (typically  multidimensional\narrays also known  as tensors)  must flow from one operation  (node) to another.\nThese computational  graphs enable parallelism,  since it’s easy to order the operations  in such a way\nthat certain  operations  that do not depend  on one another  can be run concurrently.  Another  advantage\nis that the computational  graph offers a language-agnostic  representation  of the computations  we\nwant, which can easily be ported between  underlying  frameworks.\n194 Ace the Data Science  Interview | Coding"
  },
  {
    "page_number": 207,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\name  paragraph_extraction-37  I\ncombine-37 page-37 image_processing-37  }—P header_extraction-37NH  table_extraction-37\nvam  pean ens\n[ start bof get_pat H page-35 H image_processing-35  } >| table_extraction-35  H combine-35 H total  _page_combine  |\n\\ een ora waa\npage-36 H image_processing-36  || header extraction-36 [ combine-36 |\n\\ table-extraction-36  a\nAn example  Airflow  execution  graph to extract  a data table from a large PDF file using image processing\nBreadth-First  and Depth-First  Search\nInterview  problems  related to graphs often involve traversing  the graph, either in a breadth-first-\nsearch (BFS) or in a depth-first-search  (DFS). In BFS, start with one node and add it to a queue.\nFor each node in the queue, process the node and add its neighbors  to the queue. In DFS, you also\nstart with one node, but instead of processing  all the neighbors  next, you recursively  process each\nneighbor  one by one.\nABDECFG ABCDEFG\nBelow is an example  of BFS using a queue for iterative  processing:\ndef bfs(graph,  v):\nn = len(graph.vertices)\nvisited  = [False] * (n+l)\nqueue = []\nAce the Data Science  Interview 195"
  },
  {
    "page_number": 208,
    "content": "CHAPTER  9: CODING\nqueue.append  (v)\nvisited[v])  = True\nwhile queue:\ncurr = queue.pop  (0)\nfor i in graph.get  vertex(curr):\nif visited[{i.val]  == False:\nqueue.append(i.val)\nvisited[i.val]  = True\nreturn visited\nBelow is a primitive  example  of DFS, where visited tracks the set of processed  nodes, and v is the\nnode currently  being processed:\ndef dfs  _helper(graph,  v, visited):\nvisited.add(v)\nfor neighbor  in graph.get  vertex(v).get_neighbors  ():\nif neighbor.val  not in visited:\ndfs  helper(graph,  neighbor.val,  visited)\nreturn visited\ndef dfs(graph,  v):\nvisited  = set()\nreturn dfs  helper(graph,  v, visited)\nThe runtime  for both is O(£ + V), where E is the number  of edges in the graph and V is the number\nof vertices. For most basic use cases, DFS and BFS are interchangeable.  However,  there are some\ndifferences  in their use cases and advantages.\nWhen interview  problems  concern  opumization,  such as finding the shortest  path between  nodes,\nconsider  using BFS. It can find solutions  definitively  and never gets trapped  in recursive  sub-calls.\nSome drawbacks  to BFS are that it uses more memory  when storing  nodes and can take a lot of time\nif solutions  are far away from the root.\nWhen interview  problems  concern analyzing  a graph’s structure,  such as when finding  cycles or\norderings  in directed  graphs (for example,  topological  sorting),  consider  using DFS. Because  it\nexhausts  paths before trying others, it may be trapped  recursively  and cannot guarantee  a solution.\nHowever,  DFS has fewer memory  requirements  and can find long-distance  elements  in a shorter\namount  of time compared  to BFS.\nAlgorithms\nRecursion\nA recursive  function  is one that calls itself directly  or indirectly.  It can be broken  down into two parts:\nthe recursive  case (the part that calls itself) and the base case (which  terminates  the recursion).\n196 Ace the Data Science  Interview | Coding"
  },
  {
    "page_number": 209,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nFor an example  of a recursive  algorithm,  consider  the classic problem  of producing  Fibonacci\nnumbers  (0, 1, 1, 2, 3, 5, 8, etc.):\nGef fib(n):\nif n == 0:\nreturn 0\nif n == 1 orn ==\nreturn 1\nelse:\nreturn fib(n-1)+fib(n-2)\nIf you draw the tree of recursive  calls, you’ll see that many times fib(”) for a particular  7 is called\nrepeatedly.  This repetition  leads to an undesirable  exponential  runtime  and memory  usage. Since\nrecursive  calls are stored on a stack, and often, there is limited  allocated  memory  for a program,\nrecursive  solutions  can also lead to stack overflows  which crash the program.\nfib (6)\nZoooN 4ooON\nJON fib (3) Yo \\ fib(2)\nfib(3) fib(2) fib(2) fib(1) fib(2) fib(1)\nfib(2) fib(1l\nIn contrast,  iterative  algorithms  do not overwhelm  the call stack. Such algorithms  rely on using  a for-\nioop or while-loop,  rather than calling  themselves  repeatedly  like in recursion.  An iterative  example\nof the Fibonacci  sequence  is as follows:  we use a for-loop  to iterate up to 7, set the current  result to\nbe the sum of the previous  Fibonacci  number  at indexes  n-2 and n-|, and continue  this until we reach\nthe desired  n:\ndef fib(n):\nif n ==\nreturn 0\nif n = 1 orn ==\nreturn 1\nelse:\nprev2 = 0 # fib(n-2)\nprev = 1 # fib(n-1)\nres = 0 # filb(n)\nfor iin range(i, n): # process iteratively\nres = prev2 + prev\nprev2 = prev\nprev = res\nreturn res\nAce the Data Science  Interview 197"
  },
  {
    "page_number": 210,
    "content": "CHAPTER  9 : CODING\nCompared  to the recursive solution, the iterative approach is less expressive  code-wise  but more\nmemory efficient  since there aren’t O(N) recursive  calls, but instead  just two variables  to keep track\nof (the most recent two Fibonacci  numbers).\nGreedy  Algorithms\nGreedy algorithms  choose at each step what seems to be the best option. In other words, greedy\nproblems  reduce problems  into smaller ones by making the locally optimal choice at each step.\nA classic example where a greedy algorithm  works well is when making change with the minimum\nnumber of coins. Say, for instance, you wanted to make change for 67 cents, and all you had were\npennies, nickels, dimes, and quarters. The greedy approach is to use as many coins of the highest\ndenomination  as possible  (two quarters),  before continuing  to the next denomination  (one dime). The\ncode is as follows:\ndef minCoins  (k):\ncoins = [{1, 5, 10, 25]\nn = len(coins)\nres = []\ni = n-l\nwhile(i  >= 0 and k >= Q):\nif k >= coins[1]:\nk -= coins[i]\nres.append  (coins [i])\nelse:\ni -= 1\nreturn res\nin the real world, greedy algorithms  show up in various areas of machine  learning. For example,\ndecision  trees are split in a greedy manner  to maximize  information  gain (reduction  in entropy  based\non the teature chosen) at each split. For every feature, we evaluate  all possible  features,  then, in a\ngreedy fashion,  choose the feature with the best information  gain to split on next. This process  takes\nplace recursively  until we end up with leaf nodes. The pseudocode  for getting the best feature is as\nfollows:\ninfo gains = [getInfoGain(feature)  for feature  in features]\nbest feature  index = np.argmax(info  gains)\nbest feature  = features{best  feature  _ index]\nFor interview  questions,  keep greedy  algorithms  in mind for optimization  problems,  where there's  an\nobvious  set of choices  to select from, and it’s easy to know what the appropriate  choice is. Keep in\nmind that it’s often easier to reason about a greedy algorithm  recursively,  but then implement  it later\niteratively  for better memory  performance.\nDynamic  Programming\nEarlier in the recursion  section,  we saw how an iterative  solution  to generating  the n-th Fibonacci\nnumber  had its advantage  over a recursive  version  since It used less memory  and didn’t recompute\n198 Ace the Data Science  Interview  | Coding"
  },
  {
    "page_number": 211,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nwork. That’s where dynamic  programming,  the art of storing results of subproblems  to speed\nup recursion,  comes in. Although,  technically,  the iterative  solution  serves as a form of dynamic\nprogramming,  to be more explicit,  for pedagogical  purposes,  below we show how an array can be\nused to cache existing  results.  Now, instead  of making  the previous  recursive  calls all the way down,\nwe get the stored  sub-result  from the cache to prevent  values from being recomputed.\ndp = [0] * 1000\ndef fib(n):\nif n == 0 orn == 1;\nreturn n ,\nelse:\ndp{n} = fib(n-1)  + fib(n-2)\nreturn dp[n]\nTwo properties  are needed  for dynamic  programming  (DP) to be applicable:\n1) optimal  substructure\n2) overlapping  subproblems\nAn optimal substructure  means the problem  can be solved optimally  by breaking  it down into\nsubproblems  and solving  those subproblems.  Overlapping  subproblems  indicates  the problem  can\nbe broken  down into subproblems,  that are then reusable  in other subproblems.  If both are present,\nthen calculating  and storing  the solutions  to subproblems  in a recursive  manner  will solve the overall\nproblem.  Note that the Fibonacci  satisfies  the two requirements  because:\n1) Fuib(”) is found by solving  the subproblems  fib(-1)  and fib(7-2)\n2) Subproblems  are overlapping:  both fib(7) and fib(n-1)  require  having  solved fib(n-2)\nDynamic  programming  can be implemented  with a top-down  or bottom-up  approach.  In a top-down\napproach,  we start with the top and break the problem  into smaller chunks (as in the Fibonacc1\nexample).  In practice, the top-down  approaches  often are implemented  with recursion,  and the\nintermediate  results are cached  in a hash table.\nIn contrast,  in a bottom-up  manner,  we start with the smaller  problems  and continue  to the top. These\nsolutions  are often implemented  iteratively,  where an 7-dimensional  array is used to cache previous\nresults. A bottom-up  Fibonacci  example  would  be:\ndef fib(n):\ndp = [0 for in range(ntl)]\ndp[(1] = 1\nfor iin range(2,  nti):\ndp({i] = dpl[i-i] + dp[i-2]\nreturn dpf[n]\nDynamic  programming  isn’t just an academic  exercise  or coding interview  favorite —~ it often shows\nup in the real world, too. For instance, in reinforcement  learning, the goal is to understand  what\nAce the Data Science  Interview 199"
  },
  {
    "page_number": 212,
    "content": "CHAPTER  9: CODING\nactions to take given a particular  state of the universe to maximize  an expected  eventual payoff.\nThe famous Bellman  equations  are a core part of the reinforcement  learning  process and utilize the\ndynamic  programming  approach.  The Bellman  equations  break down the total eventual  payoff  into a\nseries of smaller  subproblems  that can each be optimized,  where the best eventual  payoff  comes from\ncombining  different  subproblem  payoffs.\nGreedy  Algorithms  Vs. Dynamic  Programming\nAs explained  earlier, a greedy algorithm  does whatever  is locally optimal  and hence always chooses\nthe option that leads to the best result for the next step. This contrasts  with dynamic  programming,\nwhich will exhaust the search space and is guaranteed  to find the globally optimal solution,  not\njust the locally optimal solution. Therefore,  you can think of greedy algorithms  as getting the\n“local maximum/minimum,”  but not necessarily  the “global maximum/minimum”  that dynamic\nprogramming  achieves.\nFor a concrete  example  of where greedy algorithms  can fall short, consider  the classic 0/1 knapsack\nproblem  with the values below. In this problem,  we are choosing  between  N items, each with some\npositive  value and positive  weight.  We want to maximize  the amount  of value obtained  by selecting\nitems with a total weight  of no more than W.\nExample  Knapsack\nAa _ litem = Value = Weight = Value/Weight\nA 3 4 3/4\nB 1.2 2 1.2/2\nC 2 3 2/3\nThe greedy implementation  is to just sort the elements  that maximize  value per unit weight and\nchoose  the most efficient  items.\ndef knapsackGreedy  (values,  weights,  max weight):\ncurr weight = 0\nvalue list, weight list = ({], [(]) # values and weights\ntotal sorted  = sorted(zip(values,  weights),\nkey=lambda  x: x[{0}]/x[1],  reverse=True)\nvalues sorted, weights  sorted  = zip(*total_  sorted)\n# iterate  in sorted fashion\nfor value, weight in zip(values  sorted,  weights  sorted):\nif weight + curr weight <= max weight: # meets weight requirement\nvalue list.append  (value)\nweight  list.append  (value)\ncurr weight  += weight\nreturn sum(value  list)\nHowever,  this does not maximize  the amount  of value obtained  overall.  Take, for example,  the table\nof weights  and values provided  earlier,  along with a total weight  limit of 5.\n200 Ace the Data Science  Interview  | Coding"
  },
  {
    "page_number": 213,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nGreedily,  we would select item A (the highest  value/weight  ratio) and end up with  a total value of 3,\nsince the weight  constraint  does not allow for extra items. However,  the best possible  method  would\nbe to take B and C for a total value of 3.2, which is larger than 3.\nIn contrast, a dynamic  programming  approach  achieves  the global maximum.  An example  of a\nbottom-up  approach  is below,  where the core logic is in deciding  whether  to take an item or not. If an\nitem meets the weight  requirement,  then we either\n* Take the item, and get a new optimal maximum  value, which now gives us a new weight\nconstraint  (current  weight  minus item’s  weight),  or\n* Do not take the item, and continue  with the current  optimal  maximum  value and the current\nweight  constraint.\ndef knapsackDP(values,  weights,  max weight):\nn = len (values)\ndp = [[0 for x in range(max_weight+1)]  for x in range(n+1)]  # 2d-array\nfor i in range(ntl):\nfor w in range  (max  weight+1):\nif weights[i-1]  <= w: # meets weight requirement\ndQp{i]l{w]  = max(values{i-l]J+dp[i-1]  [w-weights[i-1]}],\ndp[i-l]{w])  # either take value or not\nelse:\ndp (i} (w]\nreturn dp[n] {max weight]Gp[i-l}](w])  # not taking value\nThe dynamic  programming  approach  coded  above leads to the optimal  solution  of taking ttems B and\nC for a total value of 3.2. When solving  coding  interview  problems,  it’s crucial to not fall for a more\napparent  greedy  approach  when the correct  answer  1s found through  dynamic  programming.\nSorting\nTwo unique sorting algorithms  —- mergesort  and quicksort  — arise in coding interviews  from time\nto time. While it’s rare to be asked to code up these algorithms,  both sorts are often modified  to solve\na problem  or used as an intermediate  step within a larger coding interview  solution.  This is because\nsorting  the input can expose some structure,  which makes the problem  simpler.\nMergesort\nMergesort  uses a “‘divide-and-conquer”  approach  as follows:\n1. Repeatedly  divide the input into smaller subarrays,  until a base case of a single element is\nreached  (this single element  subarray  is considered  sorted).\n2. Repeatedly  merge the smaller sorted subarrays  into bigger sorted subarrays,  until the entire\ninput is merged  back together.\nAce the Data Science  Interview 201"
  },
  {
    "page_number": 214,
    "content": "CHAPTER  9: CODING\nBelow is an example  of mergesort:\n5 4113 10 7 {1\n5 4 13 10 7 1\n’ \\ / Vv \\ JL\n5 4/113 10 117\n4;51]13 147110a\n1}, 4)5 ] 7 110) 13\nOverall,  mergesort  has a runtime  complexity  of O(N log NV) and also requires  O(N log N) space to\nsupport  the auxiliary  merging  steps. An example  implementation  of mergesort  that utilizes  a helper\nfunction  for merging  subarrays  1s as follows:\ndef merge helper(a,  low, high, mid):\nif len(a) ==\nreturn  a\ni= j3 =k = 0 # k is for merged array\nleft = a[:mid] # copy of left side\nright = a{mid:] # copy of right side\nwhile i < len(left)  and j < len(right):  # compare  left and right sides, add\nif seftii} < right[s):\na[k] = left[i]\ni += 1\nelse\na{fk} = right{j]\njl\nkK t= ]\nwhile i < len(left):  # remaining  on left\na{k] = left{i]\n202 Ace the Data Science  Interview | Coding"
  },
  {
    "page_number": 215,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\ni += 1\nk += 1\nwhile j < len(right):  # remaining  on right\na(k] = right[j]\n4 t= 1\nk += 1\nreturn a\ndef mergesort(a,  low, high):\n1£ low >= high:\nreturn  a\nmid = (low + high-1) // 2\nmergesort(a,  low, mid)\nmergesort(a,  mid+l, high)\nmerge helper(a,  low, high, len(a) // 2)\nreturn  a\nQuicksort\nQuicksort  selects  an arbitrary  pivot element  and puts all elements  smaller  than the pivot to the left of\nthe pivot, and larger elements  to the right of the pivot. The exchanging  of elements  occurs through\nswaps.  This process  of selecting  a pivot and swapping  the left and right elements  ensues recursively,\nuntil the base case is hit when just two elements  are swapped  into their correct  relative  order.\nPivot\n11}/-2;/5]/3]6]  8]; -7]) 0] 4\n<=4 >=4\n2} 3|-7] 0 4 815 {111} 6\n<=0 >=0 <=6 >=6\n-2|-/ 0 3 fs) 6 111 8\n>=-/ >=8\n7 9 8 14\nThe worst-case  runtime for quicksort  is O(N2),  although,  in practice  the expected  runtime 1s closer\nto O(N log N) through picking a “smart” pivot whereby  the elements  are roughly divided into equal\nhalves upon each iteration.\nAce the Data Science  Interview 203"
  },
  {
    "page_number": 216,
    "content": "CHAPTER  9 : CODING\nBelow is an example  implementation  of quicksort  that utilizes a helper function  to partition  the array:\ndef helper(a,  low, high):\n3 = low-1 # smaller element index\npivot = a{high]) # pivot\nfor } in range(low,  high):\nif a[j] <= pivot:\ni += 1 # increment\nafi], alj] = a({j], ali] # swap\na{itl], af{high] = afhigh], a[itt]\nreturn itl\ndef quicksort(a,  low, high):\nif len(a) == 1:\nreturn  a\nif low < high:\npivot = helper(a,  low, high) # place pivot\nquicksort(a,  low, pivot-1) # recurse left side\nquicksort(a,  pivot+1l, high) # recurse right side\nreturn a\nMatrix  Multiplication\nSince all machine leaming algorithms  eventually  reduce to a series of matrix multiplications,\nspeeding  up these operations  is crucial for large-scale  machine  learning  applications.  Using the direct\nmathematical  definition  of matrix multiplication,  an implementation  of matrix multiplication  gives\nan O(N‘3)  runtime,  as seen by the three nested for-loops  below:\ndef matrix multiply(A,  B):\nm= [[{0 for row in range(len(B[(0]))]  for col in range(len(A))]\nfor i in range(len(A)):\nfor j} in range(len(B[0])):\nfor k in range(len(B)):\nmlil [3] = m{i]{3] + Afi] (k] * BEk] [3] # add\nreturn  m\nNevertheless.  improving  upon this runtime is possible. For example, we can break down the\nmultiplication  into a series of sub-matrix  multiplications.  These sub-matrix  multiplications  can be\ncalculated  in parallel,  which enables  the task to be accomplished  tn a distributed  manner.\nFinance  companies  like to ask candidates  about coding up matrix multiplication  or implementing\nother common  linear algebra topics such as singular-value  decomposition  because  it allows firms\nto test your mathematical  understanding  and programming  ability at once. These questions  are also\npicked because  optimizing  numerical  calculations  done in linear algebra  is a relevant  concept  when\ntrying to reduce  the latency  of trading  systems.\n204 Ace the Data Science  Interview  | Coding"
  },
  {
    "page_number": 217,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nInterview  Questions\nEasy Problems\n9.1.\n9.2.\n9.3.\n9.4.\n9.5.\n9.6.Amazon:  Given two arrays,  write a function  to get the intersection  of the two. For example,  if\nA=[1,  2, 3, 4, 5], and B = [0, 1, 3, 7] then you should  return [1, 3].\nD.E. Shaw: Given an integer  array, return the maximum  product  of any three numbers  in the\narray. For example,  for A = [1, 3, 4, 5], you should  return 60, while for B = [-2, -4, 5, 3] you\nshould  return  40.\nFacebook:  Given  a list of coordinates,  write a function  to find the k closest  points  (measured  by\nEuclidean  distance)  to the origin. For example,  if & = 3, and the points are: [({2, -1], [3, 2], (4,\nGoogle:  Say you have an n-by-n matrix of elements  that are sorted in ascending  order both\nin the columns  and rows of the matrix. Return the k-th smallest  element  of the matrix. For\nexample,  consider  the matrix  below:\n; 2\n9\n1|Oo wo =on—_\nIf k = 4, then return 5.\nAkuna  Capital:  Given an integer  array, find the sum of the largest contiguous  subarray  within\nthe array. For example,  if the input is [-1, —3, 5, —4, 3, -6, 9, 2], then return 11 (because  of [9,\n2]). Note that if all the elements  are negative,  you should  return 0.\nFacebook:  Given  a binary  tree, write a function  to determine  whether  the tree is a mirror  image\nof itself. Two trees are a mirror  image of each other if their root values are the same and the left\nsubtree  1s a mirror  image of the right subtree.\nMedium  Problems\n9.7.\n9.8.\n9.9.\n9.10.\n9.11.\n9.12.Google:  Given an array of positive  integers,  a peak element  is greater  than its neighbors.  Write\na function  to find the index of any peak elements.  For example,  for [3, 5, 2, 4, 1], you should\nreturn either | or 3 because  the values at those indexes,  5 and 4, are both peak elements.\nAQR: Given two lists X and Y, return their correlation.\nAmazon:  Given a binary tree, write a function  to determine  the diameter  of the tree, which is\nthe longest  path between  any two nodes.\nD.E. Shaw: Given a target number,  generate  a random  sample of ” integers  that sum to that\ntarget that also are within o standard  deviations  of the mean.\nFacebook:  You have the entire social graph of Facebook  users, with nodes representing  users\nand edges representing  friendships  between  users. Given a social graph and two users as\ninput, write a function  to return the smallest  number  of friendships  between  the two users. For\nexample,  take the graph that consists  of 5 users A, B, C, D, E, and the friendship  edges are: (A,\nB), (A, C), (B, D), (D, E). If the two input users are A and E, then the function  should return 3\nsince A is friends with B, B 1s friends  with D, and D ts friends with E.\nLinkedIn:  Given two strings A and B, write a function  to return a list of all the start indices\nwithin A where the substring  of A is an anagram  of B. For example,  if A = “abcdcbac”  and\nAce the Data Science  Interview 205"
  },
  {
    "page_number": 218,
    "content": "9.13.\n9.14.\n9.15.\n9.16.\n9.17.\n9.18.\n9.19.\n9.20.\n9.21.\n9.22.\n9.23.CHAPTER  9 : CODING\nB = “abc,” then you want to return [0, 4, 5] since those are the starting  indices of substrings  of\nA that are anagrams  of B.\nYelp: You are given an array of intervals,  where each interval is represented  by a start time\nand an end time, such as [1, 3]. Determine  the smallest  number of intervals  to remove from\nthe list, such that the rest of the intervals  do not overlap. Intervals  can “touch,”  such as [1, 3]\nand [3, 5], but are not allowed  to overlap,  such as [1, 3] and (2, 5]). For example,  if the input\ninterval list given is: [[1, 3], [3, 5], (2. 4], (6, 8]], then return |, since the interval [2, 4] should\nbe removed.\nGoldman  Sachs: Given an array of strings, return a list of lists where each list contains  the\nstrings that are anagrams  of one another. For example,  if the input is [“abc”, “abd”, “cab”,\n“bad”, “bca”, “acd”] then return: [[‘‘abc”,  “cab”, “‘bca”], [“abd”,  “‘bad”’], [“acd”’]].\nTwo Sigma: Say that there are 7 people. If person  A is friends with person B, and person  B is\nfriends with person C, then person  A is considered  an indirect  friend of person C.\nDefine a friend group to be any group that is either directly or indirectly  friends. Given an\nn-by-n adjacency  matrix N, where N[i][j] is one if person i and person  / are friends,  and zero\notherwise,  write a function  to determine  how many friend groups  exist.\nWorkday:  Given  a linked list, return the head of the same linked list but with k-th node from\nthe end of a linked list removed.  For example,  given the linked list 3 > 2 > 5 > 1 > 4 andk\n= 3, remove  the 5 node and, thus, return the linked  list 3 — 2 > 1 > 4.\nGoldman  Sachs: Estimate  2 using a Monte  Carlo method.  Hint: think about throwing  darts on\na square  and seeing  where they land within  a circle.\nPalantir:  Given a string with lowercase  letters and left and right parentheses,  remove the\nminimum  number  of parentheses  so the string is valid (every left parenthesis  is correctly\nmatched  by a corresponding  right parenthesis).  For example,  if the string is “)a(b((cd)e(f)g)”\nthen return “ab((cd)e(f)g)”.\nCitadel: Given a list of one or more distinct integers, write a function to generate  all\npermutations  of those integers.  For example,  given the input [2, 3, 4], return the following  6\npermutations:  (2, 3, 4], [2, 4, 3], [3, 2, 4], (3, 4, 2], [4, 2, 3], [4, 3, 2].\nTwo Sigma:  Given  a list of several  categories  (for example,  the strings  A, B, C, and D), sample\nfrom the list of categories  according  to a particular  relative  weighting  scheme.  For example,\nsay we give Aa relative  weight  of 5, B a weight  of 10, C a weight  of 15, and D a weight  of 20.\nHow do we construct  this sampling?  How do you extend the solution  to an arbitrarily  large\nnumber  of & categories?\nAmazon:  Given two arrays with integers,  return the maximum  length of a common  subarray\nwithin both arrays. For example,  if the two arrays are [1, 3, 5, 6, 7] and [2, 4, 3, 5, 6] then\nreturn 3, since the length  of the maximum  common  subarray,  [3, 5, 6], is 3.\nUber: Given  a hist of positive  integers,  return the maximum  increasing  subsequence  sum. In\nother words, return the sum of the largest increasing  subsequence  within the input array. For\nexample,  if the input is [3, 2. 5, 7, 6], return 15 because  it’s the sum of 3, 5, 7. If the input is\n[5, 4, 3, 2, 1], return 5 (since no subsequence  is increasing).\nPalanur:  Given a positive  integer  n, find the smallest  number  of perfect  squares  that sum up to\nn. For example,  for n = 7, you should  return  4, since 7 =4+1+4  141. Forn=  13, you should\nreturn 2, since 13 = 9 + 4,\n206 Ace the Data Science  Interview  | Coding"
  },
  {
    "page_number": 219,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\n9.24. Facebook:  Given an integer  n and an integer  k, output  a list of all of the combinations  of k\nnumbers  chosen  from | to n. For example,  if n = 3 and & = 2, retum [1, 2], {1, 3], [2, 3].\nHard Problems\n9.25. Citadel:  Given  a string with left and right parentheses,  write a function  to determine  the length\nof the longest  well-formed  substring.  For example,  if the input string is “)(())(,”  then return 4,\nsince the longest  well-formed  substring  is “(()).”\n9.26. Bloomberg:  Given  an m-by-n  matrix  with positive  integers,  determine  the length  of the longest\npath of increasing  integers  within  the matrix.  For example,  consider  the input matrix:\nr+\nCO Ot Lbco & Ww[4\n4\n7\n—_\nIn this case, return 5, since one of the longest  paths would  be 1-2-5-6-9.\n9.27. Google:  Given  a number  v, return the number  of lists of consecutive  positive  integers  that sum\nup to”. For example,  for n = 9, return 3, since the consecutive  positive  integer  lists are: [2, 3,\n4], (4, 5], and [9]. Can you solve this in linear time?\n9.28. Citadel:  Given  a continuous  stream  of integers,  write a class with functions  to add new integers\nto the stream,  and a function  to calculate  the median  at any time.\n9.29. Two Sigma: Given an input string and a regex, write a function  that checks  whether  the regex\nmatches  the input string. The input string 1s composed  of the lowercase  letters a-z. The regular\nexpression  contains  lowercase  a-z, “?’, or ‘*’, where the ‘?’ matches  any one character,  and the\n‘*° matches  an arbitrary  number  of characters  (empty  as well). For example,  if the input string\nis ‘“‘abcdba”  and the regex is “a*c?*’’,  return true. However,  if the regex was instead  “b*c?*”\nreturn false.\n9.30. Citadel:  A fire department  wants to build a new fire station  in a location  where the total distance\nfrom the station to all houses in the town (in Euclidean  terms) is minimized.  Given  a list of\ncoordinates  for the n houses, return the coordinates  of the optimal location  for the new fire\nstation.\nSolutions\nSolution  #9.1\nThe simplest  way to check for intersecting  elements  of two lists is to use a doubly-nested  for-loop\nto iterate over one array and check against every element  in the other array. However,  this leads to a\ntime complexity  of O(N*M)  where  N is the length of A, and Mis the length of 8.\nA better approach  is to use sets (which utilizes a hash map implementation  undemeath)  since the\nruntime  time is O(1) for each lookup operation.  Then we can do the series of lookups  over the larger\nset (resulting  in a O(min(N,  M)) total runtime):\ndef intersection(a,  b):\nset_a = set (a)\nset b = set (b)\nAce the Data Science  Interview 207"
  },
  {
    "page_number": 220,
    "content": "CHAPTER  9: CODING\nif len(set_a)  < len(set_b):\nreturn [x for x in set_a if x in set_b]\nelse:\nreturn [x for x in set_b if x in set_a]\nThe time complexity  is O(N + M) due to the creation of the sets, and the space complexity  is also\nO(N + M) since the arrays might contain  all distinct  elements.\nSolution  #9.2\nIf all of the numbers  were positive,  then the maximum  product  of three numbers  is a matter  of finding\nthe largest three numbers  in the array and multiplying  them. Be sure to clarify with the interviewer\nwhat’s in the integer  array —- don’t just surmise  they are all positive  integers.  However,  with negative\nintegers,  the largest product  could arise if we take the two smallest  numbers  (both could be negative)\nand multiply  that result by the largest positive  number.  We'd need to compare  this potential  product  to\nthe number  involving  just the largest  three numbers.\nBy first sorting the array, you can get the largest three numbers  and the smallest two numbers.\nAlternatively,  we can use a heap (finding  the largest  three numbers  using a max-heap,  and the smallest\ntwo numbers  using a min-heap),  rather than sorting.  An implementation  involving  heaps is below:\nimport heapg\ndef max  _three(arr):\na = heapq.nlargest(3,  arr) # largest  3 numbers\nb = heapq.nsmallest(2,  arr) # smallest  2 (for negatives  case)\nreturn max(a[2]*a[1l]}]*a(0],  b{1]*b{0}*a[{0])  # compare\nThe time complexity  1s where  O(N) is the size of the input array and the space complexity  is O(1) for\nthe heap. since the number  of elements  is fixed.\nSolution  #9.3\nThe brute-force  solution  to finding  the k-closest  coordinates  to the origin would  be to iterate  over all\ncoordinates,  compute  the distance  from each coordinate  to the origin, and then sort by distance  to\nfind the A-closest  ones. This would yield a runtime  of O(N log N) because  of sorting  N coordinates.\nA more optimal way is to instead utilize a min-heap  whereby  we add coordinates  based on the\nEuclidean  distance.  Then, at the end, we can just take the first & elements,  which will already  be the\nsmallest  due to the min-heap:\nfrom heapgq import heappush,  heappop\ndef closest  (points,  k):\ndef get_dist(x,  y): # distance  helper function\nreturn x**2 + y**2\nmin heap = [] # heap\n208 Ace the Data Science  Interview | Coding"
  },
  {
    "page_number": 221,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nn = len(points)\nfor i in range(n):\nxX = points[i]  [0]\ny = points(ij][1]\nheappush  (min heap, (get _dist(x,  y), points[i]))  # add to heap\nres = [] | Oo -\nfor i in range({k):\n_ res.append(heappop  (min_heap)[1])  # get top k\nreturn  res\nThe time complexity  is O(N*log(X)),  since we have O(N) additions  to the heap, each of which takes\nO(log  K) time. The space complexity  is O(K) to accommodate  the min-heap.\nSolution  #9.4\nThe simple  approach  to finding  the 4-th smallest  element  is to iterate over all elements  in the matrix,\nadd each element  to a min-heap,  and then pop from the heap & times. However,  because  the matrix is\nsorted,  we know that the k-th smallest  element  must be within  row & and column  k. Therefore,  we can\nuse a double  for-loop  with k as the max value for each of the two indices.  Once all elements  are added,\nthen we can pop the top-é smallest  elements  from the stack and return the last element.\nfrom heapq import heappush,  heappop\ndef smallest  (m, k):\nn = len(m)\nheap = [] # heap\nres = -l\nfor i in range(min(k,  n)):\nfor 3} in range(min(k,  n)):\nheappush  (heap, m{i][j])  # add to heap\nfor in range(k):\nres = heappop(heap)  # pop k times\nreturn res\nWe will presume,  that, in general  K << N (although  the code handles  any event in which this is not\nthe case. Then the time complexity  is O(K“%2 log K) because  each heap push operation  takes O(log  K)\ntime, and this operation  is within a doubly-nested  for-loop.  The space complexity  is O(K) to support\nthe heap.\nSolution  #9.5\nA brute-force  way solution  to finding  the sum of the largest contiguous  subarray  is to compute  the\nsum over all possible  contiguous  subarrays,  and then return the biggest  sum found. This would have\na runtime  complexity  of O(N%2)  due to the doubly-nested  for-loops.  However,  there 1s no need to do\nmultiple  passes: in a single iteration  of the array, if we keep track of the current  sum and it ever goes\nbelow 0, there is no need to include  elements  in that previous  subarray,  and we can set the current\nsum to 0. Note that if the array 1s all negatives,  then we get a final result of 0 (by taking no elements).\nAce the Data Science  Interview 209"
  },
  {
    "page_number": 222,
    "content": "CHAPTER  9: CODING\nWhile doing the single iteration  through  the array, we keep track of the maximum  seen at every point\nand return that at the end:\ndef max subarray  (arr):\nn = len(arr)\nmax sum = arr{0] # max\ncurr sum = 0 # current  sum\nfor i in range(n):\ncurr sum += arr[i]\nmax sum = max(max_sum,  curr sum) # get max\nif curr sum < 0: # reset\ncurr sum = 0\nreturn  max sum\nThe time complexity  is O(N), where  N is the size of the input array, and the space complexity  1s O(1).\nSolution  #9.6\nFinding  out if a tree is a mirror image of itself  can be solved  recursively  in a straightforward  manner.\nWe will use the TreeNode  class implementation  below. The helper function  checks that for two\nsubtrees:\na) The root values  match\nb) The first subtree’s  left side is a mirror  of the second  subtree’s  right side\nc) The first subtree’s  right side ts a mirror  of the second  subtree’s  left side\nThis helper  function  is then called on the root’s left and right subtrees:\nClass TreeNode:\nGef init (self, val):\nself.val  = val\nself.left  = None\nself.right  = None\ndef is mirror(root):\nif root is None:\nreturn  True\nreturn helper(root.left,  root.right)\ndef helper(x,  y):\nif (x is None and y is None):\nreturn  True\nelif (x is None or y is None):\nreturn False\nreturn x.val == y.val and\nhelper(x.left,  y.right)  and\nheiver(x.rignt,  y. left)\n210 Ace the Data Science  Interview  | Coding"
  },
  {
    "page_number": 223,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nThe time complexity  and space complexity  are both O(N), where  N is the size of the tree since the\nentire tree is traversed  once.\nSolution  #9.7\nThe brute-force  way to check for peak elements  is to iterate through  the array and check, for each\nelement,  whether  the left and right neighbor  is less than the element's  value. That would  yield an O(N)\nruntime  for one linear  scan. However,  we can improve  upon this by doing a modified  binary  search.\nTake the middle  value and check the number  to the left of it (say it is smaller).  If the middle  value’s\nright element  is also smaller,  then the middle  value is a local peak. Alternatively,  if the right value\nis greater  than the middle value, then there must be a local peak on the right-hand  side (numbers\nascending  and then a descent,  or it keeps ascending  till it reaches  the end of the array, which is also a\nlocal peak). We do this in an iterative  manner  until we reach the condition  that the element’s  value is\nlarger  than both its left and right neighbor  values.\ndef get_peak(arr):\nstart = 0\nend = len(arr)  - 1\nwhile True:\nmid = (start + end) // 2 # get midddle element and check bounds\nleft = arr[mid-1]  if mid - 1 >= 0 else float(‘-inf’)\nright = arr[{midt+l}  if mid + 1 < len(arr)  else float  (‘-inf’  )\nif left < arr{mid]  and right < arr[{mid}:  # left < mid.> right\nreturn  mid\nelif arr{mid]  < right: # change bounds\nstart = mid + 1\nelse:\nend = mid - 1\nThe time complexity  is O(log(/V))  since it is a variant  of binary search and the space complexity  of\nO(1).\nSolution  #9.8\nCov(X,Y)\n6, *O,We know that correlation  is given by: p, , =\nwhere the numerator  is the covariance  of X and Y, and the denominator  is the product  of the standard\ndeviation  of X and the standard  deviation  of Y. Recall the definition  of covariance:\nCov(X, Y) = E[(X-  p,)(Y-  py)]\nTherefore,  a simple implementation  ts to have helper functions  for calculating  both the mean and\nstandard  deviation.  Note that calculating  both the mean and standard  deviation  1s O(V) runtime  (since\nboth take a single sum across all NV elements).  Therefore,  the correlation  runtime  is O(N), since we\ncalculate  a few means and standard  deviations,  as well as iterate over all V elements  once through.\nThe space complexity  1s O(N) to keep track of N elements  in the correlation  to be processed.\nAce the Data Science  Interview 211"
  },
  {
    "page_number": 224,
    "content": "CHAPTER  9: CODING\nimport math\ndef mean(x):\nreturn sum(x) /len(x)\ndef sd(x):\nm = mean (x)\nSS = sum((i-m)  ** 2 for i in x)\nreturn math.sqrt(ss  / len(x))\ndef corr(x, y):\nx m = mean (x)\ny_m\nxy d = {] # product of de-meaned  X and Y, for covariance  calcmean (y)\nfor i in range(len(x)):\nx d = x{i] - xm\ny a= ylil - ym\nxy d.append(x  d * yd) # add product  of X_i and Y_i\nreturn mean(xy  d) / (sd(x) * sd(y)) # from formula  above\nSolution  #9.9\nFinding  the diameter  of a tree can be solved recursively,  since at any given root node, the diameter  1s\njust one more than the maximum  of the depths  from tts left and right subtrees.  Depth, in this context,\nis defined  as the number  of nodes from the root to the farthest leaf node. We can utilize a helper\nfunction  called depth()  to calculate  tree depth, and have its base case to return 0 if the root is None.\nInside the diameter  function,  we want to recursively  call depth(root.left)  and depth(root.right)  and\nreturn | + max(left  result, right result).\nclass TreeNode:\naef init (self, val):\nself.val  = val\nself.left  = None\nself.right  = None\ndef calc diameter  (root):\ndef depth(root,  diameter):  # helper\nif root is None:\nreturn 0, diameter  # base case\nleft, diameter  = depth(root.left,  diameter)\nright, diameter  = depth(root.right,  diameter)\ndiameter  = max(diameter,  left + right) # update diameter\nreturn  max(left,  right) + 1, diameter  # return  root’s longest  path\ndepth, diameter  = depth(root,  0) # call helper\nreturn diameter\n212 Ace the Data Science  Interview  | Coding"
  },
  {
    "page_number": 225,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nThe time complexity  is O(N), where  N is the size of the tree, since the entire tree is traversed  once.The space  complexity  is also O(N), since there will be N recursive  calls on the stack in the worst case.\nSolution  #9.10\nTo generate  the numbers  at random,  we can start by calculating  the mean and defining  upper and\nlower bounds on any given integer  based on the standard  deviation.  Let the target be represented\nby T, the number  of elements  by N, and the sigma by o. The mean  p will be 7/N, and the allowable\nStandard  deviation  is given by o * yp. That means  any given element  must be within  o * u of », which\nestablishes  our lower  and upper  bounds.\nOnce we have the upper and lower bounds,  we can take all n integers,  starting  each at the lower\nbound, and increment  them at random  in an iterative  manner  until the sum of the numbers  is the\ngiven target value. For random  incrementing,  we can randomly  sample  one of the NV elements  and\nincrement  it by 1 if it is not beyond  the upper  bound. If it is, we skip it for that iteration.  An example\nimplementation  is as follows:\nimport random\ndef generate  nums(target,  n, sigma):\nmean = target /n\nsd = int(sigma  * mean)\nmax val, min_val  = mean + sd, mean - sd # bounds\nresults  = [min val] * n\nremaining  = target - n * min val\nwhile remaining  > 0:\na = random.randint(0,  n-1) # choose random  index\nif results{a]  >= max val: # skip if above bound\ncontinue\nresults[a}]  += min(remaining,  1)\nremaining  -= 1\nreturn results\nAfter creating  the initial results array of » elements  that are each min_val,  let R be the remaining\namount  results  need to be incremented  to hit the target sum.\nThe while loop has a time complexity  of O(R), which is a function  of O(7*N)  for the following\nreasons:\nR=T-n(u-o==  |\nn\nbased on the above formulation.  Simplifying  yields: R = T — n(y) +6 * T\nRecall that by definition  7 ~ n * therefore,  we can simplify  the above to be:\nT\\+o*T-o*TR= Tol\nn\nThus, the runtime  1s a function  of O(max(o  * 7), N) since the bottleneck  is either the while loop, or\nin generating  the results of size N initially.  The space complexity  will be O(/) to store the results.\nAce the Data Science  Interview 213"
  },
  {
    "page_number": 226,
    "content": "CHAPTER  9 : CODING\nSolution  #9.11\nTo find the shortest distance in a graph between two nodes (in this case, the number of friends\nbetween two individuals),  we can use a breadth-first-search  (BFS) and maintain  a list of distances\nfrom one of the input users. Each distance in this list starts at 0. In typical BFS fashion, we utilize a\nqueue and add the initial node to the queue, processing  the queue in an iterative fashion as long as it\nis not empty. At every step, for any friend node we see, we update the distance value for that friend\nnode. Since the distance array will track all of the distances from the starting user x, we return the\ndistance  of the other user y:\nimport queue\ndef friendship  distance(n,  edges, x, y):\ndistance  = [0] * n # distances  from x\nprocessed  = [False] * n # visited  or not already\nq = queue.  Queue  ()\nq-put (x)\nprocessed{x]  = True\nwhile (not q.empty()):\ncurr = q.get() # current  node\nif curr not in edges:\ncontinue\nfor neighbor  in range (len(edges[curr])):\nif not processed[edges[curr]  [neighbor]]:  # process if not visited\ndistance  [edges[curr]  [neighbor]]  = distance[curr]  + 1\ng.put (edges[curr]  [neighbor])  # add neighbor\nprocessed[edges[curr]  {neighbor]]  = True\nreturn distance[y]\nThe time complexity  is O(N*M),  where N is the number  of users, and M is the number  of friendships.\nThe space complexity  is O(N) to maintain  the queue.\nSolution  #9.12\nThe brute-force  way to find all the anagrams  of B within A ts to find all substrings  of A and then\ncheck if each is an anagram  of B. This is inefficient  because  it leads to an O(N“2*K)  runtime  since\nthere are V2 substrings  of A, and the anagram  check takes O(K) time.\nA better way is to have a rolling  window  approach:  say the length of B is 4. We can iterate over A and\ncheck a window  of size & and compare  that window  to B to check for an anagram  match.\nA basic way to confirm  if two strings  are anagrams  is to check if the sorted versions  of the two strings\nare equal. However,  that 1s O(N log N) due to sorting. To speed this up to an O(N) check, we can\nulilize two dictionaries.  For each window  in A, keep a dictionary  of character  counts,  and compare\nit to B’s dictionary  (for an O(N) comparison  rather than an O(N log N) via sorting).  After setting up\neach initial dictionary  for A and B (via utilizing  helper function  below for adding),  we can then run\nthrough  all valid windows  and check at cach step whether  the two dictionaries  have valid anagrams\n(and, if so, add to result). We can check for anagrams,  and then keep sliding  the window  within  A.\n214 Ace the Data Science  Interview  | Coding"
  },
  {
    "page_number": 227,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\ndef total_anagrams  (A, B):\nn, .k = len(A), len(B)\nlf n < k:\nreturn {[]\ndef is  anagram(d_a,  db):\nfor x in db:if x not in da: # key doesn’t exist\nreturn  False\nif d  b[{x]) != d_a{xJ:  # count doesn’t  match\nreturn False\nreturn  True\nGef add(char,  a): # tracking  char freq\nif char in d:\na{char] += 1\nelse:\nd[{char] = 1\nreturn  a\nda, db= {}, {}\nStart, res = 0, []\nfor i in range(k):\nd a= add(A[i},  da) # set up dictionary  for A\ndb add(Bi[i],  ad_b) # set up dictionary  for Bif\nfor i in range(k,  n+l):\nif is_anagram(d_a,  db):\nres.append  (start)\nif i<on:\nda = add(Ali]J,  da) # add next char of window to dict\nad afA[start]]  -= 1 # remove leftmost  char of window from dict\nStart += 1\nreturn res\nThe time complexity  1s O(NA),  since the anagram  comparison  ts O(K) and there are O(V) windows\n(assuming  N >> K). The space complexity  1s O(N) to support  the resulting  list and dictionaries.\nSolution  #9.13\nHow do we know when two intervals  overlap?  First, sort all intervals  by start time, and then by end\ntime. Two intervals  overlap  when the ending time of an interval is larger than the start time of the\nnext interval.  Therefore,  we want to iterate over the intervals  and increment  a count whenever  two\nintervals  overlap.\nTo keep track of the current  and the previous  interval,  we create two pointers  called low and high.\nLet low point to the interval with the smaller  start time. At each iteration,  we can detect when the\nAce the Data Science  Interview 215"
  },
  {
    "page_number": 228,
    "content": "CHAPTER  9 : CODING\nintervals overlap by checking  the low pointer of the next interval versus the higher pointer of the\ncurrent  one.\nIn case intervals need to be merged (when one interval is completely  within another),  we set the\nprevious  low pointer value to be the current high pointer value. In this way, no intervals  are deleted,\nbut in the next iteration  we need to compare  against the current high interval’s  begin and end time:\ndef interval  removal  (interval_list):\nif len(interval  list) == 0:\nreturn 0\nintervals  = sorted(interval  list, key=lambda  k: (k{0], k{1])) # sort\nres, low, count = 0, 0, 0\nfor high in range(1,  len(intervals)):\nif intervals[low][1]  > intervals[high]  [0]: # overlap\ncount += 1\nif not intervals[high][0]  < intervals[low][1]  < intervals[high}  [1]:\nlow = high # merge\nreturn count\nThe time complexity  is O(N) since there is just one for-loop.  The space complexity  1s O(1) since there\nis no extra space being used.\nSolution  #9.14\nWe can use a helper function  to identify  anagrams,  and then identify  the strings that have the same\ncomposition.  For the helper function  that identifies  anagrams,  we can create a character-frequency\nmap. As an example.  for “abc” we can have the following  dictionary:  {“a”: 1, “b’: 1, “c”?: 1}. To\ncompare  anagram  compositions  among  different  strings,  we will need to uniquely  identify  the maps:\nfor example,  if for “abc” we have the map: {‘‘a”: 1, “b”: 1, “c”: 1} and for “cab” we have: {“‘c”: 1,\n“ar: 1, “b”: 1} then we want to make sure both correspond  to the same anagram  grouping.  To do\nthis, we can sort each dictionary  in <key, value>  format  and use the string representation.  Therefore,\nwe simply  keep a final dictionary  whereby  that anagram  string 1s the key, and the value is the list of\nstrings that fall under that anagram  grouping:5 a\ndef anagram  _group(str  list):\ncfd_str_list  = {} # {anagram_composition  string  : {strings  of common  anagram}  }\nfor s in str_  list:\ncfd = {} # char freq dict of s {character  : frequency}\nfor c in s:\nif c in cfd:\ncfd[c}] += i\nelse:\ncfd[c] = 1\n# got sorted char freq dict of s\ncfd = dict (sorted(cfd.items()))\n# flatten  cfd to cfd str as single string\n216 Ace the Data Science  Interview | Coding"
  },
  {
    "page_number": 229,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\ncfd _ str = ‘’ .join(*{}{}’.format(k,  v).\nfor k, v in cfd.items())  # format\n# anagrams  will produce  identical  cfd str\nif cfd_str  not in cfd_str  list:\ncfd_str_list{cfd_  str] = [s] # add anagram\nelse:\ncfd_str  list{cfd_str].append(s)  # not existing  anagram  yet\nres = []\nfor cfd_str  in cfd_str  list:\nres.append(cfd_str_list[cfd_str])\nreturn res\nThe time complexity  is O(NK(log  K)), where N is the length of the string list input and K is the\nmaximum  length of any given string within the list, since the outer for-loop  will take O(N) time for\neach string, the helper function  takes O(K) time, and sorting the dictionary  takes O(K log K). The\nSpace complexity  is O(NK)  since there are at most N results  of size K.\nSolution  #9.15\nCounting  the friend groups is just a variation  of the classic graph theory problem  of finding all\ndisconnected  subgraphs  within a graph. At a high level, we can use depth-first-search  (DFS) to find\nall the nodes in a friend group.\nIn particular,  we can start with any person and run DFS recursively  until there are no more direct\nfriends  to any of the covered  individuals  in the friend group. By the end of this process,  any indirect\nfriend to the initial person is also included  in the group. Note that each person will only be in one\nfriend group (if A were in one, and directly  or indirectly  related to B, then there is no way B is ina\ndifferent  friend group). Therefore,  we do this until all people have been accounted  for, and simply\nincrement  the number  of friend groups found whenever  we need to run a new DFS for a friend group.\nThe algorithm  implementation  is below:\ndef dfs(friends,  1, N):\nfriends.add(i)\nfor j in range(len(N[{i])):  # checks all neighbors  of existing  person\nif N(iJ({j] == 1 and j not in friends:\ndfs(friends,  3, N)\ndef count groups  (N):\nnum = len(N[Q]))\ngroups = 0\nfriends = set() # tracks friend groups\nfor i in range(num):\nif i not in friends: # get all related groups that i is in\ndfs(friends,  i, N)\ngroups += 1\nreturn groups\nAce the Data Science  Interview 217"
  },
  {
    "page_number": 230,
    "content": "CHAPTER  9: CODING\nThe space complexity  is O(N) since there are NV elements in the set, and the time complexity  1s\nO(N*2)  since there are O(N2) edges to traverse  in the adjacency  matrix.\nSolution  #9.16\nTo remove the k-th element from the back, we can create a count helper function  to get the ordering\nof the nodes, and then take the k-th last node and make sure the previous  node’s next value is the node\nk + 1. Along the way, we need to address some corner cases (for example,  if & is the last element in\nthe linked  list).\nWe want to avoid recalculating  node counts, so we cache the ordering  of the nodes in a dictionary.\nThus, our count() helper function  below will store the ordering  of the nodes, with the order number\nas the key. Then, when looking for nodes k + 1 and & ~ | for editing the linked list, we can query the\ndictionary  for those two nodes. Finally,  we return the head:\nclass Node:\ndef init _ (self, val):\nself.val  = val\nself.next  = None\ndef remove  (head, k):\ndef count(node,  d): # helper function\nif node.next  is None:\na{1] = node # tail\nreturn 1\nres = count (node.next,  d) # recursive  call\nd[l+res] = node # store res in dictionary\nreturn 1 + res\nd= {}\nnm = count(head,  dad) # get counts\nif k == len({d): # tail case\nif len(d) ==\nreturn None\nhead.next  = None\nreturn d[k-1]\nprev_node  = da[k+l] # get prev node\nif k == 1:\nnext node = None\nelse:\nnext node d{k-1] # get next node\nprev node.next  = next node # set next node\nreturn head\nThe trme complexity  is O(N) since the count()  function  will visit every linked list node once, and the\nremaining  operations  are constant  time. The space complexity  1s O(N) to accommodate  the dictionary\nthat contains  all the input nodes.\n218 Ace the Data Science  Interview  | Coding"
  },
  {
    "page_number": 231,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nSolution  #9.17\nTo estimate x, start by imagining  a unit Square and choosing  a random pair of coordinates\n(x, y) inside it. Within  the unit square,  we can also form a quarter  of a circle.\n1\n) 1\nFor reference,  the equation  for a circle is given by: x? + y? = °\nTherefore,  we can sample random (x, y) pairs and verify if it falls within the unit circle\n(if.x° + y* <= 1). Then, we can count the resulting  number  of points inside the circle; when divided  by\nthe total number  of points,  this yields the proportion  of points within the quarter  circle:\nx\n4\nTherefore,  x should  be 4 times the resulting  proportion  of points.  The time complexity  is O(N), where\nN is the number  of iterations,  due to the for-loop.  The space complexity  is O(1) since only a fixed\nnumber  of values  is looked  at per iteration.Le =\n4\nimport  random\nimport math\ncount = 0 # num points inside quarter  circle\nn = 10000000  # iterations\nfor i in range(0,  n):\nx Sq = random.  random()**2\ny_sq = random.random()**2\nif math.sqrt(x  sq + y sq) < 1.0: # check if inside circle\ncount += 1]\npi = (float(count)  / n) * 4 # accounts  for quarter  circle\nreturn pi\nSolution  #9.18\nNote that an invalid  string has more left or right parentheses  than its respective  counterpart.  To keep\ntrack of parentheses,  we can use a stack and push to it when encountering  a left parenthesis,  and pop\nfrom it when encountering  a right parenthesis.  Note that for a valid string, every right parentheses\nmust be matched  by the latest (according  to the stack) left parentheses.  Therefore,  when we encounter\na right parentheses,  we must set the valid left and right parentheses  accordingly.  If we iterate over\na letter, then we use it in the result. A temporary  array can be used to track the valid letters and\nparentheses:\nAce the Data Science  Interview 219"
  },
  {
    "page_number": 232,
    "content": "CHAPTER  9 : CODING\ndef splitParen(s):\nstk = [] # stack\nres = {*‘’] * len(s) # chars to return\nfor index, val in enumerate(s):  # index and value of each char\nif val == ‘(*:\nstk.append(index)  # push index of left paren\nelif val == ‘)’:\n1£ stk:\nlatest = stk.pop()  # pop the latest corresponding  left paren\nres(latest]  = s[latest]  # or ‘(‘*\nres{index]  = val # or ‘)’\nelse:\nres[index])  = val\nreturn ‘’.join(res)\nThe space complexity  is O(N) due to the temporary  array, and the time complexity  is O(N) since there\nis one scan through  all of the characters  in the string.\nSolution  79.19\nWe can generate  permutations  in a recursive  manner  as follows:  start with an empty array of results.\nFor the base case, note that if the length  of the input is 1, we just return an array of that number.  Then,\nfor every clement  in the input array, we make a recursive  call to the subarray  with all elements  except\nthe current element.  Remember  that each recursive  call returns a list of permutations.  Therefore,\nwhile iterating  over all elements,  in each recursive  call take the results  and add the element  to each\nlist that is returned.  At the end we return that array of results:\ndef permute(nums):\nn = len(nums)\nres = [{] # store results\nif n <= i:\nreturn [nums]\nelse:\nfor iin range(n):  # this is the element  E\n# recurse  on previous  combos\nfor combo in permute(nums[:i]  + nums[itl:]):\nres.append({nums[i]]  + combo)\nreturn  res\nThe time complexity  is O(N!) since there are M! permutations  being generated  in the recursive  call,\nand the space complexity  is O(N*M!)  since each of the N! calls uses O(N) space.\nSolution  #9.20\nLet .V be the sum of the weights.  In the example,  N would be 5 + 10 + 15 + 20 = 50. A basic way to\nsample  from a fixed number  of categories  would  be to generate  a list of size N and create  list elements\nproportional  to their weights  as follows  (using  the example):\n220 Ace the Data Science  Interview | Coding"
  },
  {
    "page_number": 233,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nimport random !\nw_a, w_b, wic, wd= 5, 10, 15, 20\n1 = [‘A’] * wa + [*B’] * wb + ['C’] * wo t+ ['D']  * wa\nreturn (random. choice (1) )\nHowever,  this solution  is not optimal  because  the space usage here is O(N) since we store the number\nof elements  according  to sum of the weights.  If we wanted  to do this more generally,  keeping  space\nusage in mind, we can do the following:\n1. Calculate  the cumulative  sum of weights.\n2. Choose  a random  number  k between  0 and the sum of weights.\n3. Assign  k the corresponding  category  where  the cumulative  sum is above k.\nIn the example  below, we use the weights  [5, 10, 15, 20] to create  a list of cumulative  sums: [5, 15,\n30, 50]. Then, we choose a random  number  between  | and the total weight (50, in this case) and\nuse a modified  binary search to look for that number  in the list of cumulative  sums. That modified\nbinary search will return the closest corresponding  index to the random  number,  and we can return\nthe category  label associated  with that index.\ndef binary  search(a,  k):\nlo, hi = 0, len(a) - 1\nbest = lo\nwhile lo <= hi:\nmid = lo + (hi - lo) // 2\nif a{midj} < k:\nlo = mid + 1\nelif a[{mid] > k:\nhi = mid - 1\n“\nelse:\nbest = mid\nbreak\nif a{mid}] - k > 0:\nbest = mid\nreturn best\nimport random\ncategories  = [‘A’, ‘B’, ‘C’, ‘*D‘]\nweights = [5, 10, 15, 20] # list of weights\n# cumulative  sum [5, 15, 30, 50}\ncum sum = [sum(weights[:i])  for i in range(1, len(weights)  + 1)]\nk = random.  randrange  (cum sum{-1]) # choose randomly in range (0, total sum)\ni = binary search(cum_sum,  k) # binary search for k\nreturn categories[i]\nAce the Data Science  Interview 221"
  },
  {
    "page_number": 234,
    "content": "CHAPTER  9 : CODING\nIf there are K categories  and the total sum of weights is V, where N >> K, then this method uses O(K)\nspace and has O(K) runtime (since the binary search part is O(log(K))  < O(K)), whereas the first\nmethod uses O(N) space and O(N) time to create the full list.\nSolution  #9.21\nThe brute-force  solution  to find the maximum  common  subarray  would be to iterate over all possible\nsubarrays for each input and compare the subarrays to verify if they match. However, this takes\nO(N*5)  time, since there are N“2 subarrays  for both arrays, and there is an O(N) check to compare  any\ntwo subarrays.  It is inefficient  because it doesn’t exploit the overlapping  subproblems.  Specifically,\nonce you have a smaller common array, you can see if the next character  between the two arrays 1s\nthe same, which extends  that common  subarray  by one.\nDue to these overlapping  subproblems,  we can utilize dynamic  programming:  say that dp[i][j] denotes\nthe length of the longest common  subarray  for a[:1] and bf:  j), where a and b are the two arrays. If the\ni-th character  of a and j-th character  of b match (a[i-!] = b{j-1]), then we can extend the common\nsubarray  by one (dp[ij{j] = 1 + dp[i-1][j-1]).  Therefore,  we can iterate over all relevant  (i, j), where i\nranges from | to the length of a, and / ranges from | to the length of b, and update dp[iJ[j| within each\niteration  to keep track of the maximum  value seen:\ndef longest  common(a,  b):\nm = len(a)\nn = len(b)\ndp = ({0 for i in range(nt+1)]  for j in range(m+1)]  # setup dp\nmax val = 0\nfor iin range(1l,  mtl1):\nfor j} in range(li,  ntl):\nif afi-l) == b[j-1]:\ndp(il{j] = 1 + dpfi-1l][j-1]  # update dp{i] (j]\nmax val = max(max_val,  dp[i][j])  # keep track of max\nreturn max val\nThe time complexity  is O(MN),  where M is the length of a and N is the length of b since, in the worst\ncase, we will fill out every cell accordingly  in a bottom-up  manner.  The space complexity  is O(MN)\nas well, to accommodate  the 2d-array  of cached  results.\nSolution  #9.22\nThe brute-force  method to find the maximum  increasing  subsequence  sum (MISS) would be to\nexamine  all possible  subsequences  (checking  if it’s increasing)  and then calculate  the sums of each to\ncompare.  Since there are V“2 possible  subsequences  (each of which takes O(N) time to sum/verify  it’s\nincreasing),  this brute-force  method  takes O(N%3)  to calculate  the MISS. However,  this ts inefficient\nbecause  tt doesn’t  take advantage  of the overlapping  subproblems:  if you have an existing  best MISS\nso far, and if we encounter  a new element  larger than the current  maximum  of the MISS, we can extend\nthat MISS by that element.  As such, we can use dynamic  programming  to store previous  MISS values.\nTo be more precise,  say we have an array tracking  the max increasing  subsequence  sum (MISS)  up to\nindex / of the array. We can solve for that index by looking  over all previous  indices  / (each with the\n222 Ace the Data Science  Interview  | Coding"
  },
  {
    "page_number": 235,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nMISS up to index / < i). We know that we have a new MISS if the current  element  and the previous\nMISS sum up to more than the current  MISS.\ndef max_subseq  sum(arr):\nn = len(arr).\nres = [0 for x in range(n)]  # store results\nfor a in range(n):\nres[{a] = arr{[a}\nfor i in range(l,  n):\n- for 3} in range(i):\nif arr{j] < arr{i] and res{i] < res{j] + arr[{i]: # extend the MISS\nres[i]  = res({j] + arr[(i] # add incremental  element\nreturn  max (res)\nThe time complexity  is O(N“2)  due to the doubly-nested  for-loops,  and the space complexity  is O(N)\nto store the array of MISS results.\nSolution  #9.23\nAt first, finding  the smallest  number  of perfect  squares  that sum up to ” may look like an application\nof a greedy  algorithm,  where you try to find the biggest  square  less than the value. However,  consider\n4*°2 + 5%2 = 16 + 25 = 41 as an example.  If you used 6%2 = 36, you'd get a non-optimal  result of\n36+ 4+ 1 rather  than 25 + 16.\nAs such, to get the correct answer  for 41, check the smallest  number  of perfect squares  for 41-1,\n41-4, 41-9, etc. for all squares  up to 41. There are overlapping  subproblems,  so we can use dynamic\nprogramming  to store sub-results.  Let res{1] represent  the smallest  number  of perfect  squares  that sum\nto n. Our recursive  step is:\nres(i) = min(res(i),  res(i-j*2)  + 1)\nwhere  / is an integer  less than or equal to the square root of i. This is because  either the current  value\nis minimal,  or we can subtract  off a square and yield a smaller  number  of squares to add up to the\nremaining  value. We can solve the subproblems  in a bottom-up  manner:\ndef square  count (n):\nres = [x for x in range(n+l)]  # store results\nfor iin range(2,  n+l):\nfor j in range(l, int(i ** 0.5) + 1):\nres[(i] = min(res[i],  res{i - 3 ** 2] + 1)\nreturn res([n}\nThe time complexity  is O(N*2)  due to the doubly-nested  tor-loops,  and the space complexity  is O(N)\nto store the array of results.\nAce the Data Science  Interview 223"
  },
  {
    "page_number": 236,
    "content": "CHAPTER  9 : CODING\nSolution  #9.24\nTo find all combinations,  we can use a method called backtracking,  which builds upon a solution\nset according  to some constraints  and abandons  paths that cannot lead to a valid solution. To get all\ncombinations  with k elements, first, pick an initial element from the set of existing numbers, then\nconcatenate  that element with all other possible combinations  with k-1 elements produced so far,\nwhich occurs in a recursive  call.\nFor instance,  assume we have a backtrack()  helper function  that takes in the following  parameters:  7,\nk, res (which is the resulting list of lists that we can return eventually),  combo (any list of elements\nthat is a candidate  combination),  num (the number of elements in the list) and start (where we start\nwithin a list in generating  combinations).\nThen the logic is as follows: anytime num is equal to k, append the combo list to res. If start is\nbeyond 7 or num is beyond k, we can return early. Otherwise,  going from start until n, we can do the\nfollowing:\n1. Add the current  element  to combo.\n2. Recursively  call backtrack()  on the existing version of a combo with updated mum and start\nparameters.\n3. Remove  that element  from combo.\nStep #3 is essential,  because  further combinations  generated  do not always include that element  from\nStep #1. In the end, we return the result we obtain from calling backtrack():\ndef combos(n,  k):\ndef backtrack(n,  k, res, combo, num, start):\nif num ==\nres.append(list(combo))  # add to result\nif start > n or num >= k:\nreturn\nfor iin range(start,  n+l): # iterate over every element\ncombo.append  (1)\nbacktrack(n,  k, res, combo, numtli, i+l) # recurse\ncombo.  remove (1)\nres = []\nbacktrack(n,  x, res, [|], 90, 1)\nreturn res\nThe time complexity  is OC\"), and there is no way to circumvent  it since that is the number  of returned\ncombinations.  This also holds true for the space complexity.\nSolution  #9.25\nThe brute-force  way 1s to check every possible  substring  and see if it’s well-formed.  A more optimal\nway is to incrementally  grow the substring,  and use a stack to make sure parentheses  are balanced\nand the string 1s well formed. For every left parenthesis,  push its index within the string onto the\nstack. For every right parenthesis,  pop the topmost  element  and then calculate  the length based on\nthe current index versus the index of the first leftmost  parentheses  of the current  valid substring  (the\n224 Ace the Data Science  Interview  | Coding"
  },
  {
    "page_number": 237,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\ntop element  of the stack).  Hence,  the difference  between  the current  index and that top element  in the\nStack is potentially  the longest  length  of a well-formed  string seen so far. In the case where the stack\n1s empty after the pop, we can add the current index to the stack. This occurs when you have right\nparentheses  before  left parentheses.\nTo address  the case where  the first character  is a right parentheses,  we can add an initial dummy  value\nonto the stack. This dummy  value needs to be —1 for the following  reason:  we would  give the correct\nlength of the substring  when popping  the first ‘( at matching  the corresponding  ‘)’ at stack{i]  as i+\n1, only if the initial dummy  value is -1.\ndef longest  parens(s):\nStack = []\nlongest = 0\nhn = len(s)\nstack.append(-1)  # initial  dummy value\nfor iin range(n):\nif s[i] == *(*:\nStack.append(i)  # append  index\nelse:\nstack.pop()\nif len(stack)  == 0:\nstack.append  (i)\nelse:\nlongest  = max(longest,  i - stack[-1])  # get length\nreturn longest\nThe time complexity  and space complexity  are both O(N), since the stack goes through  every element\none time.\nSolution  #9.26\nTo find the length of the longest  path of increasing  integers,  first, consider  a single element  in the\nmatrix. Presume  that we have found the longest  path of increasing  integers  for all other paths not\ninvolving  that number.  When evaluating  the element,  we know that the length  of the new longest  path\nmust be | plus the maximum  length of the longest  paths of its four neighbors.  To be specific,  for any\ngiven indices (i, /), there are up to 4 potential  neighboring  indices,  of which the longest increasing\npath from  (i, /) 1s | plus the max of the longest  increasing  path from its four neighbors,  if the element\nis larger than its neighbor.\nThis structure  lends itself well to dynamic  programming,  and as such, we can keep track of a max\nlength and a table (to serve as a cache) with indices (/, /), which tracks the length of the longest\nincreasing  path startling  at index 7, / of the matrix. To calculate  this max, we can utilize depth-first-\nsearch (DFS), where we recurse  on the 4 neighboring  elements.  For each DFS, we also need to keep\ntrack of a previous  max element  (since the path needs to be increasing  — so 1f the current value at /,\njis less than that value, we can just return 0). To start the algorithm,  we can instantiate  the max to be\n-o and start with an empty dictionary  for the table cache, and then run DFS for each  /, /:\nAce the Data Science  Interview 225"
  },
  {
    "page_number": 238,
    "content": "CHAPTER  9 : CODING\ndef longest  increasing  _path (mx) :\ntable = {} # cache\nif len(mx) == 0:\nreturn 0\nm = len (mx)\nn = len(mx[(0])\ndef dfs(i, j, prev, table): # DFS helper\n# edge case\nif(i < 0 or i >= len(mx) or j < 0 or j >= len(mx[({0])  or mx{i}[j] <= prev):\nreturn 0\nif (i, 3) in table:\nreturn table[(i,  3)] # get cached value\ncurr len = 1 + max(dfs(i-l,  jy, mx[ij]{j],  table),\ndfs(itl, j, mx[iJ(j],  table),\ndfs(i, j-1l, mx{iJ[j],  table),\ndfs(i, j+l, mx(iJij3],  table))\ntable{(i,  j3)] = curr_len  # get max\nreturn curr len\nfor i in range(m):  # call DFS from each i,j\nfor j in range(n):\ndfs(i, j, -float(“inf”),  table) # set initial  max and table\nreturn max (table.values())\nThe time complexity  is O(N), since in the worst case, the DFS will visit every cell within the\nmatrix.  The space complexity  is O(MN)  as well to accommodate  the table cache.\nSolution  #9.27\nThe brute force method  to find consecutive  integers  that sum to 7 is to start creating  lists from 1 to 7,\nand keep adding  consecutive  integers  to each list until they equal 7 or go over. However,  this solution\nhas a runtime  of O(N“2)  and is inefficient.  Here is that code:\ndef consecutive  sum(n):\nnum = O\nfor i in range(l,  ntl):\ntotal = 0\nwhile total < n:\ntotal = total + i\ni += 1\nif total - nn:\nnum += ]\nreturn num\n226 Ace the Data Science  Interview | Coding"
  },
  {
    "page_number": 239,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nDue to the structure  of this problem,  we can look for some properties  that valid solutions  follow,  to\nmore intelligently  create candidate  lists rather than brute-force  generating  all of them. Let’s reframe\nthe problem  in a mathematical  way. Say we start at some number  k, and go up to some number  k + m,\nSo we have a sequence  of m + 1 terms. Note that:\nk+(k+ 1) tthe m= 1= (2k m2)\nNow, if we set this to 7, we have the following:  (2k + m nS t) =n\nanSolving  for k in terms of m and n, we get: 2k = —m\nSince & needs to be an integer  for a valid solution,  we can check whether  the right-hand  side satisfies\ntwo conditions:\n1. 2nis divisible  by m + 1 (otherwise  the right-hand  side is not an integer).\n2. The nght-hand  side is divisible  by 2 (..e., it is an even number,  since 24 is even).\nNote that for the right-hand  side we have: a\nm +\nWe do not need to check all values of m since we need k to be a positive  integer.  Thus, to find the\nbounds  on m, we take the above expression  and find when it is greater  than 0:\n2n —m>O\nmt+1\n2n>m\nm+1\n2n>m(m+1)=m*+m\nSince m? + m > m?, we have:\n2n> m?\nV2n>m\nAs such, we only need to iterate m from 1 to V2n:\nimport math\ndef consecutive  _sum(n):\nupper limit = int (math.  sqrt (2*n) )\nnum = 0\nfor m in range(upper  limit):\nif (2*n) % (m+1) == 0 and (2*n/(m+1)-m)  % 2 == 0:\nnum += 1\nreturn  num\nThe runtime  is O(N), and the space complexity  is O(1).\nSolution  #9.28\nThe brute-force  way to find the median  of a continuous  stream of elements  is to store all seen elements\nin an array and then find the median there. However,  this is inefficient  in the find operation  - - it Js\ngenerally  O(N log N), since a sort is needed.\nAce the Data Science  Interview 22/7"
  },
  {
    "page_number": 240,
    "content": "CHAPTER  9: CODING\nA more efficient  way is to keep track of two heaps: a max-heap  to store the smaller  half of numbers,  and\na min-heap  to store the larger half of numbers.  The intuition  is that the median will always be either the\nlargest value of the max-heap,  the smallest  value of the min-heap,  or the average of the two (try some\nexamples  out on paper). The question,  then, is how to maintain  the balance, since this approach  only\nworks if both elements  contain about half the elements  (otherwise,  the median would be deep inside\none of the two heaps if the sizes were skewed).  To handle this, simply check the heap size when adding\nelements. If the size of one heap is larger than the other, add an element from one heap to another to\nbalance  it out.\nBelow, we use the heapgq library for a priority queue implementation  and use the negatives  of values\n(since by default the priority queue implementation  in Python weights elements  by their value from\nsmallest  to largest):\nimport heapq\nclass MedianFinder  (object):\ndef init (self):\nself.min  heap = []\nself.max  heap = []\ndef add  num(self,  num):\nheapq.heappush(self.max  heap, -num) # add negative  for max heap\nheapq.heappush(self.min  heap, -heapq.heappop(self.max_  heap)  )\nif(len(self.min  heap) > len(self.max_heap)):\n# balance  heaps\nheapq.heappush(self.max  heap, -heapq.heappop(self.min_heap)  )\ndef find  median(self):\nif len(self.max  heap) > len(self.min  heap): # return max heap value\nreturn 1.0 * ~seilf.max  heap[0]\nelse: # return average  of two\nreturn 1.0 * (self.min_  heap[0] - self.max  heap[0})  / 2\nThe runtime  of add num() 1s O(log(4))  because  each heap insertion  (up to 3) takes O(log())  time.\nThe runtime  of tind median()  1s O(1). Therefore,  the overall procedure  takes O(log NV). The space\ncomplexity  1s O(N), since NV elements  are stored among  the heaps.\nSolution  #9.29\nInstead  of checking  whether  the whole input string matches  the regex, let’s start by breaking  down\nthe problem  and considering  just one character  at a time. We can start from the back of both the regex\nand the input strings.  If both are regular  ‘a-z’ characters  and do not match,  we can immediately  return\nfalse. Otherwise,  check whether  the remaining  regex string and input strings  match without  those two\nending  characters.\nof\n?’. Because  *?’ matches  any single\ncharacter,  we can check if the regex without  the *?” matches  the rest of the input string without  the\nmost recent character.  Here we begin to see the recursive  nature of the problem.Now, say we see that the last character  of the regex string is\n228 Ace the Data Science  Interview | Coding"
  },
  {
    "page_number": 241,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nIf we see a ‘*’, then there are two cases. The first is that ‘*’ could represent  an arbitrary  number  of\ncharacters.  In this case, its behavior  is similar  to ‘?’ - for any one character,  we just recursively  check\nthe rest of the input minus that one character  with the same regex string. On the other hand, ‘*’ could\nhave the behavior  of an empty string, in which case check the rest of the input with the same regex\nstring without  the ‘*’.\nNote the overlapping  subproblems:  if we find that a part of the regex matches  a substring  of the input,\nwe can determine  whether  another  character  added onto the regex (*?’, ‘*’, or character)  matches  the\ninput substring  with another  character  added on to it.\nIn particular,  say we have inputs s (string)  and r (regex).  We can keep track of a table that stores\nBooleans  for the function  output. In this table, indices  (i, /) will store the function  result of checking\nwhether  or not the substring  of the input up to character  i matches  that of the regex substring  up to\ncharacter  / (whether  s[:/] and r[:/] match).  Thus, after this table is filled, we want to check the index\nj= —], 7 = -1 (the last element  of the table) to confirm  whether  the full regex string matches  the full\ninput string.\nWe can start by instantiating  the dp matrix to be false for all indices.  Then, for the value at dp[0)[0],\nwe can set it to true, since it’s trivially  true thal an empty regex string matches  an empty string. In\naddition,  since we know that any regex substrings  with just ‘*’ characters  matches  any substring,  we\ncan set dp[0][j]  to be 0 as long as the regex contains  ‘*’ characters  up to index  /.\nSummarizing  the above, our recurrence  relations  are as follows:\n1. Ifr[j-1]  is ‘?’ or matches  s[i-1], then we set dp[1}{j]  = dp[i-1]Jj-1]  (recursive  call where we check\neverything  before the current  last character  in both the regex and input string).\n2. Ifr[j-1]  is ‘*’, then the result is the logical OR of the following  3 sub-cases:\na) dp[i-1][j-1]  (recursive  call as if the ‘*’ functioned  as a ?).\nb) = dp[i]{j-1]  (recursive  call where the ‘*’ matches  the current  last character  and will continue\nfrom there); or\nc) dpfi-1)[j]  (recursive  call where the ‘*’ matches  an empty string).\nThe time complexity  is O(MN), where M is the length of the first string, and N is the length of the\nsecond string, since, in the worst case, we will fill out every cell accordingly  in a bottom-up  manner.\nThe space complexity  is O(MN)  as well, to accommodate  the 2-d array.\nIf neither  of the prior cases applies,  then we ignore changing  dp[i}[)] since the entry will be false (and\nwe can set it to that by default).  Finally, we return the last indices for the string and regex within dp,\ni.e. dp[-1][-1]:\ndef wildcard  match(string,  regex):\nm len (string)\nn = len(regex)\ndp = [[False for _ in range(n+i)]  for _ in range(m+1)  ] # initialization\ndp{o}][(0]  = True\nfor j in range(1, ntl): # filling in Trues for any *s\nif regex[j-1]  != “*\":\nbreak\ndp[(0]{j]  = True\nAce the Data Science  Interview 229"
  },
  {
    "page_number": 242,
    "content": "CHAPTER  9 : CODING\nfor iin range(1,  m+l1):\nfor } in range(1,  ntl):\nif regex(j-1]  == ‘?’ or regex[j-1]  == string{i-1]:  # case 1\ndp({ijJ{3] = dp(i-1)}  [3-1]\nelif regex[j-1]  == “*”: # case 2\ndp{i](3] = dpf{i-1][j-1]  or dpfi][j-1]  or dpfi-1] [5]\nreturn dp[-1]  [-1]\nSolution  #9.30\nTo find the optimal  location  of the new fire station,  minimize  the following  cost function,  where (x, y.)\nrepresents  the coordinates  of the new fire station, and there are n houses  each with coordinates  (x,, y,):\nL(x, y) = YX ve. -x,) +(¥.-9,)\nOne possible  method  to solve this optimization  problem  is through  grid search, where you manually\nsweep through  all possible  locations.  Our solution,  however,  will focus on gradient  descent. Recall\nthat gradient  descent  allows you to find a local minimum  for a convex  function  by taking steps in the\nopposite  direction  of the gradient  of the function.  Note that because  there is no other local minimum\nother than the global minimum,  and the sum of squares function  above is convex,  gradient  descent\nleads us to an optimal  result. For each iteration  of gradient  descent,  as we refine the current  location\nand get closer to the optimal  location,  we decay the learning  rate and take smaller  and smaller  steps\nuntil we arrive at the solution.\nBefore we can implement  gradient  descent  in code, we need to calculate  the partial derivatives  of\nthe sum of  squares  function  for both x and y. We get the following  for the partial with respect  to x:\nOL (x,y) _ (x, —x;)\nox Mx =x, +(y, -9,)'\nAnd an analogous  partial with respect to y: OL (x,y) = 2 _ yi)\nn 2 2ay a (x, — x,) +(y, 7” y,)\nTo control  the rate of gradient  descent,  we initialize  the following  hyperparameters:\n1. Initial learning  rate: the starting  value for the learning  rate. The learning  rate is used to scale the\nsize of the moves we take in the opposite  direction  of the gradient.\n2. Rate of  decay:  used to decrease  the learning  rate. We make the learning  rate progressively  smaller\nto converge  upon an eventual  answer.\neo)Terminal  rate: the stopping  condition.  When the learning  rate becomes  smaller  than the terminal\nrate, we can exit the gradient  descent  process.\n4. Damping  factor: constant  that helps with numerical  stability.  During  gradient  descent  there are\nalways  oscillations  back and forth around  a range of values — by applying  a damping  factor to\nreduce  the magnitude  of step sizes, there is a better chance  of a smooth  convergence.\nTo implement  gradient  descent,  we begin with the mathematical  centroid  of all the coordinates  as the\nstarting  estimate  for the optimal  location  (« , 1). Then, we can set the hyperparameters  defined  above\nand update the values  of (x. 4) iteratively  until the terminal  condition  is hit and the estimate  for the\noptimal  location  1s reached:\n230 Ace the Data Science  Interview  | Coding"
  },
  {
    "page_number": 243,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nimport  math\ndef get_optimal  location  (coords)  :\ndef partialx(x_c,  y_c): # partial  with respect  to x\nreturn  sum(((x_c-x_i)/math.sqrt((x_¢c-x  i)**2 +\n(y_c-y_i)**2)  if xc != x i else 0) for x  i, y i in coords)\ndef partialY(x_c,  yc): # partial  with respect  to y\nreturn  sum(((y  c~y_ i)/math.sqrt((x_c-x  i)**2 +\n(y_cm~y i)**2) if yc != y i else 0) for x i, y i in coords)\nrate = 1 # initial  learning  rate\nrate decay = 0.99 # rate of decay\nterminal  rate = le-8 # le-8 # terminal  rate (stop condition)\ndamping  factor  = 0.7\nit = len(coords)\nxX Cc = sum(x for x, y in coords) / 1 # centroid  x\ny_¢ = sum(y for x, y in coords) / 1 # centroid  y\ndx = 0\ndy = 0\nwhile rate > terminal  rate:\ndx = partialX(x_c,  y_c) + damping  factor * dx\ndy = partialY(x_c,  y_c) + damping  factor * dy\nxX _C = K Cc - rate * dx # update coordinates\ny c= yc - rate * dy\nrate = rate * rate decay # update learning  rate\nreturn (x Cc, y _C)\nTo assess the runtime  complexity,  assume that the minimum  values of (x, y.) are given by (Xa\nYnin) and the maximum  values are given by: (x, 0.5 Vinay): Let X) = Xnav — Xmin 200 Vy =Vroe ~ Yrin\nThen, we know the upper bound, in terms of points tested, is x, * v,. Since calculating  the partial\nderivatives  for x and y takes O() each, then the time complexity  should  be O(N * x, * v_). The space\ncomplexity  1s O(N) since, during  the partial  derivatives  calculating,  we are summing  over N elements\nin a temporary  list.\nThe algorithm’s  runtime  is also dependent  on the hyperparameters  that control the gradient  descent.\nHere is how the runtime  is affected  by each:\n1. Initial learning  rate: this 1s often set at 1. The higher  it is, the longer the runtime.  since the initial\nlearning  rate needs to converge  to the terminal  rate.\n2. The rate of decay: this is set between  0 and |. The higher it is, the faster the runtime,  since a\nlarger rate of decay means reaching  the terminal  learning  rate faster.\nAce the Data Science  Interview 231"
  },
  {
    "page_number": 244,
    "content": "CHAPTER  9 : CODING\n3. The terminal  rate: the higher it is, the faster the runtime,  since convergence  conditions  are\nreached  more quickly.\n4. The damping  factor: set between  0 and |. The smaller  the value, the faster the runtime,  since\nthere is less damping;  hence, larger  moves  due to the gradient.\n232 Ace the Data Science  Interview | Coding"
  },
  {
    "page_number": 245,
    "content": "Product  Sense\nCHAPTER  10\nA magikarp,  a one-legged  man in an ass-kicking  contest,  and an ejector  seat in a helicopter\nThese three are examples  of things more useful than a data scientist  with a weak product\nsense and business  acumen. Because  data scientists  often work cross-functionally  with\nproduct  managers  (PMs) and business  stakeholders  to help create  product  roadmaps  and\nunderstand  the root cause of various  business  problems,  they are expected  to have a strong\nproduct  and business  intuition.  Its not just data scientists  who can expect  product-sense\ninterview  questions  — these topics  are also frequently  covered  during  product  analyst,  data\nanalyst,  and business  intelligence  analyst  interviews.\nBetween  questions  on the art of selecting  product  metrics,  troubleshooting  A/B test results,\nand weighing  business  trade-offs,  the scope of product  interview  questions  is massive.  But\nfear not! In this chapter  we cover both actionable  strategies  to approach  the four most\ncommon  types of product  interview  questions  you'll  face and long-term  lips to develop  your\noverall  product  and business  sense. We also solve 18 real product-sense  interview  questions\nfrom companies  like Amazon,  Airbnb,  and Facebook.\nFour Most Common  Types  of Product  Interview  Questions\nBefore we dive into specific  product  management  topics, it’s important  for you, the reader, to first\nget a glimpse  at the four most common  types ofproduct-focused  data science interview  questions:\n¢ Defining  a product  metric: What metrics would you define to measure  the success of a new\nproduct launch? If a product manager  (PM) thought il was a good idea to change an existing\nfeature,  what metrics  would you analyze  to validate  their hypothesis?\nAce the Data Science  Interview 233"
  },
  {
    "page_number": 246,
    "content": "CHAPTER  10: PRODUCT  SENSE\nDiagnosing  a metric change:  How would you investigate  the root cause behind a metric going\nup or down? What if other counter metrics changed  at the same time -— how would you handle\nthe metric trade-offs?\nBrainstorming  product  features:  At a high level, should a company  launch a particular  new\nproduct?  Why or why not? For an existing  product,  what feature ideas do you have to improve\na certain  metric?\nDesigning  A/B tests: How would you set up an A/B test to measure  the success  of a new feature?\nWhat are some likely pitfalls  you might run into while performing  A/B tests, and how would you\ndeal with them?\nBy keeping  these frequently  asked types of questions  top of mind, we hope you’ll better understand\nhow the following  high-level  advice can be concretely  applied  to acing product  questions.\nBig-Picture  Advice  for Product  Sense Interview  Questions\nFramework  for Approaching  Product  Interview  Questions\nThe tips below work for approaching  product questions,  as well as for the occasional  business\nquestion:\nAsk clarifying  questions:  Make sure you understand  the user flow for a product,  who the end\nusers are for the product,  who the other stakeholders  are that are involved  with this problem,\nand what product  and business  goals we aim to achieve  by solving  the problem.  Even if you’ ve\ndone your research  into the company  and product  and know many details,  frame your knowledge\nas a question  so you don’t inadvertently  head down the wrong path. For example:  “I know\nRobinhood’s  mission 1s to democratize  finance for all. It seems this crypto wallet feature is\nmeant to democratize  access to crypto currencies,  which can also help us better compete  with\nCoinbase.  Am I on the right track?”\nEstablish  problem  boundaries:  Thesc are big problems;  scope them down. Establish  with your\ninterviewer  what you're  purposely  choosing  to ignore  to solve the problem  within  the time frame\nof the interview.\nTalk Out Loud: You've seen this tip now many times. These interviews  are held to see your\nthought  process  - and until Elon Musk invents mind reading  at Neuralink,  you need to voice\nyour thinking!\nBe conversational:  Don't talk at the interviewer  — talk to the interviewer.  Engage them in\nconversation  from time to time as a means of checking  in. For instance,  “I think a good metric\nfor engagement  on YouTube  is average  time spent watching  videos,  so I'll focus on that. How\ndoes that sound?”\nKeep goals  forefront:  \\t’s easy to get lost in technical  details. Never forget your answer  stems\nfrom the company’s  mission  and vision, which you hopefully  articulated  at the start of your\nconversation!\nBring in outside  experience  tactfully:  Because  these problems  are rooted in the real world, it’s\nokay to flex your past domain  experience.  Just don’t go overboard  and come across as arrogant\nor cargo cult-y by saying,  “This is the only way a problem  should  be solved,  because  that’s how\nwe solved it at Google.”\n234 Ace the Data Science  Interview  | Product  Sense"
  },
  {
    "page_number": 247,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nFeeling  like there’s  too many tips to keep track of? Ultimately,  if you can remember  just one thing\nwhen solving  product  problems,  it’s this: pretend  you’ ve already  been hired at the company  as a data\nscientist.  You’re  just having  a meeting  about the problem  with another  co-worker.  When you adopt\nthe mindset  that you’re  already  working  for the company,  behaviors  like talking  out loud with your\n“co-worker”  or keeping  the company  mission  top of mind should  come naturally.\nHow to Develop  Your Product  Sense\nBecause  data scientists  help Product  Managers  (PMs) quantitatively  understand  the business  and\nlook for opportunities  for product  improvement  within the data, they play a crucial role during the\nproduct  roadmap  creation  process.  As such, questions  asking you to brainstorm  new products  and\nfeatures  are very common  during  product-focused  data science  interviews.  The best way to improve\nyour performance  on this type of problem  is to improve  your general  product  sense.\nHowever,  don’t let the term “product  sense”  faze you; this isn’t an innate gift you’re  born with, but,\nrather, a skill that can be developed  over time. By following  the tips in this section  to enhance  your\noverall  product  sensibilities,  you won’t freeze up like a deer in the headlights  in your next interview\nwith Google,  when you’re  asked to brainstorm  features  to help students  better use Google  Hangouts.\nInstead,  you’!l tackle the problem  with the confidence  of Sundar  Pichai after yet another  Alphabet\nquarterly  earnings  beat.\nThe Daily Habit  You Need to Build Your Product  Sense\nAn easy way to develop  your product  sense is through  analyzing  the products  you naturally  encounter\nin your daily life. When using a product,  think about:\n¢ Who was this product  created  for?\n¢ What’s  the main problem  it was designed  to solve?\n* What are the product’s  end-user  benefits  (this is bigger  than simply  what problem  it solves!)?\n¢ How do the visual design  and marketing  copy help convey  the product’s  purpose  and benefits?\n¢ How does the product  tie in with the company’s  mission  and vision?\nA great deal of good product  sense is having empathy  for a product’s  or service’s  users. That’s\nwhy, when answering  the above questions  while analyzing  a product  or service,  you must try to put\nyourself  in a user’s shoes.\nTake Snapchat,  for example.  Sure, you can post photos to your story or send messages  to people on\nSnapchat.  But so can iMessage,  WhatsApp,  Instagram,  and Messenger.  At a deeper level, Snapchat\nis about being able to stay in touch with your closest  friends in a casual, authentic  way. That’s why\nopening  up the Snapchat  app puts you directly  on the camera, in order to make it frictionless  to\nexpress  yourself  and live in the moment  — two core elements  of Snap’s company  mission.  It’s also\nwhy photos and messages  disappear  by default  —— this lowers the barrier to expressing  yourself  and\npushes  you to share whatever  you captured  rather than spending  time editing  a photo you know will\nsoon be gone.\nContrast  this with Instagram,  which defaults  to the feed to promote  consumption  rather than visual\ncommunication.  On Snap’s more polished rival, you are made to feel that your posts need to be\nperfect,  lest they be judged  by acquaintances  and extended  family. There’s  an associated  permanence\nto the photos you post, which takes away some of the whimsicalness  that Snap’s optimized  for.\nSimilarly,  Snap’s default ephemeral  messages  set it apart from other messaging  platforms like\niMessage  and Instagram.\nAce the Data Science  Interview 235"
  },
  {
    "page_number": 248,
    "content": "CHAPTER  10: PRODUCT  SENSE\nWhile we could go on and on about the two apps, which have overlapping  functionality  but serve two\nvery different  user needs, we want to emphasize  that the point of this exercise  is to go beyond simply\nrelegating  the app to just a “dumb Gen Z” thing or “basically  the same as Instagram.”  By thinking\nmore critically  about products in your everyday  life, you can sharpen your product intuition,  ace\ninterviews,  and eventually  build successful  products  in the workplace.\nCalibrate  Your Intuition  by Analyzing  Reviews\nThe daily habit of analyzing  products,  who they’re made for, and what benefits they offer is fine\nand dandy, but how do you know you’re right’? How do you know if your reasoning  lines up with\nhow others perceive  the product and its benefits?  One way to calibrate  yourself  and fine tune your\nintuition  is by analyzing  customer  reviews.\nBy looking at the positive reviews and press for the product, you can see how user benefits are\ndescribed  and what is expected  of the product. Reading these positive reviews helps you better\narticulate  user benefits;  it also helps you notice benefits  you might have taken for granted.  Similarly,\nby reading  negative  reviews,  you can understand  how the products  you encounter  are falling short in\nmeeting  user needs. By comparing  the issues the negative  reviews  flagged  against  your own product\nevaluation,  you can start to develop  a more critical eye. Additionally,  negative  reviews  are a great\nsource  of ideas for product  brainstorming.\nReddit is a great place to see unfiltered  conversations  about products  and services.  For apps, also check\nout the App Store and Google  Play Store. For enterprise  products,  check out G2 Crowd  and Gartner\nSpectal Reports.  For physical  products,  check out the reviews  on Amazon.  (While  you're  there, help\nus immensely  by spending  two minutes  to rate and review  our book —- we greatly  appreciate  this!).\nHow to Build Your Business  Sense\nWhile it’s exceedingly  rare to be exphcitly  asked business  questions  like “How do you measure\nthe health of the enterprise  sales pipeline?”  or “How do you model free cash flow from revenue?”\nhaving  some general  business  knowledge  is crucial.  Why? Because  cash rules everything  around  me\n(C.R.E.A.M.).\nWu-Tang  reference  aside, the honest truth is that, in the workplace,  having  the best technical  skills\npossible  won't matter if you solve the wrong business  problems.  By following  the money and\nunderstanding  how the products  you work on help the business  make more money,  you give yourself\na better chance  of working  on high-leverage  technical  projects.  Stronger  business  sense helps even\nyour product-sense  get sharper,  since ultimately  knowing  what products  to build and what product\nmetrics  to improve  upon stems from the company’s  business  model and strategy.\nThe Daily Habit  You Need to Build Your Business  Sense\nMuch like the daily habit of understanding  user incentives  we recommended  earlier in order to\ndevelop  product  sense, asking yourself the following  questions  when you encounter  a new business\ncan help develop  your business  sensee:\n* Business  Model:  How does the business  monetize?  What product  levers  can be pulled  to improve\nthe business’s  ability to monetize?\n¢ = Metrics:  Which key performance  indicators  (KPIs) would I measure  if  | were working  on this\nbusiness?  What factors  and variables  influence  those particular  metrics?\n* Landscape:  How does the business  fit into the broader  ecosystem  of the companies  comprising\nthat industry?  What companies  does the business  compete  with, and what companies  does it\npartner  with?\n236 Ace the Data Science  Interview | Product  Sense"
  },
  {
    "page_number": 249,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nAnother  way to grow your business  awareness  is by reading  some of the best business  books out\nthere. We'll admit, there’s a lot of fluffy business  books out there, most of which are just self-help\nmanuals  or written  to pad the author’s  ego. And plenty  should  have been a blog post but got padded\nwith filler to become  a book. Then there’s books like How to Win Friends  and Influence  People\nor Think & Grow Rich, which, while interesting,  don't really help you foundationally  understand\nbusiness.  As such, we curated  a list of what business  and product  books helped  us out the most in our\ncareer:  acethedatascienceinterview.com/business-books.\nHow to Hack Your Domain  Experience\nWe'll let you in on a psychological  quirk interviewers  have which you can hack: your interviewers\nlikely live in a bubble.  They’re  knee deep in the problem  in workplaces  designed  to be hard to unplug\n— eating  the company-provided  free three meals a day, talking  about the problem  at dinner  with their\nteammates,  and thinking  about work while commuting  back home on the company  shuttle.  That’s\nwhy it shouldn’t  be surprising  that, come interview  time with you, an outsider,  they forget you don’t\nhave as much context  as they do! Bubbles  are real, y’all, and techies  love to live in them!\nWhile unfair,  the data scientist’s  familiarity  and time spent with the product  and company  can cloud\ntheir ability  to accurately  assess your product  sense. Plus, you might be interviewing  against  internal\ncandidates  with better context  on the problem.  Or you might be competing  against  candidates  who\nworked  in similar  businesses,  so naturally  get a leg up. That’s why doing your homework  is one of\nthe best ways to level up.\nDoing  Your Homework:  Uber Eats Example\nAs a real-world  example,  let’s say that you had an upcoming  interview  with the Uber Eats team and\nwanted  to prepare  for any product  or business  case questions  you might  be asked.  To prepare,  you should\nlearn how Uber as a whole makes its money  and how much revenue  comes from their transportation\nproducts  versus their delivery  business.  You should dig deeper into Uber Eats in order to understand\nhow it fits into Uber’s  overall  strategy  as a logistics  and transportation  company.  In addition,  learn about\nthe common  metrics  used to measure  two- and three-sided  marketplaces.  Finally,  prior to the interview,\nyou should attempt  to uncover  the key inputs for Uber’s pricing  and payout algorithms  that determine\nhow much it charges  a customer  and how much it pays the delivery  driver and restaurant.\nMost of the research  described  above can be done with information  available  free-of-charge  on the\nweb, by searching  “company  name business  model.”  Google News reports what financial  analysts\nsay about the company.  If the company  is public, looking  at its earnings  reports  provides  another  good\nway to hone your business  sense, and this lets you directly see what key business  metrics are being\ntracked.  If the company  you are analyzing  isn’t public, see if there are comparable  public businesses,\nsince they’lI most likely use similar  metrics to what the private company  tracks internally.\nAnother  great resource  which candidates  unfortunately  tend to underutilize  (to their great peril) 1s\na company’s  engineering  blog. This resource gives you an inside look into the business and the\ntechnical  systems underlying  it. For instance,  Uber Eats (from the above example) often publishes\nblog posts with titles like “Optimizing  Delivery  Times on Uber Eats” and “Food Discovery  with\nUber Eats.” You could discover  and deduce a great deal about the business from posts like these.\nDoorDash’s  Engineering  blog wouldn’t  be a bad place to look either!\nDoing Your Homework  Means Use the Damn Product\nIt’s easy to be an armchair  analyst, reading  earnings  reports and blogs about the product. But at some\npoint, it’s absolutely  necessary  to do your own due diligence  by exploring  the product on your own.\nAce the Data Science  Interview 237"
  },
  {
    "page_number": 250,
    "content": "CHAPTER  10: PRODUCT  SENSE\nAnd even though you probably  knew this, and maybe the recruiter  told you to do this already,  we are\nsurprised  by how many candidates  we’ve coached  that skip this crucial step.\nSo, let’s be honest — how are you going to reason about a high-level  business strategy if your\nfundamental  understanding  of the product is weak? In the Uber Eats example, you, like most\ncandidates,  have most likely ordered food before, but have you also tried to download  the Uber\nDriver app? Have you looked up the UI that restaurants  use to fulfill orders? Have you looked at\nUber’s marketing  website targeted at signing up new eateries,  which explicitly  spells out the value\nprops that Uber offers restaurants?  Putting in this extra effort to understand  the product from tts\nmultiple  angles would easily put you in the top 1% of candidates.\nThis much company  and product  research  is crazy, right?\nYes, dear reader, we realize this is a lot of work to do before each interview.  But the more company\nand product knowledge  you can sneak into your answers, the better you'll do. Additionally,  a\ncommon  interview  question  — especially  at smaller  companies  — 1s “Did you have a chance  to use\nour product?”  followed  up with “What did you think? Any ideas on how to improve  it?” This amount\nof preparation  will help you knock these questions  out of the park.\nFinally, even if there aren’t product-sense  questions  directly concerning  the information  you\nresearched,  don’t be dismayed.  Your effort wasn’t wasted!  At the end of the interview,  your research\nwould enable you to ask the interviewer  more intelligent  questions  than would have been possible\notherwise.  Moreover,  it would demonstrate  your passion  and willingness  to put forth extra effort.\nInstead of asking the interviewer  run-of-the-mill  questions  like “What’s  your favorite  part about\nworking  at Uber Eats?” you can instead comment,  “I was reading about the food delivery  time\nestimation  algorithm  on your blog and found X fascinating.  | was curious  why you used approach\nY, and if you ever thought  about trying out Z instead?”  Suddenly,  you’re showing  enthusiasm  and\ninsight into the business,  offering  a suggestion,  and gaining  the opportunity  to learn something  new\nand meaningful  in the process.\nMetrics  for Product  & Case Interviews\nUser Acquisition  Funnel\nBefore we address the art of metric selection,  it’s key we cover the popular  marketing  concept\nof a customer  acquisition  funnel. There is no one funnel to rule them all —— you’ll see different\nframeworks  depending  on the product’s  use case (B2B vs. B2C), how the product is monetized\n(one-time  purchase  vs. recurring  vs. freemium).  One specific  customer  lifecycle  we like is the pirate\nmetrics framework  created by the founder  of startup accelerator  500 Startups,  Dave McClure.  It’s\ncalled  pirate  metrics  because  the funnel steps form the acronym  AARRR.\nUser  Acquisition  Metrics\nUser acquisition  metrics  try to capture  how people  are finding  and trying  out your product.  Standard\nmetrics used to track success in this area include new user counts, sign-up  conversion  rates, and\ncustomer  acquisition  costs (CAC).  Youll often hear the word “top of funnel”  when referring  to this\nstage of the customer  journey.  For Pinterest,  relevant  user acquisition  metrics  might be the number  of\nnew users who download  the app or the number  of new customers  who create an account  per weck.\nRelated  metrics  Pinterest  would track for this step include  the sign-up  conversion  rate and customer\nacquisition  costs (CAC).\n238 Ace the Data Science  Interview  | Product  Sense"
  },
  {
    "page_number": 251,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nAARRR  Pirate  Metrics\nHow do users  find you?\nACTIVATIONDo users have  a great first experience?\nDo users come back?\nDo users tell others?\nHow do you make money?\nUser Activation  Metrics\nUser activation  metrics  refer to the point at which a user has successfully  onboarded  and reached  the\nproduct  “aha”  moment  -—- the spot where  they experience  the core value of the product. For DoorDash,\nit could be the number  of people  who make their first delivery  order per week. On Instagram,  it could\nbe after a user views 10 unique  posts in their newsfeed,  or after they make their first post or story.\nUser Engagement\nWhile not an explicit  step in the AARRR  pirate metrics  framework,  we thought  it was important  to\nbring up the term user engagement.  After user activation,  it’s important  to then look at how often and\nhow well users interact  with the product.  The product  interaction  being measured  could be time spent\non platforms  like Facebook  or rides booked  for Uber. A common  way to incorporate  frequency  into\nthe measure  of user engagement  is by looking  at the unique  number  of users who took a core action\nwithin a fixed time period.  For example,  Facebook  measures  daily active users (DAU),  weekly  active\nusers (WAU),  and monthly  active users (MAU).\nUser Retention  Metrics\nUser retention refers to whether or not users keep coming back to a product or service over a\nprolonged  period  of time. The process  of users joining  and then leaving  permanently  1s called churn.\nMaximizing  retention  and minimizing  churn is a primary  focus of most businesses,  because  acquiring\na new customer  is typically  tougher  and more costly than retaining  an existing  user or reactivating  a\nprior user who has churned.  Metrics  to measure  user retention  include  monthly  retention  and monthly\nchurn.\nUser Referral\nReferral,  where a user shares the product  with others, could technically  happen at any time. However,\nin practice,  users typically  invite others only after having been activated,  engaged,  and retained on\nthe product for a while. To quantify virality, growth marketers  use the k-factor, which is just the\nAce the Data Science  Interview 239"
  },
  {
    "page_number": 252,
    "content": "CHAPTER  10: PRODUCT  SENSE\naverage  number  of referrals  sent per user multiplied  by the conversion  rate of each referral. A k-factor\nover | would indicate  exponential  growth.\nUser Revenue\nThe popular funnel idea starts to break down, as you may be able to make money from the user at\nany time -— not necessarily  after they have been retained or sent referrals. One way to measure a\nuser’s revenue is the ifetime value per customer  (LTV) — how much money a customer  brings into\nthe business  before they churn. Sustainable  companies  seek to have a high LT V-to-CAC  ratio, so that\ntheir marketing  efforts pay off before the user eventually  churns.\nDo Your Homework:  Metrics  Edition\nAs mentioned  in the “do your homework”  section,  you need to look up the primary  metrics  associated\nwith the type of company  with which you are interviewing  prior to your interview,  so as to have a\nleg up on metric-related  questions.  For example,  if you're interviewing  with an enterprise  Software-\nas-a-Service  (SaaS) company  like Hubspot  or Workday,  knowing  what terms like “ACV” (average\ncontract  value) or “MRR”  (monthly  recurring  revenue)  mean is helpful.\nAs we explained  at the start of Ace the Data Science Interview,  we are merely trying to refresh\nyour memory with quick deep dives. For a better deep dive into different  types of companies  and\ntheir associated  metrics, we recommend  the book Lean Analytics  by Alistair Croll and Benjamin\nYoskovitz.  The book devotes entire chapters to measuring  such different business models as a\nfreemium  mobile app and a two-sided  marketplace.  (For a full list of books we recommend,  visit\nacethedatascienceinterview.com/best-books-for-data-scientists)\nWhat Makes  a Metric  Good or Bad?\nNow that we understand  the stages of the user journey,  we are one step closer to addressing  the most\ncommon  product interview  question  you'll face: defining  an appropriate  metric for a new product\nor feature.  An example  interview  question  of this nature is “How would you measure  the success  of\nFacebook  Dating?”\nProduct-focused  tech companies  ask this type of interview  question  because  data scientists  often\nhelp product  managers  determine  the best analytics  to measure  the success  of a new product  launch\nor feature change. Note, these conversations  don’t necessarily  happen  just at launch time: often it\nhelps to have the end result in mind before building  a new product  or feature.  As such, often mght\nafter the product  brainstorm  phase, when ideas are being triaged  to see if they should  make it into the\nroadmap,  conversations  about what success  would look like occur. Working  to define metrics  isn’t\njust a skill for data scientists:  data analysts  and business  intelligence  engineers  also play a role in\nbuilding  out the dashboards  to visualize  and monitor  the health of the product  post launch.\nBefore we jump into the framework  for answering  a product  definition  question,  it’s important  we\nclarify what makes a metric good or bad in the first place. [ts easier to start with examples  of what\nmakes a metric bad, since a good metric essentially  manages  to avoid the flaws of bad metrics.\nExamples  of Bad Product  Metrics\nHere’s some common  types of bad metrics,  as well as an example  of each for the Facebook  dating\nquestion  above:\n. Vanity  metrics:  These metrics  sound nice, but don’t capture  anything  meaningful.  For example,\ndating profiles  viewed  could be a proxy for engagement,  but if this number  is too high or low,\n240 Ace the Data Science  Interview | Product  Sense"
  },
  {
    "page_number": 253,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\ndoes it really tell us if the app is working  well? Does it really impact  the number  of meaningful\nrelationships  formed  from the product?\ne Irrelevant  Metrics:  These metrics  aren't tied to the business  goal. For example,  time spent\nusing Facebook  Dating. Does it really matter? Sure, it’s an indicator  that there is use, but it\ndoesn’t  capture  the true value a dating app offers. Time spent is better for media consumption-\nrelated  products  like YouTube  and Netflix,  not activity-driven  dating  apps.\n° impractical  Metrics:  An example  of an impractical  metric is the number  of 3rd dates that\noccurred.  While  a good sign of a meaningful  match  being made, how would  you even measure\nthis? More advanced  dating  apps have a “did you meet?”  user prompt,  or they use NLP on the\nconversation  to determine  if they think you met, but this approach  likely works only for a first\ndate, after which the conversation  usually  moves  off-app.\n° Complicated  Metrics:  \\s the metric easy to explain to stakeholders?  If a complex  metric\nchanged,  would it be easy to understand  what actually  happened,  or would you have to break\ninto its sub-component  parts and definition  to really understand  which pieces were affected?\n. Delayed  Metrics:  Number  of marriages  that occurred.  Not only is this impractical  to know (just\nlike the 3rd date metric),  it would take a very long time to figure out because  people  tend not to\nget married  within a few months  of connecting  on an app. While downstream  success  metrics\nhave their place, you want to track leading  indicators  so that data can be collected  earlier,  which\nenables  you to make decisions  and notice  problems  faster.\nWhat Makes  a Good Metric  Good?\nBy reversing  what makes  metrics  bad, you end up the following  four qualities  good metrics  possess:\n° Meaningful:  Tied to business  goals; isn’t easily gamed;  is actionable  and can drive decisions\n° Measurable:  Simple  to consistently  and reliably  track\n° Understandable:  Easy for stakeholders  to understand;  intuitive  to know what is being measured\nbased on the name\n° Timely:  Can be collected  within  a reasonable  time frame\nNorth Star & Guardrail  Metrics\nWhen asked an interview  question about defining metrics, you need to go beyond the simple\nframework  of only suggesting  good metrics and avoiding  bad ones. Besides  mentioning  the most\nimportant  and relevant  metric to the problem  at hand (known  as a north star metric),  you should  also\nmention  guardrail  metrics  (also known as counter  metrics).  Guardrail  metrics  are business  metrics\nthat should not be degraded  while optimizing  the metric of interest.  These are important  to monitor\nbecause  it’s easy to boost a given metric at the expense  of counter  metrics  or other KPIs.\nFor example,  when Kevin was working  on Facebook  Groups,  trying to reduce harmful  content like\nhate speech  and spam, his team’s topline  metric was to decrease  the prevalence  of harmful  posts. One\nway to achieve  this goal is to remove  99% of posts from the feed. You can’t have harmful  content\non newsfeed  if there isn’t any content  on the newsfeed  to begin with! Obviously,  such an approach\ndoesn’t make sense. That’s why, for every A/B test, they’d also monitor guardrail  metrics (posts\nmade, posts viewed,  number  of likes and comments  per post).\nIt is good to bring up counter metrics with the interviewer  after they agree with your key success\nmetrics.  Proactively  having this discussion  shows that you realize building  a product isn’t just about\nAce the Data Science  Interview 24]"
  },
  {
    "page_number": 254,
    "content": "CHAPTER  10: PRODUCT  SENSE\noptimizing  a specific set of feature-related  metrics, but more about holistically  making sure the\nproduct  is benefiting  the business  at-large.\n3-Step Framework  to Answer  Product  Metrics  Definition  Questions\nNow that we understand  what good metrics  look like, and the stages of the user journey,  we can cover\nthe specific framework  you can use to answer the question,  “How would you measure  the success of\nFacebook  Dating?”\nStep 1: Clarify  the Product  & Its Purpose\nWe need to start by defining  the problem.  This can be done by clarifying  the business  purpose  behind\nthe product, the product’s  goal, who this feature is for, and what the user flow looks like. For the\nFacebook  Dating  example,  you might clarify:\n* Why is Facebook  interested  in a dating product?  Is it to boost engagement  on the app? Is it to\ncreate another  surface  for ads, which can help increase  ad revenue?\n¢ What’s the point of Facebook  Dating -— is it meant for casual dating, or people looking for\nmariage?\n* Is the dating feature for all people,  or a certain  age demographic  or orientation?\n¢ How does the product  work? Is this a stand-alone  app from Facebook,  or integrated  to be within\nFacebook  itself? Is there a Tinder-esque  swipe-based  UI? Are there any gimmicks  involved,  like\nwomen  initiating  the conversation,  similar  to Bumble?\nIn the situation  with Facebook  Dating,  let’s assume  that you learn that it’s an app, similar  to Tinder,\nbut with more elaborate  profiles  to foster more intentional  swipes  and matches.  You learn it’s being\nlaunched  in New Zealand  first, because  it approximates  the larger English-speaking  populations  like\nthe U.S. and the U.K. where Facebook  eventually  wants to launch  the product.\nStep 2: Explain  the Product  & Business  Goals\nWhile you might expect  that you can just ask the interviewer  what the product  and business  goal are,\noften, the interviewer  is looking  to you to synthesize  what the main purpose  behind  the product  is,\nand how it ties into the business.  You'll get bonus points for tying it back to the company  mission.\nThis is exactly  why earlier  we strongly  recommended  you to do company  and product  research  —- it\nmakes  a huge difference  on this step.\nIn the case of Facebook  Dating,  it relates back to the product  czars wanting  to drive engagement  to\nthe app. If you log in to check your matches,  it’s easy to see one of the super-targeted  ads and drive\nrevenue  for the company.  Plus, it keeps the rest of the Facebook  ecosystem  attractive  — if you check\nyour notification  about a new match,  maybe  you also check your notification  about a group post or a\nfriend's  birthday  at the same time. Plus, with in-built  messaging,  it keeps people  within  the Facebook,\nInstagram,  Messenger,  WhatsApp  ecosystem.\nLastly, the Facebook  company  mission  is to help you develop  meaningful  relationships.  Keeping  in\ntouch with family, interacting  with your community  via groups,  and messaging  with your friends  are\nall part of the mandate.  So why not help you in the romantic  relationship  department  as well?\nStep 3: Define  Success  Metrics\nIt is crucial to determine  and measure  the main actions a user needs to take in order to drive the\nproduct  and business  goals. You can follow the uscr-acquisition  funnel we talked about earlier to\n242 Ace the Data Science  Interview | Product  Sense"
  },
  {
    "page_number": 255,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nserve as a framework  on how to guide the analysis.  As you are defining  the metrics,  remember  to\nrestate  how they align with the product  and business  goals you had mentioned  earlier.\nAcquisition  metrics:  How many users sign up and how many users fill out their profile could be\ngreat metrics  to measure  top-of-funnel.  This is important,  because  people don’t think of Facebook\nas a dating company,  and it’s important  to know if Facebook  is in the news for privacy  and data-use\nconcems.  This reputation  can get in the way of users trusting  it for their most intimate  needs.\nActivation  metrics:  These  could  be onboarding  metrics  if the profile  is filled out and a certain  number\nof photos  are uploaded.  Why? Because  rich detailed  profiles  are the point of a dating app seeking  to\nbe more long-term-relationship  focused.  Other activation  metrics  might be if they view ten profiles,\nor if they manage  to get a single match.  You want them to hit a point of user delight.\nEngagement  metrics:  Some example  engagement  metrics  can be: how many matches  were formed\nper user, how many matches  were made overall,  swipes done per user, and real life meetings  (this\ncould be indicated  by a phone number  swap). This would reflect if they were able to foster actual\nmeaningful  connections.  Retention  metrics  could be an off-shoot  of these core engagement  metrics,\nlike what percentage  of users swipe on the app after 28 days of signing up, or, of the users who\nmanaged  to get a match last month,  what percentage  of them managed  to get a match this month?\nRevenue  metrics:  Revenue  could be measured  by the number  of paid memberships  or upgrades\nbought. If Facebook  Dating is completely  free but ad-supported,  then revenue  from ad impressions\ncould be measured  per user.\nHowever,  paid memberships  or ad revenue  isn’t a priority  on launch for a company  like Facebook,\nwhich is primarily  trying to nail product-market-fit  first. Facebook  tends to take a long-term  approach\nto monetization.  Their biggest worries  are feature adoption  and user retention;  they know they can\neasily toss in ads to support  the product  when needed.\n4-Step  Framework  for Diagnosing  Metric  Changes\nThe second most common  product interview  question  that data scientists  face is one of diagnosing\nmetric changes.  For example,  you might be asked: “‘Instagram’s  average  number  of comments  per post\nis declining  — how would you troubleshoot  this?”\nStep 1: Scope  Out the Metric Change\nBefore even jumping  into a solution,  you need to clarify and gather context.\nQuestions  to ask:\n¢ Metric Definition  Nuance: What does the metric in question mean? Are we dealing with\nproportional  metrics?  If so, by isolating  which part of the ratio changed  — the numerator  or the\ndenominator  — you’re able to offer a more targeted hypothesis  for what is driving the metric\nchange.\n¢ Importance:  \\s this metric actually consequential  to the team? With thousands  of metrics being\ntracked at a company  like Facebook,  there’s constant  fluctuation  in all parts of the business,  but\nnot all changes  are equal or relevant.\n¢ Time  frame: \\s this a singular  occurrence  or an ongoing  issue? A sudden change or an ongoing\ntrend? At what granularity  (over a day, a week, a month) has the trend occurred?\n¢ Magnitude:  How big is the change, in both relative and absolute  terms? Are we comparing  this\nchange to last week or on a year-over-year  basis?\nAce the Data Science  Interview 243"
  },
  {
    "page_number": 256,
    "content": "CHAPTER  10: PRODUCT  SENSE\nSay that the interviewer  tells you that the number of posts has been increasing  on the platform.  The\nnumber  of comments  being made on the platform  has also been increasing,  but at a lesser rate, causing\nthe average  number  of comments  per post to be decreasing.  This has been a slow decline  over the last\n6 months, and today’s average comments  per post is down 10% from this time last year. This trend\nconcerns  leadership  because  people engaging  with posts in the comments  section 1s an important  part\nof Instagram,  and helps the product  be more interactive  rather than consumption  oriented.\nStep 2: Hypothesize  Contributing  Factors\nNow that you know what the metric change entails, it’s time to start brainstorming  possible issues\nthat could have occurred  to cause the metric to change. Generally,  contributing  factors fall into four\ndifferent  buckets:\n* Accidental  Changes:  \\s the metric drop even real? Were there any data generation  processes  or\nbugs with instrumentation  and logging  that caused a change that isn’t actually  reflective  of user\nbehavior?\n¢ Natural  Changes:  |s the change simply due to seasonality?  Could the day of the week, or the\nfact that it’s a holiday,  or a change in weather  cause the product  changes  you are seeing?\n¢ Internal  Changes:  \\ssues like new feature launches,  bug fixes, intentional  product  changes,  or\nnew marketing  campaigns  going live can cause metrics  to change.\n¢ External  Changes:  Competitors  launching  new products,  or more macro events such as a\npandemic  or a recession,  can cause shifts in user behavior.\nOne good way to brainstorm  factors,  especially  internal  ones, is to walk up the product  funnel. You\ncan do this by starting  with the feature at hand (local) and working  your way upstream  to broader\nissues that could affect the metric. For example,  in the Instagram  case, we can look at the following:\n¢ Were there any UI changes  to how users can make comments?  Maybe  the comment  composer\nUI got less emphasis?  Or maybe  there 1s stronger  comment  moderation  in place, leading  to more\nautomatically  deleted  comments?\n¢ Were there any changes  to how users can give feedback  on a post (like, comment,  or share)?\nMaybe the UI changed  so that liking or sharing  a post to your story got more emphasis  than\ncommenting  on a post, leading  to cannibalization?\n¢ Were there any changes  to the types of posts in the feed? Maybe there are more ads with\ncomments  tumed  off? Or maybe  there are more reels, which  are more consumption  based,  rather\nthan things people comment  on. Maybe the ranking  model changed,  favoring  posts which are\nmore likely to be liked and shared,  rather  than commented  on?\n* Were there any changes  to the feed itself?  Maybe  comments  and posts are both being  cannibalized\nby stories,  reels, or Instagram  Explore?\nAs you walk backwards  through  the product  funnel, listing a few of the likely culprits  behind the\nmetric change, the interviewer  may stop you and ask you how you'd validate  each hypothesis.\nOtherwise,  stop yourself’ it’s easy to list off 20 potential  reasons  for why something  happened,  but\nyou don't want to overdo it. The meat of these types of problems  is explaining  what metrics  you'd\nlook at to validate  each factor, not how many you can come up with.\nStep 3: Validate  Each Factor\nValidating  hypotheses  means  slicing  and dicing  data into many  different  segments,  hunting  for insights\nthat validate  or disprove  your hypothesis.  For example,  you could slice along user demographics\n244 Ace the Data Science  Interview | Product  Sense"
  },
  {
    "page_number": 257,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\n(1.e., age, gender,  location,  language,  device  type) to see if any of these factors  correlate  to a trend of\nfewer comments  per post.\nYou should  also look at upstream  metrics.  This is where the product  funnel approach  we mentioned\nearlier becomes  applicable,  where you continuously  zoom out until you find something  that can\nexplain  the metric  change.  For the Instagram  example,  metrics  to look at include:\n* Number  of views on the comment  composer  or comments  sections  of a post, to understand  if\nthere is less emphasis  on commenting.\n* Ratio of likes to comments  per post, and ratio of shares to comments  per post, to understand  if\nthis issue is specific  to comments  or a more general  issue with post engagement.\n* Amount  of engagement  per post relative  to trends in engagement  per story and engagement  per\nreel to see if the issue is Instagram  feed losing  engagement  to other features  within the app.\nGenerally,  an interviewer  isn’t expecting  you to ramble  on about every dimension  you'd cut or every\nmetric  you’d check. By prioritizing  which  hypotheses  are most likely based on your product  sense and\nproduct  research,  you can narrow  down the space and focus the conversation  on the most likely culprits.\nStep 4: Classify  Each Factor\nAfter you've  explained  how you’d validate  each potential  factor, the interviewer  will often share the\nresults  of your hypothetical  data analysis.  Based  on this information,  the next step would  be to bucket\neach of the hypotheses  into the following  categories:\n¢ Root cause:  The root cause of the metric  change\n° Contributing  factor:  While not the root cause, still contributing  to the root cause\n¢ Correlated  result: Factors  that are symptoms  of the root cause but not a contributing  factor\n¢ Unrelated  factor:  Factors  that are unrelated  to the metric  change\nFor the Instagram  example,  you find that overall activity  on Instagram’s  feed looks normal;  people\nare stil] viewing  posts, and liking and sharing  posts at the same rate. This tells us the issue is localized\nto comments  — not a more general  engagement  or Instagram  feed issue.\nFrom slicing comments  per post by account  age, you notice that posts made from newer accounts  are\nexperiencing  a sharper  decline  in the average  number  of comments  than posts made from older accounts.\nRunning  cohort analysis,  based on when a poster  joined Instagram,  you are able to confirm  that about\n6 months  ago, the average  number  of comments  made per post by an old user starts to diverge  from a\nnew user. This might give you intuition  that something  changed  for new users about 6 months  ago. From\nauditing  the product  and checking  in with the PMs of the Instagram  onboarding  team, you find out that\nthere was an interstitial  added by the Trust & Safety Team that makes new users aware they can turn off\ncomments  on a post. This feature has led to some percentage  of posts simply having no comments  on\nthem at all. By removing  posts with comments  turned off from the analysis,  you realize there was no\ndecline  in the number  of comments  per post. Boom! You found the root cause for the metric change!\nAssessing  Metric  Trade-Offs\nLauded economist  Thomas Sowell insisted, “There are no solutions,  only trade-offs.”  As a data\nscientist,  you'll be asked to weigh in on tough business  decisions,  where there is no clearly nght\nanswer.  That’s why, to assess your judgement,  you might get asked a question  like “We tested a new\nfeature that shows more ads on LinkedIn.  It increases  revenue  by 1% but hurts time spent by 3% —-\nshould  we ship it?”\nAce the Data Science  Interview 245"
  },
  {
    "page_number": 258,
    "content": "CHAPTER  10: PRODUCT  SENSE\nThe steps to approach  a metric trade-off  problem  are very similar to the steps in the frameworks  for\ndefining  a metric and troubleshooting  a metric change. First, to reason about a trade-off,  you’d need\nto know more details about what the metrics in question  actually  mean. Thus, you follow the first step\nof the metric troubleshooting  framework,  where you clarify what the metric definitions  are (except\nnow you have two metrics  to ask clarifying  questions  about instead  of one).\nThe second major aspect of solving this type of problem is understanding  the product and business\ngoal. By applying  your own research into the company,  along with your intuition,  and then asking\nsmart clarifying  questions,  you can determine  which metrics are more important  than others. By\ndoing so, you can reason on how to proceed with the trade-off.  After collecting  more information\nabout the trade-off,  next steps usually  are either to\n¢ revert the feature change  since the trade-off  is not acceptable,\n* minimize  the trade-off’s  impact by brainstorming  new product  interventions,  or\n* accept the metric trade-off  since it’s justified.\nIt can be hard to give a definitive  recommendation  on what to do, because,  in reality,  you’d be solving\nthis collaboratively  with other stakeholders.  However,  since this is an interview,  the interviewer\nisn't looking for a specific correct answer, but more your thought process. As such, it’s okay to\nexplain in what scenario  or what more information  you’d need to make each of the three different\nrecommendations.\nFor the LinkedIn  example,  you could mention that depending  on how hard or easy it was to get\na 1% revenue  gain or a 3% engagement  gain from other features,  you’d know what to do. This is\nreasonable,  since maybe |% revenue  gains are easy to achieve  through  more sophisticated  means  (via\nbetter targeted  ads), whereas  a 3% engagement  drop is very hard to recover  from and could unwind\nprogress  in other strategic  areas for the business.  You could also mention  how maybe the decrease\nin lume spent is mostly from users who are getting  poor quality  ads, and that there should  be product\nfeatures  like “hide this ad” or “block this advertiser”  that can minimize  the engagement  drop while\nstill maintaining  the revenue  gain.\nA/B Testing  & Experimental  Design\nWhile the mathematical  underpinnings  of A/B testing were discussed  in “Chapter  6: Statistics,”  in\nthis chapter  we dive into the nuances  of real-world  A/B testing, because  it’s brought  up in many\nproduct  data science  interviews.  Usually,  at the end of a success  metrics  definition  problem,  you'll\nbe asked, “How would you test this new feature?”  Interviewers  will ask you to walk through  the full\nexperimental  design setup for a hypothetical  test. Often, interviewers  steer the conversation  towards\none of the many practical  testing  pitfalls  you may face.\nExperimental  Design  Setup Overview\nWhen you're faced with a general A/B testing question  like “How would you A/B test a 1-click\njob-apply  feature  on LinkedIn?”  it can be challenging  to not ramble  aimlessly.  To keep your answer\nfocused,  make sure to address  the four main steps of setting  up an experiment:\nI, Pick a Metric to Test: While you'll be tracking  a whole host of core and guardrail  metrics,\nnarrow  each experiment  down to a few key metrics  that capture  the essence  of the goal of testing\nthe feature  change.  Don’t cheat by cherry-picking  a metric  post-hoc  based on whatever  ended up\nbeing statistically  significant!\n246 Ace the Data Science  Interview | Product  Sense"
  },
  {
    "page_number": 259,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\n2. Define  Thresholds:  Decide  ona particular  statistical  significance  level (alpha)  which  is generally\nset to 0.05, as well as a power  threshold  (1 — beta), which is generally  set to 0.8. The value for\nthe power  is dependent  on the minimal  detectable  effect (MDE)  and is usually  set by consulting\nwith stakeholders.  That is, we can calculate  the minimal  detectable  effect (MDE)  at x% power\n(for example  0.8 MDE for 80% power)  for any given sample  size and sample  variance.\n3. Decide on Sample  Size & Experiment  Length:  Based on the MDE and power, along with\nthe metric variance,  one can calculate  the required  sample  size needed  for the test. Using this\nrequired  sample  size, the length of the experiment  can be determined  based on the daily traffic\nof the feature  in question.  A good rule of thumb,  though,  is to run a test for at least two weeks,  to\naccount  for day-of-week  effects  typical  in most consumer  products.\n4. Assign Groups:  When deciding  the control  group and treatment  group, we want to randomize\nthese groups  sufficiently;  otherwise,  there will be confounding  variables  down the line. At large\ncompanies,  this consideration  is often abstracted  away for you by the A/B testing  infrastructure.\nReal-World  A/B Testing  Considerations\nLet’s face it: in the messy real world, product  and business  constraints  can get in the way of proper\nexperimental  design.  As such, data science  interviews  often touch on the A/B testing  pitfalls  you’re\nlikely to encounter  in practice,  and how you'd guard against  them. Unless  you’re  a seasoned  product\ndata scientist  with many battle scars from A/B tests gone bad, pay careful  attention  to the A/B testing\nnuances  below.\nWhen Not to A/B Test\nSure, A/B testing is essential  to businesses  like Facebook  and Amazon.  But night off the bat, we\nneed to acknowledge  that A/B testing isn’t always  the correct  answer  to every product  and business\nproblem  that arises. As 16th century  British  playwright  and data scientist  William  Shakespeare  once\nsaid, “To A/B test, or not to A/B test, that is the question.”\nSome scenarios  where  A/B tests typically  shouldn’t  be run:\n¢ Lack of infrastructure:  Having  the data engineering  infrastructure  needed  to reliably  test isn’t a\ntrivial matter;  keep this in mind when interviewing  at smaller  companies  that might not have the\ndedicated  resources  to perform  complex  A/B tests.\n¢ Lack of impact:  Don’t test things that don’t matter. Try to size up the opportunity  -— if the test\npans out, how much impact on the business  would you expect? Is this benefit much larger than\nthe time and engineering  resources  you’d have to spend on the test?\n¢ Lack of traffic:  Without  enough  people using a feature or performing  a certain action, it can be\ndifficult  to make  a statistically  sound conclusion  within a reasonable  time frame.\n¢ Lack of conviction:  For high-traffic  and high-value  features like the Amazon “Buy Now”\nbutton, it might be worth trying out 60 variants  of different  copy, color, size, and iconography.\nBut, at some level, conviction  and intuition  behind why a variant would be the winning  option\nis crucial —- especially  if you are operating  on a much smaller scale than Amazon. While a\nrandom  monkey-throwing-darts  approach  might work for stock-market  investing,  it won't cut it\nfor making  great products!\n¢ Lack of isolation:  How would you A/B test a logo? It’s not easy to have a “control”  group. since\nthese changes make the headlines  and get rolled out to every user. In these cases, qualitative\nmethods  like customer  interviews  and focus groups can work better.\nAce the Data Science  Interview 247"
  },
  {
    "page_number": 260,
    "content": "CHAPTER  10: PRODUCT  SENSE\nIn cases where A/B testing is not useful, we can do the following:\n* Conduct user experience  research via focus groups and surveys to understand  what options are\nbetter.\n¢ Analyze  user activity  logs to get a better sense of what option is a better fit.\n* Make the product change, but then run a retrospective  analysis by looking at historical  data to\nsee if the metric that we are interested  in responds  as we expect.\nDealing  with Non-Normality\nAs mentioned  earlier, in the statistics chapter, A/B tests are just dressed up Z and t-tests. These\nstatistical  tests assume  that the distribution  of the random  variable  of interest  (the metric we are testing\nfor) is normally  distributed.  At larger tech companies,  this assumption  tends to hold, thanks to plentiful\nuser data and the Central Limit Theorem.  But what happens  if this isn’t the case for some reason?\nSeveral  methods  exist to deal with non-normality  of the test metric:\n¢ Bootstrapping:  The process of generating  extra samples randomly  for each variant, and then\naveraging  results at the end, in order to invoke the Central  Limit Theorem.\n¢ Running  alternative  tests: The Wilcoxon  rank-sum  test is a popular  alternative  to the t-test and\ndoes not assume  normality.\n¢ Gathering  more data: With budget and time constraints  permitting,  collecting  more data can\ngive you more confidence  that what is being measured  is more representative  of the true effect\non the population  at hand.\nDealing  with Multiple  Tests Simultaneously\nRecall the multiple  testing  problem  we covered  in the statistics  chapter  —- if you run 100 A/B tests at\nthe same time, by pure chance,  you're  bound  to have some test succeed  with  a statistically  significant\nresult. In interviews,  you'll be expected  to have a high-level  understanding  of why this problem\nexists and how to deal with it.\nOne way to account  for the multiple  testing  problem  is to use Bonferroni  Correction,  which adjusts\nthe significance  level required  based on the total number  of tests running.  Alternatively,  you can\ncontrol the false discovery  rate (FDR) or the familywise  error rate (FWER).  Recall from the ML\nchapter  false positives  (FP) and true positives  (TP). The FDR is equal to FP / (FP + TP) and, hence,\nis the rate of type I errors during multiple  testing.  In a similar  vein, FWER  is simply  the probability\nof making  one or more type I errors when performing  multiple  tests.\nWhile  statistical  approaches  are helpful,  the reality of the situation  is that experiments  will unfortunately\nalways interact  to a degree.  Often, the best you can do when faced with surprising  or weird results,  is\nto dig into the active experiments  and see if anything  could have impacted  the primary  experiment.\nDealing  with Network  Effects\nGenerally,  it 1s assumed  that during  an A/B test, each user is selected  to either  the control  or treatment\ngroup at random,  that each user is independent,  and there is no interference  between  the contro!  and\ntreatment  group. However,  this condition  doesn’t  always  hold in practice.\nConsider  a social network  like Facebook  -- the actions  of users are likely impacted  by that of those\naround them (network  effect).  Therefore,  it will likely be the case that the behaviors  of those in the\ncontrol  group are being influenced  by behaviors  of those in the treatment  group;  hence,  the true effect\nis not exactly  as is stated.\n248 Ace the Data Science  Interview  | Product  Sense"
  },
  {
    "page_number": 261,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nAs a concrete  example,  say Facebook  is testing out a new kind of Facebook  Live, which ends up\nbeing super entertaining.  The test group might see a large increase  in engagement  metrics  due to the\nnew feature,  but their increased  engagement  on the platform  overall  may drive them to also interact\nwith their friends  in the control  group more. This spillover  can lead to increased  engagement  for the\ncontrol  group, which causes  the overall  positive  effect of the experiment  to be understated  because\nof the control  group contamination.\nOne possible  way to control for network  effects is to create clusters  of similar  or connected  people\nand divide  these sub-networks  into control  and treatment  groups  for better isolation.  Mathematically,\nthis separation  can be done through  various graph partitioning  methods  (for example,  normalized\ncuts) that place users into various  groups  based on social interactions.\nDealing  with Novelty  Effects\nA/B tests may have an exaggerated  initial effect,  due to the novelty  effect,  where  a new feature  attracts\nsocial media  attention  and PR hype, causing  inquisitive  users to rush to check out the new changes.\nThis flock of curious users leads to metrics like time spent and engagement  to jump spuriously\nhigh. For example,  when Facebook  first launched  emoji reactions  like “haha” and “angry,”  post\nengagement  massively  spiked  due to curious  users trying  the feature  out. Eventually,  though,  reaction\nrates came back down and stabilized  at a new, slightly  higher,  baseline  rate.\nIn the opposite  direction,  there could be a primary  effect, where users are fixed on what is familiar\nand have an aversion  to changes  in general.  For example,  when Facebook  originally  launched  News\nFeed in 2006, it was reviled by its then 8-million  student users. To get a sense of the backlash,\nconsider  this — a Facebook  Group called “Students  against Facebook  News Feed” grew to 750k\nmembers  in just two days. Ten percent  of the entire user base was upset enough to join a group\nboycotting  the news feed. We know how that experiment  turned  out!\nTo detect primary  and novelty  effects, you could analyze  the test results for new users only — if\nthere is a statistically  significant  change there, but not for the old users, novelty  effects are likely\nat play. Similarly,  to guard against the novelty  effect, you can always  just run the A/B test on new\nusers. However,  this opens up its own can of worms, if new users aren’t similar to existing  users.\nFor example,  at Facebook,  back when Nick was on the New Person Experience  team, most new\npeople came from developing  countries  like India, Indonesia,  and Nigeria. Usage patterns didn’t\napproximate  well to the average  tenured  user of Facebook,  who resided  in a more developed  country.\nNuances  of Taking  Action  on A/B Test Results\nIf you ran an A/B test, managed  to avoid all the common  experimental  design pitfalls,  got a positive\nresult, and p < 0.05, you’d definitely  launch it, right? Maybe.\nJust because  a test reaches statistical  significance  doesn’t mean it’s automatically  shipped.  At large\ntech companies  that run experiments  on hundreds  of millions  of users, it’s easy for small differences\nto become detectable.  So even with a small p-value, it’s crucial to assess the effect size before\nshipping  the change.\nIt’s not just the direct effect size to consider  — what happened  to the counter  metrics and guardrail\nmetrics you’d set up? For example,  if revenue went up, but retention  went down, it’s not obvious\nwhether to ship it or not. As we mentioned  earlier, it’s best to confer with product and business\nstakeholders  when making  these metric trade-offs.\nAnother consideration  to factor in is that any time you ship a change, there is human labor cost\nassociated  with it. There’s  always a cost of properly  deploying  and then supporting  the feature change\nAce the Data Science  Interview 249"
  },
  {
    "page_number": 262,
    "content": "CHAPTER  10: PRODUCT  SENSE\n— for example,  increased  customer  support  tickets from users who are confused  by a new experience.\nAnd then there’s the chance of hidden bugs in the new feature you’re launching  that didn’t get caught\nin the test. You need to make sure the impact from the A/B test justifies  launching  the new variant.\nLaunch  with A/B Test Holdouts\nSuppose  you ran a successful  A/B test and stakeholders  have given you the green light to roll out\nthe new feature. Typically,  even after launch, there remains  a small group of users —— usually  Just a\nfew percent --- who don’t receive the new experience.  This group is known as the A/B test holdout.\nBecause  A/B tests are typically  run over a shorter  time period, holdouts  help you quantify  the long-\nterm lift from shipping  features.  This makes identifying  potential  novelty  effects,  where metrics  tend\nback towards  their original  baseline  over time, much easier.\nFor a practical example of how holdouts  are used, when Nick was a part of Facebook’s  Growth\nEngineering  division,  his team would create a shared  holdout  group every quarter.  By having  the entire\nteam’s launched  features  not affect a tiny portion  of users, the Head of Growth  could easily measure\nthe team’s  combined  quarterly  output,  which was useful come performance  review  and promotion  time.\nNote that not every A/B test merits  a holdout.  For bug fixes or very sensitive  changes,  it can be better\nto launch the feature to the entire user base. For example,  when Kevin was working  on Facebook\nGroups,  implementing  a holdout  for a new model that flagged  child trafficking  would  mean that some\nsmall number  of holdout  users would  still see such content.  In these scenarios,  after getting  a signal\nthat the new model was helpful without  too much downside  risk, we'd say, “F*ck it, ship it!” and\nlaunch  the model to 100% of users without  any holdouts.\nProduct  Questions\n10.1. Facebook:  Imagine  the social graphs for both Facebook  and Twitter.  How do they differ?\nWhat metrics  would  you use to measure  how these social graphs  differ?\n10.2. Uber: Why does surge pricing exist’? What metrics  would you track to ensure that surge\npricing  was working  effectively?\n10.3. Airbnb:  What factors  might make A/B testing  metrics  on the Airbnb  platform  difficult?\n10.4. Google:  We currently  pay the Mozilla  foundation  9 figures per year for Google  to be the\ndefault  search engine  on Firefox.  The deal is being renegotiated,  and Mozilla  is now asking\nfor twice the money.  Should  we take the deal? How would  you estimate  the upper  bound  on\nwhat Google  should  be willing  to pay?\n10.5. LinkedIn:  Assume  you were working  on LinkedIn’s  Feed. What metrics  would you use to\ntrack engagement?  What product  ideas do you have to improve  these engagement  metrics?\n10.6. Lyft: Your team is trying to figure out whether  a new rider app with extra UI features  would\nincrease  the number  of rides taken. For an A/B test, how would you split users and ensure\nthat your tests have balanced  groups?\n10.7. Amazon:  If you were to plot the average  revenue  per seller on the Amazon  marketplace,\nwhat would the shape of the distribution  look like?\n10.8. Facebook:  Besides  posts Facebook  is legally  obligated  to remove,  what other types of posts\nshould  Facebook  take down? What features  would you use to identify  these posts’?  What are\nthe trade-offs  that need to be considered  when removing  these posts?\n250 Ace the Data Science  Interview  | Product  Sense"
  },
  {
    "page_number": 263,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\n10.9. Amazon:  The Amazon  books  team finds that books with more complete  author  profiles  sell\nmore.  A team implements  a feature  which  scrapes  Wikipedia  and Goodreads  to automatically\nfill in more information  about authors,  hoping to see an improvement  in sales. However,\nsales don’t change  — why might  this be?\n10.10. Snapchat:  Let’s say Snapchat  saw an overall  5% decrease  in daily active users, a trend that\nhad been consistent  over the week. How would you go about determining  the root cause of\nthis?\n10.11. Pinterest:  Say you ship a new search ranking  algorithm  on Pinterest.  What metrics  would\nyou use to measure  the impact  of this change?\n10.12. Netflix:  Say a given category,  such as sci-fi TV shows,  has less total watch time, compared\nto other similar  categories.  What metrics  would  you look into to determine  if the problem  is\nthat people  aren’t interested  in that category  of content  (demand  problem),  or if the category\nhas interest  but the content  is bad (supply  problem)?\n10.13. Apple: Say you have data on millions  of Apple customers  and their purchases  made at\nphysical  Apple retail stores. How could customer  segmentation  analysis  increase  a store’s\nsales performance?  What techniques  would you use to segment  brick & mortar  customers\ninto different  groups?\n10.14. Facebook:  If 70% of Facebook  users on 1OS also use Instagram,  but only 50% of Facebook\nusers on Android  also use Instagram,  how would you go about identifying  the underlying\nreasons  for this discrepancy  in usage?\n10.15. Capital  One: How would  you assess the stickiness  of the Capital  One Quicksilver  credit card?\n10.16. Google:  Say you worked  on YouTube  Premium,  which 1s an ad-free version  of YouTube\nbundled  with YouTube  Music -— a music streaming  service.  You’re launching  the product\nin a few new countries  — how would  you determine  pricing  for each country?\n10.17. Twitter:  Should  Twitter  add Facebook-style  emoji reactions  (love, haha, sad, angry, etc.) to\ntweets?\n10.18. Slack: What metrics  would you use to measure  user engagement  at Slack? How would you\nbe able to tell early whether  or not user engagement  was declining?\nProduct  Solutions\nSolution  #10.1\nA bad answer  glazes past the nuances  of the social graph and jumps straight  into defining  metrics.  For\nclarity — not just for you, dear reader, but also for the hypothetical  interviewer  asking this question,\nwe’ll structure  our answer  into three steps.\nSten 1: Explaining  User Behavior  on Each Platform\nBefore explaining  how the social graphs of Facebook  and Twitter  differ, it’s crucial to first consider\nhow each platform’s  average  user interacts  with their respective  platform.  Facebook  ts mostly about\nfriendships,  and so two users on the same social graph are mutual friends. Twitter, on the other\nhand, is more focused  on followership,  where one user typically  follows another (who 1s usually an\ninfluential  figure) without getting a followback.  Thus, Twitter likely has a small number of people\nwith very large followerships,  whereas,  on Facebook,  that pattern appears less often.\nAce the Data Science  Interview 251"
  },
  {
    "page_number": 264,
    "content": "CHAPTER  10: PRODUCT  SENSE\nStep 2: Describing  the Social Graph and Its Differences  Between  Facebook  and Twitter\nModeled as a graph, let’s say each user 1s represented  as a node, and the edge linking two nodes\ndenotes  a relationship  (typically,  friendship  on Facebook  and fellowship  on Twitter) between  the two\nusers whose nodes the edge connects. Most nodes on Twitter would have low degrees, but a small\nnumber of nodes (those of influential  people) would have very high degrees, resulting  in a “hub-and-\nspoke” social graph for that platform.\nStep 3: Defining  Metrics  to Measure  the Social Graph\nOne way to quantify the difference  between the two platforms’  social graphs is by looking at the\ndistributions  of friendships/followerships  represented  by the social graphs of each platform’s  typical\nusers. Because a typical node’s degrees — that is, the number of connections  it has to other nodes\n— should capture the difference  in these platforms’  social graphs, one concrete metric would thus\nbe the average degree among all nodes on each platform. Alternatively,  to obtain an even more\ndetailed understanding  of the differences  between the two social networks,  we could construct  box-\nand-whisker  plots of the platforms’  degrees  among all their respective  nodes.\nStill another way of looking at the two graphs would be to check the distribution  of degrees across\nall nodes in each platform’s  network. In all likelihood,  the Twitter  distribution  would show  a greater\namount of right skewness  than that of Facebook. Metrics that quantify a distribution’s  skewness  or\nkurtosis  could thus be used to describe  the difference  between  the two platforms’  degree distributions\nand, hence, social graphs.\nSolution  #10.2:\nFor any metrics definition  question, it’s important  to first explain the business  goal of the product\nor feature and then explain the related stakeholders,  before ultimately  landing on good metrics to\nmeasure  the success  of that feature.\nStep 1: Explain  Uber’s Motivation  for Surge Pricing\nYou don’t have to be an econ major to realize that surge pricing is about fixing imbalances  between\nsupply and demand.  In the case of Uber, such an imbalance  could result from either a lack of drivers\nor an excess number  of potential  riders. Therefore,  surge pricing’s  goal would be to increase  supply\nby enticing  more drivers to use the app through increased  pay, and reduce demand  by raising prices\nfor riders.\nStep 2: Consider  Stakeholders  Related  to Surge Pricing\nA nuanced  answer  would consider  the various  stakeholders  involved  in surge pricing,  beyond  just the\nimmediately  obvious  drivers and riders. For example,  a good candidate  would mention  associated\nbusiness  functions  within Uber that could be affected  by the surge pricing algorithm  not working\neffectively.\nStep 3: Define Metrics  & Counter  Metrics  for Surge Pricing\nSurge-specific  metrics are the duration  of the surge, the surge pricing multiplier,  and the number\nof mders and drivers in the affected  area. We should also track the following  metrics  during surge\nperiods: number  of rides taken. number  of rides cancelled,  total revenue  made for both Uber and\nUber’s drivers. total profit made by both Uber and Uber’s drivers,  and the average  ride wait time.\nThese are all standard  metrics,  but critical to monitor  to ensure the business  is healthy  during surge\nperiods.\n252 Ace the Data Science  Interview | Product  Sense"
  },
  {
    "page_number": 265,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nIn addition,  topline  metrics  like user’s lifetime  value (LTV),  driver  retention,  rides taken, daily active\nriders, and drivers  should  also be tracked,  so that we can be sure surge pricing  isn’t having  adverse\nimpacts  on the business  overall.\nAs with any good metrics  definition  question,  a discussion  on counter  metrics  is important.  Even if\nthe surge pricing  is bringing  in extra money,  one counter  metric  to implement  would  be net promoter\nscore (NPS). Surge pricing  can annoy users, for whom frequently  fluctuating  sky-high  prices can\nbe a source of frustration.  And then there’s the potential  for mistakes,  or users in a less-than-sober\nState accidentally  making  a purchase  (we authors  can neither  confirm  nor deny that $158 Uber from\nSan Francisco  to Palo Alto after a fun night out). It can even be a PR risk. Like clockwork,  every\nNew Year’s,  there’s  a news story about someone  getting  drunk and taking  an $800 Uber by accident.\nBetween  bad PR, frustrated  users, and potentially  increased  support  tickets,  some metric  to make sure\nit’s a quality  program  is key.\nSolution  #10.3:\nA good answer  would  demonstrate  not just thorough  A/B testing  knowledge,  but some understanding\nof the Airbnb  product.  Also, to demonstrate  your problem-solving  attitude,  it can be a wise move to\nnot just mention  the difficulties,  but briefly also mention  techniques  to deal with these A/B testing\nproblems.  Finally,  to earn bonus points on this problem,  remember  to relate your own A/B testing\nwar Stories  if you think it would  also be relevant  to Airbnb.  By coloring  your answer  with your own\nexperience,  you’re demonstrating  your time spent in the trenches,  which can help separate  you out\nfrom the more green candidates.\nIssue #1: Complexity  of User Flow\nTesting  Airbnb platform  metrics would be difficult  because  of the complexity  of the company’s\nbooking  process,  which starts with a user search and often requires  user-host  communication  before\na booking  can be finalized.  Alternatively,  a user can sometimes  book a rental without having to\ncontact the property  host at all. Also, adding to the complexity  of conducting  A/B tests on Airbnb\nplatform  metrics, booking flows frequently  depend on factors outside of Airbnb’s  control, such\nas host responsiveness  to messages  lett by prospective  renters. Therefore,  since a booking  can be\ninstantaneous  or a long, drawn-out  process,  timeboxing  an experiment  could be difficult.  Because\nof these issues, any data generated  by tests on the booking  process  would most likely be quite noisy.\nTo mitigate these issues, we want to make sure we are looking at the correct non-intermediary\nmetrics.  For example,  there could be a few steps in between  searching  and booking,  but the searching\nto booking  conversion  rate should be the main metric.  Additionally,  we want to employ  best practices\non managing  the data generation  and collection  process (logging and other downstream  event\ncollection).\nIssue #2: User Bucketing  Due to Multiple  People & Devices  in Booking  Flow\nA second source of complexity  arises because planning  a vacation  often involves multiple  people.\nSince even a single person could employ multiple devices during the booking process, it could\ninvolve multiple and discontinuous  uses of the Airbnb platform from different IP addresses  and,\nto further complicate  things, occur over an extended  time period. Ideally, there would be a clear\none-to-one  user-to-device  mapping, and, also ideally, the booking process would occur nearly\ninstantaneously.  In that case, testing would be easy since the variables  being tested tor a specific\nuser (e.g., demographics,  booking  details, time needed to book, and so on) could be clearly identified\nand then measured.  However,  different members  of a user group (e.g., a family) could be involved\nAce the Data Science  Interview 253"
  },
  {
    "page_number": 266,
    "content": "CHAPTER  10: PRODUCT  SENSE\nin different  parts of the booking  process,  or a single user could employ different  devices during the\nprocess and might not be logged in to any of these devices during some parts of the process. Then,\nthe correct user profiles  of each would need to be determined  during each contact  in order to correctly\nidentify  them and, consequently,  the correct  A/B test group(s)  to which they belonged.\nThere are a number  of ways to address these various issues. For example,  doing extra checks with\ninternal and potentially  external datasets (which various vendors can provide) could help address\nthe device-mapping  issue. Note that data cleaning  can have nuances  -—- for example,  in the cases of\nmultiple  devices, it may be the case that the user deletes their cookies,  which is a useful signal in\nitself that may be easily imputed  or predicted.  In such cases, where there is missing  user information,\nthe best approach  is to see if you can use other variables  to try and predict  the missing  information.\nIssue #3: Long Time Horizon  for Measuring  Success\nLastly, successful  consumption  of a use of Airbnb’s  services  — a happy stay at an Airbnb  listing —\nhappens  over a much longer time horizon  than, for instance,  use of social media (where consumer\nenjoyment  of the service is instantaneous).  This delay makes it difficult  to accurately  measure  the\ninfluence  of various features of Airbnb’s  service through  calculation  of various success metrics,\nwhich can be done only much later. Plus, these are low-frequency  events  — it’s hard to know if there\nwas a statistically  significant  increase  in bookings  if the majority  of users don’t even make a booking\nin a given month.\nAs an example,  consider  measuring  longer-term  metrics  such as user retention  or customer  lifetime\nvalue. Since A/B tests cannot  be run for many months,  we need to find a shorter-term  proxy for such\nlonger-term  metrics. A machine  learning  approach  works well here — for example,  using various\nfeatures  to predict retention  or customer  lifetime  value, and choosing  any of the important  features\ncorrelated  with the target metric that can be measured  on a shorter-term  basis. Then, on the A/B test,\nwe want to simply  see the expected  moves  on these important  features.\nSolution  #10.4:\nAt first, it may seem that Google’s  search market  share goes unchallenged.  You might think, “If we\ndon’t make a deal with Firefox,  users would still default  to Google  on their own, because  what are\nthey going to do, Ask Jeeves?”  However,  to answer  this question  well, it’s important  to first fully\nexplain  the business  motivations  for why Google  wants to remain Firefox’s  default  browser.  Based\non these business  considerations,  we can then specify  the metrics  Google  should  use to price the deal.\nStep 1: Explaining  Google’s  Immediate  Motivation  to Be Firefox’s  Default\nGoogle's  advertising  revenue,  most of which comes from search ads, makes up more than 80%\nof Alphabet’s  revenue.  Clearly there’s a lot of money at stake when it comes to search. However,\nGoogle's  motivations  for closing  a deal go beyond  the immediate  profit that’s at risk.\nFor example,  Google  likely has a business  goal of beating  competitor  search  engines  and making  sure\nthey stay beat. That’s  because  some percentage  of users probably  don’t care about what search  engine\nthey use and simply  stick with the default  one. Plus, Firefox  defaulting  to a competitor  like Bing just\nmight be the spark that Microsoft  needs to invest more heavily  in challenging  Google’s  monopoly\non search. In the case that Firefox defaulted  to DuckDuckGo,  there would be additional  brand\nramifications  to deal with too. Because  Firefox  positions  itself  as a more privacy-aware  browser,  and\nDuckDuckGo’s  brand is built on protecting  a searcher’s  privacy,  a vote of confidence  by Firefox  in\nDuckDuckGo  could bring Google  privacy  concems  to the forefront,  fueling  a negative  news cycle\nand ultimately  hurting  Google’s  brand reputation.\n254 Ace the Data Science  Interview  | Product  Sense"
  },
  {
    "page_number": 267,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nGoogle’s  not just worried  about beating  direct  competitors  — vertical  search  engines  such as Amazon,\nExpedia,  and Yelp compete  heavily  with Google on specialized  searches.  Google  boosts its own\nancillary  services,  like Google  Shopping  and Google  Maps, in search  results.  That means if Firefox\nwere to default  to a competitor,  Google  Shopping  couldn’t  benefit  from its top-of-page  position  in\nGoogle,  which  would  cause them to cede market  share to Amazon.\nSimilarly,  if Firefox  switched  from Google  to Bing, searching  for a nearby  restaurant  wouldn’t  result\nin a Google  Maps listing,  but instead,  might take a user to a Yelp page. Clearly, a whole ecosystem  of\nproducts  and business  lines would be adversely  affected  if Google  isn’t able to be the default  search\nengine  on Firefox.\nStep 2: Explaining  Google’s  Secondary  Benefits  from Being Firefox’s  Default\nThere’s  also network  effects to consider  as well. Google’s  search relevance  algorithm  takes into\naccount  how users interact  with their search results. This means the more people who use Google\nSearch,  the better the product  becomes,  creating  a positive  feedback  loop of user engagement  and\nproduct  improvement.  Furthermore,  these network  effects  extend  to products  downstream  of Google\nSearch, like Google Reviews  and Google Maps. More people searching  on Google means more\npeople  reviewing  businesses  on Google,  which helps Google  Maps compete  better against  Yelp and\nApple Maps in the lucrative  local search  market.\nIn considering  the whole ecosystem  of products  that Google  Search  leads to, along with the multitude\nof business  Google  competes  with, it’s clear Google’s  motivations  to be the default  search engine  on\nFirefox  go far beyond  the immediate  search ads revenue  at stake.\nStep 3: Metrics  Used to Inform  Google’s  Willingness  to Pay\nA lazy way of setting  the price is by assuming  what Google  paid previously  to Firefox  (~$450  million\nin 2020) was fair, and adjusting  it slightly  according  to the change  in Firefox’s  install base. A similar\napproach  could be to price this deal relative  to the deal Google  has with Apple, where it pays ~$10\nbillion a year to be Safari’s  default search engine, and scaling the price to Firefox’s  market share.\nDepending  on the interviewer,  these answers  can be seen either as clever or missing  the point of the\nproblem,  so tread carefully!\nFor an answer  based on first principles,  you could Jook at the amount  of search ad revenue  Google\ngains from all Firefox users. If this segment  completely  stopped searching  on Google, how much\nrevenue  would  be lost? This could be one way to price the deal, but it assumes  the worst-case  scenario:\nthal everyone  wouldn't  use Google anymore  if it wasn't the default. To make the estimate  more\nrealistic,  you could look at other browsers  where Google isn’t the default and see what percentage\nend up using Google.  This way, you can get a more reasonable  estimate  of how much revenue  you'd\nstand to lose, and then determine  the price based on this number.\nInstead of considering  direct search ad revenue, we could also do a similar analysis,  except base\nour bid on the total revenue generated  from Firefox users. This number would account for all the\ndownstream  ways Google  makes revenue  from a Firefox user, and thus better account for the second\norder effects of weakened  market share amongst  Firefox users.\nSolution  #10.5:\nBefore jumping into defining metrics and brainstorming  product improvements  (like forcing all\nLinkedIn users to follow linkedin.com/in/nipun-singh/  and linkedin.conv/in/kevin-huo/  for career\nadvice),  it’s best to start by explaining  the goal of LinkedIn’s  Feed.\nAce the Data Science  Interview 255"
  },
  {
    "page_number": 268,
    "content": "CHAPTER  10: PRODUCT  SENSE\nStep #1: Explain  Why Linkedin’s  Feed Exists\nLinkedIn’s  mission is to connect the world’s professionals  to enable them to be more productive  and\nsuccessful  in their careers. The newsfeed  helps fulfill this mission  by helping users keep tabs on their\nprofessional  network,  stay up-to-date  with industry  news, connect  with new people through  engaging\ncontent,  and more.\nFrom  a business  perspective,  newsfeeds  (not just at LinkedIn,  but other social media companies  as\nwell) tend to be very engaging  products.  For LinkedIn,  the feed ensures people keep checking  the\nproduct  often, which is crucial,  since without  the feed, the product  doesn’t  hold much utility for users\nnot actively  job hunting  or networking.  By helping  to keep users on the platform,  LinkedIn  is able to\nmake more money  through  displaying  ads and sponsored  jobs.\nStep #2: How LinkedIn  Can Measure  Feed’s Engagement\nFor a product  as expansive  and critical as its feed, LinkedIn  needs to track a whole host of metrics.  A\nfew top-level  engagement  metrics  include  daily active users, weekly  active users, and monthly  active\nusers on Feed. You could also track L7 and L28 (how many days in a week or month do users check\nthe feed). To add a notion ofduration  to these visits, you can also track average  user session  time on\nFeed, and average  daily, weekly,  and monthly  time spent on the feed.\nHaving users log on frequently  and not engage with anything  doesn’t  help LinkedIn’s  product  goal\nwith feed. To measure  the depth and quality  of engagement,  we can track important  user actions  taken\non feed. You could track the number  of posts seen, posts liked, posts commented  on, and posts shared\nper month. To make it easier to report on, you could combine  all the activity into a single score.\nHowever,  not all post engagement  is equal. By weighting  the value of a post view vs. post like vs. post\ncomment  differently,  you can more accurately  capture  post engagement  activity  using  just one metric.\nYou could also make a similar  metric to measure  content  creation,  which incorporates  the number  of\nposts made, comments  made, and posts shared.\nBecause  LinkedIn  Feed is so closely  tied to a significant  source  of revenue  —- ads —- you should  also\nseparately  track engagement  on ads in Feed. Metrics  like ad impressions  and ad clicks in Feed could\nbe bundled  under the umbrella  of feed engagement.\nLastly. for all of these metrics,  we want to make sure we are measuring  genuine  engagement  — not\nthe results of automated  spam or scraping  bots, which inflate activity  metrics.\nStep #3: Ways to Improve  Engagement  on Linkedin’s  Feed\nAs mentioned  in the intro section  on doing your homework  before the interview,  hopefully  you’ve\nused the product a bit and had a chance to look at competitors.  That can make tackling  product\nbrainstorm  questions  easier since you’ve  scoped  out the competition  and can “borrow”  ideas, much\nlike Facebook  and Instagram  love to borrow  from Snapchat.\nTo improve  these metrics, LinkedIn  would want to incentivize  people to stay engaged.  Example\nfeatures  that boost engagement  include personalizing  the News Feed to the user and encouraging\npeople  to post more to keep the news feed fresh. Specific  ways to achieve  these goals would include\ncreating  up-to-date  ranking  models  that accurately  rank how likely a LinkedIn  user is to consume\nand engage with newsteed  content  or adding some new, highly requested  reaction  type. Enabling\nnew post types, like LinkedIn  Live streams  or LinkedIn  Stories  can also aid in keeping  the content\ninventory  engaging.\nAnother  way to improve  metrics  would be to build a model using features  that you believe  would\naffect the metric. Generally,  this will be a combination  of user data (demographics)  and event data\n256 Ace the Data Science  Interview | Product  Sense"
  },
  {
    "page_number": 269,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\n(browsing  behaviors  and session  events).  Decision  trees or random  forests  can be useful here, as they\ntend to have high accuracy  and also can easily display  feature importance.  After assessing  model\noutputs,  you can determine  factors contributing  to the target metric and decide on an action plan\naccordingly.\nEach of the features  suggested  above  can be A/B tested  against  core engagement  metrics  to determine\nif they would  drive increased  engagement  on the newsfeed.  Since other metrics  would likely also be\naffected  (for example,  with a large increase  in time spent and news accessed,  more bad content  would\nalso be consumed),  it is paramount  that such A/B testing  be evaluated  holistically  and potential  trade-\noffs kept in mind. Note that, overall,  this process  implies  that improving  metrics  improves  what you\nintend  to measure  (i.e., that metrics  drive behavior)  rather  than the other way around,  which can have\nsome consequences.  As we painfully  learned  firsthand  running  A/B tests at Facebook,  sometimes  the\nmetrics  measured  don’t accurately  reflect  true user behavior  and sentiment.\nSolution  #10.6:\nA good answer  would  walk through  the basics of user assignment  and address  basic pitfalls  the Lyft\nrider app may face in A/B testing.  A great answer  would  also mention  advanced  issues that can occur,\nlike network  effects,  and how doing geo-based  randomization  can help keep groups balanced.  For\nbonus points,  we also explain  ways to quantitatively  prove that our groups  are, in fact, balanced.\nStep 1: A/B Testing  Basics  for Lyft’s Rider  App\nSince we want to quantify  whether  and to what extent instituting  a change  in how Lyft operates  (in\nthis case, adopting  new UI features)  would improve  a metric of interest  (number  of rides taken),  the\nmost feasible  strategy  1s to conduct  an A/B test, in which users are divided  into two groups — one\nexposed  to the change,  and one not exposed  to It.\nHowever,  we can’t just arbitrarily  split Lyft riders into two groups  with one having  the new UI and\nthe other having the old version;  splitting  the data haphazardly  in that way could, and most likely\nwould, cause the demographics  of one group to differ greatly from that of the other. This would\nintroduce  a source  of variability  in the outcome  variable  not related  to the UI change  and would most\nlikely skew the distribution  of the dependent  variable  being measured  (1i.e., number  of rides taken).\nTherefore,  we would have to choose A users and B users so as ideally to balance  the groups with\nrespect to such user characteristics  as demographics,  locations,  etc. Employing  stratified  random\nsampling  would provide  the best means of ensuring  homogeneity  of groups.\nStep 2: Accounting  for Network  Effects  with Geo-Assignment\nWe also need to take into account  marketplace  dynamics.  Consider  any location...say  New York City.\nIf we give half of the riders the new features  and keep half of the riders on the old features,  then if the\nnew features  do help people book more rides, there will be more competition  for drivers on the new\nfeatures  (and vice versa if the features  are detrimental  to conducting  more rides). In either direction,\nthe resulting  effect is exaggerated  due to these marketplace  dynamics.  Therefore,  our best option ts\nto test by using comparable  markets  (comparable  meaning  the metrics in aggregate  should be similar\nacross both markets).\nStep 3: Account  for Geo-Assignment  Flaws\nEvery method has its drawbacks  — geo-based user assignment  included. Assuming that two\ncomparable  markets  are independent  may not be accurate in many cases. Additionally,  even if user\ndemographics  are relatively  balanced  between  the two markets, there is no guarantee  that the users\nAce the Data Science  Interview 257"
  },
  {
    "page_number": 270,
    "content": "CHAPTER  10: PRODUCT  SENSE\nwill always stay comparable  and that any metric changes tracked by these pools of users will be\ncomparable  forever. Lastly, external events may happen that cause the two markets to diverge in\nsome aspect of comparable  distributions  (regulatory  or political change, certain competitors  launch\nmarketing  campaigns  in particular  areas, etc.).\nTo make sure there weren’t any geo-based  assignment  issues, it’s best practice to check on a few\nbaseline  metrics that aren't supposed  to change by market, and validate  that they stayed the same so\nyou can have more confidence  in the test.\nSolution  #10.7:\nIn statistics  class, it’s beaten over our heads how many phenomena  follow a normal distribution.  But\nin the business  world, a great many distributions  actually  follow the Pareto principle,  where 80% of\noutcomes  come from 20% of the causes. Just how 80% of the world’s income is earned by the top\n20% of people, or how many tech companies  have found that 80% of crashes come from 20% of\nbugs, we’d expect the 80/20 rule to be valid here too.\nWe’d expect many small sellers doing small amounts  of revenue,  with a long tail of a few power\nsellers with enormous  amounts  of revenue.  Hence, we’d expect the distribution  to be right skewed.\nSolution  #10.8:\nThis open-ended  discussion  on what makes for “bad” content is one that can test your product,\nbusiness,  and even PR savvy. There’s no right answer  -— people debate this question  everywhere,\nfrom Facebook's  headquarters,  courtrooms,  and even the Senate floor. As long as you’re able to\nbrainstorm  content  removal  features  well and convey  the many nuances  of taking  down posts, you’!]\nbe golden.\nStep 1: Brainstorm  What Posts Should  Be Taken Down\nBesides  what Facebook  ts legally  obligated  to take down (exploitive  photos  of minors,  copyright  and\nIP violations,  etc.) other types of content Facebook  could potentially  take down:\n© Explicit Content:  Nudity,  sexually  suggestive  imagery,  self-harm,  excessive  violence\n¢ Hate Speech:  Death threats,  posts that incite violence,  bullying,  doxxing\n¢ Misinformation:  Conspiracy  theories,  fake news about vaccines  or elections\n¢ Content  from Bad Actors:  Everything  from  a terrorist  organization  or criminal  organization\n¢ Regulated  Goods:  Posts that promote  selling  or trading  firearms,  narcotics,  and human  organs\n¢ Scams:  Ponzi schemes,  fake fundraisers  or charities,  posts from people who stole someone’s\nidentity  and are trying to now solicit money\nWe should also mention  that there are other types of posts which could possibly  be taken down; in\nsome cases, you could avoid this by tacking  on a warming  below the post, with a link to verified\nresources  that fact-check  or debunk  the post. This way, you can reduce  hann on the platform  while\nstill allowing  for freedom  of expression.\nStep 2: Propose  Features  to Find Bad Content\nIn classifying  content,  features  to be considered  would include  the type of content,  the entity posting\nit (1.¢., who posted it), and the context  (when/where  the post occurred).  Here are examples  of features\ndemonstrating  each of these aspects:\n258 Ace the Data Science  Interview  | Product  Sense"
  },
  {
    "page_number": 271,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\n* Content:  Contains  inappropriate  language  (curse words), nudity (in photos),  hate speech, or\nsensitive  keywords  (e.g., “vaccine,”  “election  fraud”).\n* Entity:  Posted  by a suspected  fake account  or bot; an entity with a history  of posts taken down\nin the past; an entity with an unverified  phone number  or email address:  an entity connected  to\nother bad actors,  etc. Since it is likely that people  rarely  just commit  one act of harm, they may\nmasquerade  as various  accounts  with similar  behavior.  Thus, it is important  to keep track of all\nthe detailed  user information  (IP, device ID, etc.) to try and triangulate  such users.\n¢ Context:  How much spam the group or feed it was posted in has, the amount  of “bad actors”\nwithin  the group  or feed posted,  etc. Often rings form online  where  multiple  people  organize  and\nengage  in harmful  activity.\nWe should  also work with product  operations  and manual  review  teams to understand  what types of\nbad posts they are seeing and the heuristics  they use to find these bad posts, as this human  intuition\ncan help us generate  new features.\nStep 3: Explain  Trade-Offs  of Taking  Down Content\nAs with any kind of classification  problem under uncertainty,  there’s false positives  and false\nnegatives.  Classifying  a post as harmful  and taking it down, when in fact it was benign,  can confuse\nand anger users. They might be perplexed  as to why their post was removed,  which could lead them\nto post less of that type of content  in the future. They can also feel like their voice doesn’t  matter\nin the purported  community  Facebook  is building,  and might deactivate  or cancel their Facebook\naccount  in protest  of censorship.  Sometimes,  these news stories  even work their way to Capitol  Hill,\nwith congressmen  and senators  calling  for regulation  or breaking  up the purported  monopoly  because\nsomeone  they like posted  something  innocuous  and it got taken down (which  is perceived  as damning\nevidence  of Facebook  bias and censorship).\nIn the case of false negatives,  letting harmful content remain on the platform  can have many ill\neffects. People can be confused  or harmed  (no, drinking  hand sanitizer  won’t clean the COVID  out\nof your system!).  People who don’t even see the original misinformation  can be affected.  Almost\nalways,  when something  harmful  goes viral, there are negative  news stories, like “5 million  people\nsaw fake news on Facebook  claiming  that Drake is NOT the best rapper of the 2010s” which can\ncause a PR shitshow  for the company.  Long term, it can even make the company  seem complicit  with\nthe misinformation  that spreads.\nAs such, for different  bad content types, there can be different  sensitivities  used, depending  on the\ndownside  risk of having a false positive  or a false negative.  Additionally,  we will want to tweak the\nalgorithm  for sensitive  accounts.  For example,  if a political figure with a large following  posted\nsomething  questionable  but allowed,  but by accident  the algorithm  flags it and it gets taken down,\nFacebook  gets tarred and feathered  in the press for censorship.  For sensitive accounts like news\nagencies,  political figures, or governmental  agencies,  there likely should be a human in the loop to\nimprove  accuracy.\nSolution  710.9:\nTwo words, one equation:  Correlation  ! = Causation\nJust because  more complete  author  profiles  correlate  with increased  sales doesn’t  mean it’s the cause.\nMaybe books with more complete  author profiles had a more highly reputed publisher  fill 1t out for\nthem, which means they likely also have better designed  book covers, and that’s why they have better\nsales.\nAce the Data Science  Interview 259"
  },
  {
    "page_number": 272,
    "content": "CHAPTER  10: PRODUCT  SENSE\nSolution  #10.10:\nThis is your classic metrics troubleshooting  problem. Please raise your right hand, and repeat after\nme: / do solemnly  swear to stick to the 4-step  framework for diagnosing  metric changes.\nStep 1: Clarify  the Scope of the Metric  Change\nAlways start by asking clarifying  questions  about the metric in question.  What is meant by “active\nusers’? Is this a decrease  in daily log-ins, or is it a decrease  in usage of a specific feature? Did the\nmetric suddenly  drop by 5% one day, or was there a gradual decrease  in active users over the week?\nAlso, is 5% a lot? What are we comparing  this drop against  — the daily active users this time last week,\nlast month, or last year? How much does this metric typically  fluctuate  — maybe a 5% drop for a week\nisn’t large enough or sustained  long enough to be a cause for alarm. Or maybe there is seasonality  at\nplay — a 5% drop wouldn't  be too surprising  if this was the week after Christmas  and New Year's.\nFor the sake of our solution, we'll assume that the interviewer  tells us this was a decrease  in the\nnumber  of times logged-in  users have opened the app, and that there is no seasonality  at play. We'll\nassume  the interviewer  says the 5% drop is relative  to last week, and that on a weekly  basis, the daily\nactive user count tends to be consistent,  which is why stakeholders  are so worried.\nStep 2: Hypothesize  Contributing  Factors\nHere are some potential  reasons  behind  the 5% drop:\n¢ Logging  Issues: Data pipelines  responsible  for logging  daily active users broke somehow,  which\nmakes it seem like a genuine  drop, but it actually  isn’t.\n¢ Upstream  Issues: There could be a problem  upstream  of daily logins, like a bug in keeping  users\nlogged in or a decrease  in push notifications  being sent, which is having  an effect on app opens.\n¢ Product  Changes:  Did we change something  inside the product,  like how snap streaks work,\nor extend the expiration  date of snaps, which 1s causing  users to check the app less frequently?\n¢ kxternal  Events: Has some large market experienced  a hardship,  like a natural disaster  or an\ninternet  shutdown,  which is causing  users to not use the app as much?\nStep 3: Validate  Each Factor\nFor each of these factors,  we can validate  our hypothesis  by looking  at various  metrics  and talking\nto teammates:\n* Logging  Issues:  We can check in with the SRE (site reliability  engineering)  team,  data engineering\nteam, and metrics  and logging  teams, to make sure pipelines  are healthy  and the drop is genuine.\n* Upstream  Issues: How is push notification  volume looking?  How are login and password\nrecovery  numbers  looking —-- anything  out of the ordinary?  Did uninstalls  spike in the last week?\n* Product  Changes:  How many snaps were sent, and what was the average  open rate? What about\nthe number  of messages  sent between  users and the number  of stories  posted and viewed  — are\nthese down by 5% too, or much more? If it’s a more drastic drop, maybe something  within the\napp broke that is causing  users to not check the app as much. You can also directly  check for\nproduct  quality  issues by sceing if bug reports  or app crashes  spiked within the last week.\n* External  event: Can you segment  by market,  language,  and OS to check to see if this problem  is\nlocal to any one subgroup,  and then research  into that area for changes  that occurred  in the last\nweek?\n260 Ace the Data Science  Interview | Product  Sense"
  },
  {
    "page_number": 273,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nStep 4: Classify  Each Factor\nImagine  after going through  the above factors,  your interviewer  tells you that logging  is working\nfine, and that this drop shows up across all markets,  all ages, all genders,  and all types of users (both\nnew users and tenured  users). However,  you learn that the drop is 7% for iOS users but only 1% for\nAndroid  users. Now we’re getting  somewhere!\nYou could bucket  the iOS users by the app release  number  they are using,  what carrier  they are using,\nand what model  of iPhone  they are using. Say your interviewer  tells you that people  running  the latest\nversion  of the app have a 20% lower  chance  of being  a daily active user compared  to the baseline  iOS\nuser. From this, you look at other upstream  metrics  for users on the latest app and compare  them to\nthe general  iOS population.  The interviewer  then tells you that user logins and password  resets both\nspiked  the day the new app became  available.\nNow we’ve found  our likely culprit  — there must be a bug when upgrading  to the newest  app release\nthat’s accidentally  logging  out users, who are then forced  to log back in or reset their password.  Some\nsubset  of people  probably  can’t recover  their password,  and thus drop off from being a DAP.\nSolution  #10.11:\nWe first clarify what the new search-ranking  algorithm  change  is, then connect  how this algorithm\nchange  relates to Pinterest’s  product  and business  goals. Once you have done this, you can suggest\nconcrete  metrics  to measure  the impact  of the change.\nStep 1: Clarify  the Product  Change\nIt’s important  to clarify  the scope of the change.  Questions  to ask include:\n* Did this algorithm  change  have any high-level  goals in mind (e.g., prioritizing  trending  Pins,\nimproving  discoverability  of niche Pins, increasing  the personalization  of search results)?\n¢ Did this change involve  any UX or UI changes  perceptible  to the end user, or was it solely a\nchange  on the backend?\n¢ Do search  results  return  just as fast for users as before?\nStep 2: Explain  Why Search  Relevance  Is Important  for Pinterest\nPinterest  is a visual discovery  engine built for helping users find inspiration  for their lives, from\nfashion  to home decor to recipes and more. With billions  of Pins on Pinterest,  the ability to search\nfor and find the content  that sparks a user’s imagination  is key. At a higher level, to keep Pinterest’s\nproduct  competitive  against similar  content  discovery  platforms  hke Instagram  Explore  or Houzz,\nthe search,  content  recommendation,  and user personalization  algorithms  need to be top-notch.\nWhen the discovery  engine  is working  well, users will stay engaged  with Pinterest  and keep coming\nback for more. This user engagement  and retention  is key for Pinterest’s  ad-supported  business\nmodel. Plus, with shoppable  product pins — Pinterest’s  push to diversify  away from ad revenue\nand into e-commerce  -—— the ability for users to search for products  and find exactly  what they were\nseeking  is even more critical to the business  than before.\nStep 3: Propose  Metrics  to Quantify  a Search  Algorithm  Change\nWe could measure  the direct amount  of engagement  the search functionality  received.  For example,\ntime spent searching  or the median number  of searches  made per user session could be used. While\ngrowth in these metrics could be a sign that users like the new search experience,  it isn’t definitive\nAce the Data Science  Interview 261"
  },
  {
    "page_number": 274,
    "content": "CHAPTER  10: PRODUCT  SENSE\nproof of a successful  algorithm  change. For instance, if our search results weren’t relevant,  a user\nmight perform  multiple  searches  to find what they were looking for. Here, an increase  1n time spent\nand searches  made actually  indicates  user frustration.\nThus, a more complete  way to capture  the impact  of the change  would be to also look at the downstream\neffects of a relevant  search algorithm.  For example,  we could measure  how often a search leads to a\nuser pinning  a search result to their board —- a sign that the user found what they were looking for.\nTo go one step further, we can quantify the direct monetary  benefits of improved  search. Here you\ncould measure  the revenue  generated  from purchases  of buyable  pins that came up as a search result.\nFor bonus points, you could also mention evaluation  measures  for information  retrieval systems.\nIf you take a binary approach  to the results — for example,  is this pin relevant  or not — you can\nuse a metric like “precision@10”  to understand  the percentage  of relevant pins amongst  the first\n10 results. However,  this won't take into account each pin’s position  within search results. If you\nwant to account  for the actual degree of relevance  for each pin, a metric like normalized  discounted\ncumulative  gain (nDCG)  measures  how close the results are to the best possible  result. The technical\ndetails of this measure  are beyond  the scope of this text.\nSolution  #10.12:\nThis product interview  problem  is a hybrid between  a root-cause  analysis  question  and a defining\nsuccess  metrics  question.  We'll first start by connecting  the supply vs. demand  question  to Netflix’s\nbigger business  model and product  goal. Then, we'll define some metrics  we’d want to analyze in\norder to troubleshoot  the root cause of the content  supply  or demand  problem.\nStep 1: Why Netflix  Cares  About  Content  Supply  & Demand\nNetflix  *s mission  is to entertain  the world. Netflix  Studios  not only has the power  to produce  original\ncontent,  it can directly  influence  what entertainment  hundreds  of millions  consume.\nBy deeply understanding  what its users want to watch, thanks to the myriad of analytics  Netflix\ncollects,  Netflix can greenlight  new shows that delight niche audiences.  Let’s face it: you’d never\nsce a show like /ndian Matchmaking  or Orange  is the New Black on cable TV. As long as Netflix\ncreates  high-quality  tailored  content,  its users will continue  to engage  and retain on the service  rather\nthan churn out due to stale inventory,  or switch  to competitors  like Disney  + or Amazon  Prime Video.\nHowever,  creating  original  shows is very expensive.  You can’t just blindly  do things, like Sandra\nBullock in Bird Box. Netflix needs to prioritize  what types of shows to produce  so that every\ndollar spent brings back many more in the form of increased  customer  retention  and NPS. As such,\nIt’s crucial to vet how much demand  there is for a show category  before investing  resources  into\nproducing  Netflix  originals  or licensing  more media  from other studios.\nStep 2: Content  Supply  vs. Demand  Metrics  to Investigate\nKnowing  that there is less total viewer watch time devoted  to sci-fi TV shows compared  to other\nsimilar  categories  doesn’t  tell you much. Sure, total watch time for sci-fi might be less compared  to\nother categones,  but what about on a per-show  basis’? Maybe  there’s  just fewer available  sci-fi TV\nshows, so even if people like the show, there just isn’t enough  inventory  to support  high total watch\ntimes compared  to a category  like comedy  or drama,  which  has many more shows  to watch. Knowing\nthat even though  the total watch time is low, the watch time per show is high would indicate  a supply\nproblem,  not a lack-of-interest  problem.  This would signal that Netflix should create or buy more\nsci-fi TV shows  — not drop the few sci-fi shows  they do have.\n262 Ace the Data Science  Interview  | Product  Sense"
  },
  {
    "page_number": 275,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nAnother  way to get a clue on whether  it’s a user interest  or show quality problem  is by looking  at\nmetrics  related  to sci-fi TV show recommendations.  Are people  browsing  for titles in this category  but\njust not hitting  play? This could indicate  there is demand,  but nothing  catches  a user’s eye. One step\ndown the funnel,  what’s the conversion  rate between  watching  the first episode  of a sci-fi series and\nfinishing  the first season?  How does it compare  to other categories?  Maybe  sci-fi TV shows  just aren’t\nbingeable.  Or maybe  the show is low quality  (ok, maybe terrible  quality),  and people can’t stand to\nfinish a season,  let alone a whole series (Sense8).  To more directly  measure  supply quality,  you can\nalso look at user ratings  on sci-fi TV shows.  Do they tend to be much lower than in other categories?\nWhen looking  at all these metrics,  we should segment  by user attributes,  like viewer country  or\nlanguage.  Netflix is a global platform,  and it’s not fair to expect users to act as a monolith.  Maybe\nsci-fi watchtime,  relative  to other categories,  is okay in English-speaking  markets,  but more effort\nneeds to be put into closed  captioning  and voice dubbing  to serve other countries.\nBonus  Points:  Zooming  Out\nA good answer should also consider  that it might not even be an actual supply or demand  issue.\nMaybe,  there’s an issue at a step higher in the funnel, like sci-fi shows that, for some reason,  don’t\ntend to make it into Netflix  show recommendations.  Or maybe more mainstream  shows get most of\nthe advertising  budget.  In both cases, there is demand  for sci-fi, and there is content  available  to meet\ntheir needs, but discovery  is broken  so people  aren’t  aware of any sci-fi show besides  Stranger  Things.\nWe can also look outside  of Netflix  for an answer.  Through  consumer  surveys  or audience  insights\ndata from a company  like Nielsen,  we can benchmark  engagement  in sci-fi shows against  broader\ninterest  in the category.  This way, we could tell if there is generally  less demand  for sci-fi content,\nboth on and off Netflix, or 1f Netflix in particular  is underperforming  in this segment  relative to\ncompetitors  and cable.\nSolution  #10.13:\nBefore we jump into the technical  details of performing  customer  segmentation,  it’s important  to\nflex your business  muscles  and explain  what customer  segmentation  analysis  is and brainstorm  a few\nconcrete  ways the analysis  results  could boost store sales.\nStep 1: Explain  How Apple Benefits  from Customer  Segmentation\nEach person  that walks into an Apple store has individual  needs, desires,  and preferences.  In an ideal\nworld for Apple, they would be able to hyper-target  their product  offerings  and store design to cater\nto a single person’s  needs, one at a time. Obviously,  this isn’t feasible. On the other hand, treating\nall Apple customers  the same misses the variety of customer  needs. As such, by grouping  similar\ncustomers  and creating  customer  segments,  Apple can customize  its in-store sales strategy  to large\ngroups  of customers  at once.\nHowever,  our approach  does have a caveat: by relying  on historical  data for customer  segmentation,\nwe aren’t able to analyze  non-Apple  customers  since they wouldn’t  show up in prior store sales data.\nTherefore,  it’s important  to let the interviewer  know that a customer  segmentation  analysis  should be\ncomplemented  with some competitor  research  or market-level  analysis.\nStep 2: Brainstorm  Ways Customer  Segmentation  Can Boost Sales\nThere’s many different ways to segment users, and then use those insights to boost sales. For\nexample,  one axis you could segment  users on is their tech savviness.  This information  could impact\nthe different  kinds of sales scripts Apple uses. For example,  a store salesperson  convincing  a software\nAce the Data Science  Interview 263"
  },
  {
    "page_number": 276,
    "content": "CHAPTER  10: PRODUCT  SENSE\ndeveloper  to buy a MacBook  would use a very different  pitch than when explaining  the benefits of\nthe product to a nontechnical  person. Jt could also impact the store staffing — maybe each store\nshould have a technical  expert to field the toughest  questions.\nAnother dimension  on which you could segment customers  is by the main type of product they\nbought. Say, for example,  our analysis  found three main types of people routinely  walk into an Apple\nstore: iPhone purchasers,  MacBook  purchasers,  and Apple Genius Bar customers  who pay for an\nissue to be fixed. By separating  customers  into three groups, we learn that iPhone purchasers  are five\ntimes as likely to buy AirPods  than MacBook  purchasers  or customers  coming in to get their device\nfixed by the Apple Geniuses.\nThis insight from customer  segmentation  could mean that it’s best to place the AirPods  next to the\niPhones,  to increase  the chance of a cross-sell.  Another  implication  of this insight would be to train\nsalespeople  to upsell AirPods to customers  who are about to buy iPhones,  but not waste their time\non the upsell for MacBook  shoppers.  Another  idea is creating  a new discounted  iPhone and AirPods\nbundle  to entice customers  into buying  both products  at once.\nStep 3: Explain  How to Perform  Customer  Segmentation\nTo perform  customer  segmentation,  we could use K-means  clustering.  We could visualize  the data\nor do hyperparameter  tuning to find the appropriate  number  of clusters to segment  the users into.\nBesides running  K-means  on the transactions  data, we could also try to connect  online sales data\nto in-store  customers.  While this analysis  is primarily  for understanding  brick and mortar  shoppers,\ncross-referencing  in-person  customers  with their potential  online purchases  on Apple.com  could help\ngive a more complete  picture  of the customer.\nSolution  #10.14.\nThe first step would be to gather  the basic data on both the 10S and Android  users for both Facebook\nand Instagram.  You could analyze  user demographics  such as age, gender,  race, and location.  You\ncould also analyze user activity,  looking  at metrics such as time spent overall,  and time spent on\nvarious  activities  (feed, in-app messaging,  etc.) for both groups  of users on both apps.\nWe can visualize  the user activity metrics by each cut of user demographics  to get a top-level\nunderstanding  of where any differences  may lie. For example,  iOS users may, on average,  spend\nmuch more time on the Facebook  ecosystem  than Android  users do, and this ‘“‘top-of-funnel”  reason\nmay lead them to use Instagram  more also. Alternatively,  iOS and Android  users may, in general,  be\nfrom different  age groups,  and this could be affecting  their respective  levels of Instagram  usage, as\nInstagram  isn’t as widely  used by older people.\nAnother  set of factors  to consider  would  be the actual Instagram’s  device  and resource  requirements,\nrelative  to Facebook's  requirements.  Maybe iOS devices  have a much easier time downloading  the\nInstagram  app, since the app size is smaller  for iOS than Android.  Maybe the Instagram  app only\nworks on devices  that have updated  their OS within the last two years, and Apple  devices  tend to run\nthe latest OS much more than Android  devices.\nMaybe the problem  les with the actual app experience  -— do Facebook  and Instagram  perform  the\nsane way on both platforms?  What do app store ratings,  number  of bug reports,  feed scroll latency,\nand percentage  of sessions  with app crash for both apps on both platforms  look like? Maybe  Facebook\nworks  equally  well on both plattorms,  but Instagram  has under-invested  in its Android  app optimization.\nFinally. for a difference  so big. across so many users, I'd make sure to talk with user experience\nresearchers,  folks from the product  strategy  teams,  and the Android  and iOS leads for both Facebook\n264 Ace the Data Science  Interview  | Product  Sense"
  },
  {
    "page_number": 277,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nand Instagram.  While cutting  the data may reveal the underlying  reason,  our gut intuition  is that for\nsuch a large difference  across so many users, there’s likely a bigger  structural  or strategic  cause for\nthe disparity  that a purely SQL-based  analysis  may not uncover.  At the very least, by talking  to other\ndomain  experts,  we can add some more color to our analysis.\nSolution  #10.15.\nJust like with every metrics  question,  a good answer  should start out with a brief discussion  of the\nbusiness  goal -— in this case, into the goal of Capital One’s credit card. It should also mention  a\nfew of the stakeholders  involved  with this business  goal before determining  the best metrics for\nmeasuring  retention.\nStep 1: Explaining  Capital  One’s Motivation  Behind  Credit  Card Retention\nA credit card’s “stickiness”  is the frequency  and duration  of its use by its holders.  The bank issuing  a\ncard is motivated  to encourage  a card holder  to use the card frequently  and over the long term —— in\nother words,  to increase  its “stickiness,”  as this earns it a greater  profit in interest  (if a holder  carries\na balance)  and directly  on transactions.  A card holder would prefer a non-cash  option having low\nmonthly  repayments,  a low interest rate, and, perhaps,  rewards  for using the card. The goal of a\nbank’s reward system  (cash back or other perks associated  with card use) is to increase  card usage\nand its customers’  reliance  on the card over the long term. Thus, if Capital  One gave no incentives  to\ncardholders  to encourage  them to use their card to pay for purchases,  then users would most likely\nuse their cards less frequently  or, possibly,  not use them at all — hence, a decrease  in stickiness.\nStep 2: Brainstorming  “Stickiness”  Metrics\nFor Capital  One to assess the stickiness  of its Quicksilver  card, some potential  metrics  it might use\nare as follows:\nDaily active users to monthly  active users: Although  this ratio can be over any interval  that you\ndeem appropriate,  the goal of this metric is to see what percent  of active users during  a longer  interval\n(in this case, a month)  are active over a shorter  interval  (daily).  A ratio of, for instance,  0.7 would\nsuggest  that 70% of cardholders  who spend with the card on a monthly  basis also do so on a daily\nbasis. The higher  this metric 1s, the stickier  the product.\nMonth-over-month  retention:  |f you were to create cohorts  of people  based on when they signed  up\nfor the card, you could determine  what percentage  of the cohort churns from any given time interval.\nIn this case, a month-to-month  time interval seems reasonable,  as credit cards are billed monthly.\nSeeing  what percentage  of cardholders  remain after X months  enables  you to see trends in duration\nof use before a customer  leaves or the average  duration  of time a customer  remains  with the card\nbefore leaving. However,  a card holder can also be inactive  without  actually  closing  the account  (a\n“silent churn”).  Many cardholders  typically  use various cards for one particular  purpose  or activity\n(e.g., one card for travel, one for dining, and so on). Therefore,  attempting  to track such behavior  as\nwell is advisable.\nTransaction  volume churn: Tracking  the total amount spent by cardholders  ts critical, since this\ncorrelates  to the amount  of revenue  Capital  One could make. By looking  at all users who spent money\nlast month, and their total transaction  volume,  and comparing  it to the total transaction  volume of\nthose same users this month, you can see if adoption  is growing  or shrinking.\nAce the Data Science  Interview 265"
  },
  {
    "page_number": 278,
    "content": "CHAPTER  10: PRODUCT  SENSE\nSolution  #10.16:\nOne good question  to ask the interviewer  before answering  1s “What's  the goal of pricing?”  For anew\nservice, it could be okay to run the feature without profit if you believe you can aggressively  gain\nmarket share and turn on monetization  later. Assuming  we don’t want to run a free service, we can\nprice YouTube  Premium  using cost-plus  pricing, value-based  pricing, or competitor-based  pricing.\nCost-Plus  Pricing\nFor a cost-plus  pricing approach,  we’d look at how much it costs to provide YouTube  Premium  in\nthat country and then add a margin on top of that number to enable the business  to earn a profit.\nWe'd account for product localization  costs, marketing  costs to advertise  to the new geography,  and\nbandwidth  costs to serve content.  While not technically  a cost, because  there’s no ads in this feature,\nyou could also account  for the lost ad revenue  per user that YouTube  would’ve  earned.\nFinally,  because  content  licensing  can be a bulk part of music-streaming  service  costs, and oftentimes\nmedia is licensed  on a per country or per region basis, it’s important  to understand  how expensive\noffering  our music library would be. By adding up these costs and then tacking  on a premium  to this\nnumber,  we’ve got one way to price the product.\nValue-Based  Pricing\nA value-based  pricing strategy  would price the service relative  to the amount  of value a consumer\nperceives  from using the service. The most direct way to gauge the perceived  value of YouTube\nPremium  would be to ask users themselves  through  consumer  surveys  and focus groups.\nAn alternative  could be to see what an optimal price was in other countries  where this product\nlaunched,  and assume  that price hits the optimal  value offered.  Then, adjust the price to the local\nmarket  based on the market’s  average  per capita income.\nCompetitor-Based  Pricing\nYou could price the service based on competitor  video and music streaming  services  like Netflix,\nHulu, Spotify,  and Apple Music. In each new country  where  YouTube  Premium  is being launched,  by\nseeing how much more valuable  (or less valuable!)  our service  is, we can adjust the pricing  against\nthe competitors.  All else being equal, the larger the number  of alternative  options  for a country’s\nresidents,  the more likely that YouTube  Premium’s  pricing  would need to be discounted,  especially\ngiven the relatively  low switching  costs available  to consumers.\nWrapping  It Up\nLikely, an appropriate  price for YouTube  Premium  would be a blend of all three pricing  strategies.\nTo triangulate  on an exact answer,  you’d want to consult  with stakeholders  like the sales, marketing,\nand finance  teams. You could also always try A/B testing the prices, or running  discounts  or tiered\nmemberships  to get more signal into what an optimal  price point may be.\nSolution  #10.17:\nFor questions  dealing  with whether  a company  should launch  a certain  feature  or not, it’s best to not\nprematurely  discuss  the proposed  idea's merits. Instead,  clarify  with the interviewer  what the feature\nactually  is, what the company’s  hypothesis  is behind proposing  such a feature,  and how it would\nimpact key business  metrics.  Great answers  would elaborate  on the potential  pitfalls  of shipping  the\nfeature  and, lastly, end with a final recommendation.\n266 Ace the Data Science  Interview | Product  Sense"
  },
  {
    "page_number": 279,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nStep 1: Clarify  Twitter’s  Product  Hypothesis\nCurrently,  users on Twitter  can only like, comment,  or retweet a tweet. Emoji-style  reactions  are\nlikely being considered  as a way to improve  the engagement  rate of Tweets.  The hypothesis  is that\nby reducing  the friction  for expressing  more complex  emotions  like “this tweet was funny”  or “this\ntweet made me mad,” users will be more likely to engage  with tweets.\nThis feature  also addresses  a common  user issue: hitting  “like”  on sad news feels weird. By providing\na more nuanced  alternative  to liking a tweet, there could be more engagement  on tweets  overall.\nStep 2: Explain  Twitter’s  Business  Goal\nIf emoji-style  reactions  lead to more tweet engagement,  there would be many positive  downstream\neffects to Twitter’s  business.  Firstly, users would be more engaged  with their feed and interacting\nwith more pieces of content,  leading  to longer  and likely more meaningful  sessions.\nIt’s also not just about the amount  of engagement  given — it’s about the positive  impact on the\nreceiving  end too. People  whose tweets get more engagement  will get more notifications  of people\ninteracting  with their post, driving  them to check Twitter  more often.\nWhen tweeters  notice their posts getting  more engagement,  that dopamine  hit will surely incentivize\nthem to post more on Twitter. This will improve  tweet creation  metrics,  which would boost the\namount  of interesting  content found on the timeline  for all users. This positive  flywheel  of more\nengagement  leading  to more content  leading  to further  engagement  would improve  both time spent\non Twitter  and user retention  rates. More time spent and more user retention  means more ads seen,\nwhich 1s crucial  for Twitter’s  ad-supported  business  model.\nStep 3: Identify  Data to Support  Product  Hypothesis\nWe want to find some data that can guide us on whether  emoji-style  reactions  are desired  by users.\nThe best way to do this would be to analyze  current  user activity  and see if there is some unmet latent\ndemand.  If we have evidence  that users are taking multiple  steps to express the common  reactions\nlike “love”  and “haha,”  then simplifying  the process  for expressing  these feelings  via emoj! reactions\nwould  be a logical  next step.\nIn terms of specific user data on Twitter that we can use as a proxy for demand  for the reactions\nfeature,  we could look at current  Twitter  threads  and analyze  how often sentiments  corresponding  to\nthe proposed  reactions  are expressed.  Take the “haha” reaction,  for example.  If we see many short\ncomments  laughing  at a particular  thread or reply, using keywords  like “lol” and “haha”  and “Imao,”\nwe’d have a strong indicator  that users want to express  that they found the tweet funny. In that case,\noffering  them an easier way to express  that emotion  would be beneficial.\nWe could also work with user research  teams to conduct surveys on groups of users to confirm the\ndata: do these people desire an expanded  set of reactions?  If so, for which reactions,  and why? The\nuser research  and the data should collaboratively  point to the same direction  (that people want the\nability to express  more reactions).\nStep 4: Counter-Argument  for Shipping  Reactions\nCounter-arguments  for emoji-style  reactions  are that it increases  the complexity  of Twitter. Right\nnow, having a single reaction makes things intuitive.  Another  issue is that more complex  emouons\nare expressed  in comments.  If we made reacting easier, we'd likely have fewer people replying\nto a tweet with messages  like “this is so funny” or “i hate this.” Plus, from an aesthetic  and brand\nviewpoint,  the Facebook-ification  of the Twitter  product  may not be desirable.\nAce the Data Science  Interview 267"
  },
  {
    "page_number": 280,
    "content": "CHAPTER  10: PRODUCT  SENSE\nThere are also several  challenges  from the product  and engineering  side. For example,  which reactions\nshould be used? It may be redundant  or confusing  to have “love, “enjoy,”  and “like.” reactions.  Also,\ndo we have the engineering  resources  to dedicate to build and test this nontrivial  feature? It’s not a\ngiven that the potential  engagement  boost warrants  the time and money needed to launch the product.\nHas opportunity  sizing been done to show the potential  ROI on this project?\nLastly, all engagement  isn’t equal. Before running this kind of experiment,  you'd want to align with\nstakeholders  how important  replies are versus reactions.  By anticipating  the likely trade-off  that will\nneed to be made  if this feature is successful  (overall  reactions  would be up, but number  of comments\nwould be down) and discussing  this issue with stakeholders,  you’d mitigate  launch blockers  and have\nan easier path to shipping  it after the A/B test results come back in.\nStep 4: Make Final Recommendation\nIf we have alignment  that the goal is to increase  engagement,  have found good reason that the feature\ndemand is legitimate,  have intuition on why implementing  the feature as described  would drive\nengagement,  and think the business  benefits  outweigh  the development  cost and time required  to A/B\ntest the feature,  then and only then does it makes sense for Twitter  to test reactions.\nAs mentioned  in the counter-argument  section,  after we have A/B test results, we should align with\nstakeholders  about the likely metric trade-off  that will occur — increased  reactions  but decreased\ncomments  — before making  the final launch decision.\nSolution  #10.18:\nStep 1: Stakeholders  for Slack Engagement\nSlack’s user engagement  1s important  because  it aims to be the go-to work and productivity  tool.\nSince Slack serves as a place where people collaborate  together,  user engagement  ts critical to\nmonitor  and measure.  A user in this context  is simply  any person  who has an account  on the platform.\nUser engagement  affects the business  directly  since Slack operates  as a subscription-based  model,\nwhere users pay per month for features  on the platform.  With more consistent  user engagement,  there\nis likely to be longer-term  retention  and new customers  over time.\nStep 2: Defining  User Engagement  Metrics  for Slack\nTo measure  user engagement  for Slack, we could look at DAUs (daily active users), WAUs  (weekly\nactive users), and MAUs (monthly  active users). An active Slack user is defined  simply as anyone\nWho signs into Slack on a given day. Since a product  like Slack is meant to be used daily, tracking\nDAUs is crucial, and the ratios DAUs/WAUs  and DAUs/MAUs  are typically  used, since a higher\nvalue of one of these ratios would mean the product  was more sticky. Note that we would need to be\naware of weekends  and holidays  (seasonality).\nSince the product is a collaboration  software,  another  core metric for engagement  would be the\nnumber  of messages  sent. Again, as with the number  of active users, we would want to look at\nmessages  sent per day, per week, and per month.  Other auxiliary  metrics  we could track include  the\nfollowing:  creating  organizations.  applying  for membership  in organizations  created  by another  user,\netc. However.  the two primary  metrics  would be number  of active users (i.¢., those who sign in) and\nnumber  of messages  sent.\nAdditionally,  measuring  these trends at the cohort level would be important  to ensure consistency\nover ume. For example,  DAUs could be dropping  slightly,  but the behavior  among  various  cohorts\n268 Ace the Data Science  Interview  | Product  Sense"
  },
  {
    "page_number": 281,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\ncould be very different.  In this case, identifying  the cause(s)  of why specific  cohorts  are seeing  drops\nof greater  significance  than others would  be important.\nStep 3: Defining  Leading  Indicators  for Engagement  Decline\nTo receive an early warming  of declining  user engagement,  we could look at trends over time in\nnumbers  of both DAUs and daily messages  sent, as declines  in either or both of these could be\nleading  indicators  of an overall decline  in users. For example,  a user who eventually  leaves might\ninitially  be a DAU, but then slowly  become  a WAU  , and, finally,  become a MAU only. Such a lack\nof engagement  would show up in DAUs and in numbers  of messages  sent, and, hence, tracking  these\nwould be important,  as would calculating  and tracking  the ratios DAUs/WAUs  and DAUs/MAUs.\nAgain,  looking  at these metrics  on a cohort  level is also important.\nAce the Data Science  Interview 269"
  },
  {
    "page_number": 282,
    "content": ""
  },
  {
    "page_number": 283,
    "content": "Case Studies\nCHAPTER  11\nData Scientists  don t do pure “'statistics”  work or write SOL queries  in isolation.  They solve\nbusiness  problems,  end-to-end,  as part of a larger  team. To reflect  this job’s need for a wide\nrange of soft and hard  skills, and assess  your overall  holistic  problem-solving  ability,  some\ncompanies  ask you open-ended  case interview  questions.  These  problems  force  you to apply\nyour  product  sense, statistics  knowledge,  ML experience,  and system  design  skills to a real-\nworld  business  problem.  While some companies  ask case interview  questions  as part of an\non-site interview,  other companies  adapt these problems  into a take-home  project, often\ngiving  you both a problem  statement  and a dataset  to base  your answers  on. In this chapter,\nwe present  tips to tackle  these open-ended  questions,  as well as solve eight interactive  case-\nstyle questions  from companies  like Facebook,  Amazon,  and Citadel.\nWhat Case Interview  Questions  Look For\nBefore we jump into what hiring  managers  look for from candidates  solving  case interview  questions,\nlet’s start with a concrete  example  of what a case interview  entails. At LinkedIn,  for example,  you\nmight be asked how you’d recommend  jobs to people. You'd need to clarify what the business  and\nproduct  goals are for the recommendation  feature,  and tie this information  into your solution.  You'd\nbe asked about what data you’d use to train the model, what business  metrics  you'd use to know if\nyou were successful,  and what technical  and product-related  edge cases you might run into. Finally.\nyou could be asked how to A/B test and deploy the solution.  Essentially,  it would cover the End-to-\nEnd Machine  Learning  steps we covered  in Chapter  7, along with some of the product-sense  topics\nfrom the last chapter.\nAce the Data Science  Interview 271"
  },
  {
    "page_number": 284,
    "content": "CHAPTER  11: CASE STUDIES\nBecause of the similarity  to the product and business-sense  questions,  many of the tips from Chapter\n10 apply to case interviews  too. Being conversational,  stating your assumptions,  bringing in outside\nexperience  to show your depth, and keeping business  goals top of mind are all table stakes.\nHere’s a few more tips to keep in mind for case interview  questions:\n° Clarify, Clarify, Clarify: \\n the product-sense  chapter, we beat this dead horse, but it’s worth\nrepeating  because  jumping into a solution without clarifying  the business and product goals 1s\nthe #1 mistake  we see candidates  make!\n¢ Be Coachable:  Be open to suggestions  from your interviewer  ~~ often these are hints that they\nwant to steer the conversation  in a particular  direction.  Agree and amplify when they suggest\nsomething:  “Oh, yeah, | didn’t consider  boosting;  that’s an interesting  angle. Maybe XGBoost\ny”?could be used\n¢ Remain Pragmatic:  The interviewer's  team probably has been working on this problem for\nseveral months and is not expecting  a revolutionary  solution from you. Stick to solutions  you\nthink are reasonable  that align with the product  and business  goals.\n° Mention Tradeoffs:  Remember  to bring up trade-offs  and limitations  of the approach  you are\npresenting.  Briefly mention other options or techniques  you could have used, and how they\ncompare  —-- it helps you show depth. Plus, you can even ask the interviewer  which of the options\nthey'd like to discuss in more detail, which lets the interviewer  steer the conversation  towards\nwhat they find most interesting.\n¢ Timebox Yourself:  Time is of the essence. Avoid going down rabbit holes. The interviewer  1s\ntrying to get as much signal as quickly as possible,  so rambling  on about something  irrelevant\nhurts you. Hf you're uncertain  about how much detail you should provide,  give an initial answer,\nand then ask the interviewer  if they’d like you to elaborate  -- remember,  this 1s a two-way\nconversation!\nApproaching  Take-Home  Challenges\nThe types of questions  you're asked in case interviews  are akin to what you might be assigned  in\na take-home  data challenge.  The main difference  is, for a question  like “How would you improve\ncustomer  salistaction  of our products?”  you’]] now have data you need to analyze  to back up your\nrecommendations.\nThe biggest  piece of advice we have Is to tell a story. As you saw in the earlier  chapters  on portfolio\nprojects  and behavioral  interviews,  storytelling  permeates  every aspect of selling your abilities  and\nyour work. A take-home  challenge  is no different  — now, you just have a way to incorporate  concrete\nnumbers  and data visualizations  into the story you tell. From surveying  hiring managers,  we know\nthis advice works, because  one of the most crucial skills hiring managers  assess candidates  on is\ntheir ability to turn their analysis  into an accessible  recommendation.  And the best way to convey  a\nrecommendation  and ensure stakeholders  make the right decision  with data is to tell a narrative  that\nthey can easily comprehend.  Storytelling  is how you do that!\nSince take-home  challenges  are long, widely  differ from company  to company,  and refer to a specific\ndataset,  they were difficult  lo incorporate  into this book. But fear not: we filmed step-by-step  video\nwalkthroughs  of us solving  three real take-home  challenges  to make available  to you, our readers.  In\nthe past. we've usually  sold our take-home  challenge  video course,  along with the actual case study\nproblem  and data, to our coaching  clients for $89. But, if you share what you liked about Acc the Data\nScience  Interview  in an Amazon  review,  we’ll give you a copy for free!\n272 Ace the Data Science  Interview  | Case Studies"
  },
  {
    "page_number": 285,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nJust email  a screenshot  of  your “Verified  Purchase”  review  to nick@acethedatascienceinterview.com\nfo access  our  free take-home  data challenge  video course!\nCase Study  Questions\nCase interview  problems  have multiple  follow-up  questions.  In a real interview,  you wouldn't  be\npresented  with all the  follow-up  questions  at once. As such, to simulate  a proper  interview,  the  follow-\nup questions  are included  in the solutions.\nI1.1. Citadel:  Say you wanted  to estimate  the physical  store sales for a publicly  traded  retail chain\nin the U.S. You have access to third-party  foot traffic data for each store that was derived\nfrom anonymized  GPS data collected  from a panel of 10 million  mobile  phones.  At a high\nlevel, how could you use this foot traffic dataset  to predict  the chain’s  in-store  revenue?\n11.2. Amazon:  Assume  that you are designing  a system whose purpose  is to recommend  what\nshows a user should watch on Amazon  Prime Video. What data and techniques  would you\nuse?\n11.3. Airbnb:  Assume  that you are modeling  the yearly revenue  of new property  being listed.\nWhat kinds of features  would  you use?\n11.4. Walmart:  How would you build an algorithm  to price products  sold physically  at Walmart\nstores?\n11.5. Accenture:  Assume  that you want to help a major  hotel chain analyze  what people  say about\ntheir brand on websites  like Facebook,  Twitter,  and Reddit.  Why might this be useful, and\nhow would  you go about doing it?\n11.6. Facebook:  Suppose  you are to build out Facebook’s  friend recommendation  product,  also\nknown as the “people  you may know” (PYMK)  feature. How would you go about doing\nthis?\n11.7. Stripe: Assume you are working  on a loan approval  model for small businesses.  What\nmetrics  would you use to evaluate  the model, and what trade-offs  would be involved  in the\nevaluation?\n11.8. Instagram:  How would you provide  content recommendations  for Instagram  Explore?\nCase Study  Solutions\nSolution  #11.1\nCitadel  - Retailer  Revenue  Estimation\nSay you wanted  to estimate  the physical  store sales for a publicly  traded retail chain in the U.S.\nYou have access to third-party  foot traffic data for each store that was derived  from anonymized\nGPS data collected  from a panel of 10 million mobile phones.  At a high level, how could you use\nthis foot traffic dataset  to predict  the chain’s  in-store  revenue?\nSure -—— before starting,  I just wanted  to clarify a few things. First off, what's the use case for predicting\nthe in-store  revenue  — why do stakeholders  want this number?  I’m assuming  it’s to make investment\ndecisions.  Second of all, we are trying to make revenue predictions  for the entire chain --- not for\neach store, correct? Lastly, what’s the granularity  needed for our revenue prediction?  I’m assuming\nit’s quarterly  due to quarterly  earnings  for public stocks”\nAce the Data Science  Interview 273"
  },
  {
    "page_number": 286,
    "content": "CHAPTER  11: CASE STUDIES\nGood questions  — we are trying to use the revenue estimate  to decide whether  to buy (long)\nor sell (short) stocks of certain big box retailers  before their quarterly  earnings  come out. We\nhave robust models  to measure  the brand’s  e-commerce  sales, but are trying to proxy the entire\nchain’s sales from physical  retail with this foot traffic dataset.  Any other questions?\nCan you tell me more about this data source — what do the rows/columns  look like? I presume  each\nrow is for a given store, and the columns  will include some kind of non-normalized  foot traffic value\nbased on devices tracked for a given time period. I’m guessing  it’s at a daily granularity?\nYes, it’s the amount  of visits detected  to a store, reported  at a daily granularity.  Each row is a\ndate-storelD  pair, and the column is an integer that holds the number  of detected  store visits\nthat day. The numbers  aren’t normalized.  So, how would you model the store sales?\nThe high-level  approach  would be to run a model to predict revenue based on some aggregated\nmeasure  of foot traffic. Since our third-party  foot traffic is likely to not be rolled out —- for example,\neach store location per hour ---- we’d want to roll up the foot traffic to the same granularity  as the\ncompany’s  revenues  (which is reported  quarterly  as a publicly  traded company).\nNote that we want to normalize  this traffic to account  for the fact that the data comes from a sample,\nso the quality of foot traffic data (volume,  frequency,  noisiness,  etc.) would affect the quality of\npredictions.  Then we can look at a simple regression  model that correlates  revenue  as a function  of\nthis normalized  foot traffic.\nSay the foot traffic  data only goes back 3 years, which means  you only have 12 quarters’  worth\nof revenue  for the retailer  to train a model on. How would you avoid overfitting  on such little\ndata?\nTo reduce overfitling  in general,  we can pick simpler  models like linear regression  (versus more\ncomplicated  models)  and apply regularization.  We could also report a 95% confidence  interval  for\nour revenue  estimate  to better help decision-makers  account  for uncertainty  in our forecast.  In the\ncase of Just having  so few data points (12 quarters’  worth),  we can look to other similar  retailers.\nAs a concrete  example,  say we were modeling  Target revenue  --—- we include  data from not only\nTarget but also similar  retailers  like Walmart  and Costco  to add more data points to produce  a more\nrobust regression.  This helps our model learn how foot traffic affects  retail sales in general.\nHow would you use the foot traffic panel data to determine  the “true” number  of visits to a\nstore?\nBecause  this foot traffic data would come from a constantly  changing  panel of mobile phones,  to\naccount  for panel size changes,  we’d incorporate  the panel size into the equation.  As such, the number\nof “true” visits to that store would be (sample  foot traffic for store / sample  population  size)* U.S.\npopulation  size. You could also localize  this normalization  -—- it’s not obvious  the panel coverage\nwould be equal across the entire country.  Instead,  you could divide by the amount  of people in the\npanel in a given census block group or zip code, and then multiply  by the true census  population  of\nthat locality.\nWhat is sampling  bias? In what ways do you think this dataset  may be biased due to the way\nit was sampled?\nSampling  bias is when data 1s sampled  from some segments  of the population  at disproportionate\nrates. In this case, location  data may be biased because  not everyone  has a smartphone.  Of those\nthat do, not everyone  Is as likely to have apps with a location  component  to them. And finally, not\neveryone  is as likely to turn their location  permissions  on.\n274 Ace the Data Science  Interview  | Case Studies"
  },
  {
    "page_number": 287,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nHow would  you check  for sampling  bias within  the panel?\nWe can check if the given location  analytics  dataset  is a representative  sample  by comparing  it to\nthe distribution  of the underlying  U.S. population.  Based on where a smartphone  spends  most of its\nlime stationary  during  nighttime  hours, we can assume  that to be a device’s  home location.  We can\ncompare  the home location  of the devices  in our dataset  to the U.S. Census’s  data on population  at\nthe state, county,  and census  block group levels to account  for any geographic  bias in our dataset.\nSimilarly,  from the home location  and census  data, we can understand  what types of neighborhoods\nthese devices  live in. From this, we can approximate  the average  income  and race of people in our\npanel. We can again check this against census data to see how well it lines up with the true U.S.\npopulation.\nEssentially,  we want to ensure  that for any given dimension,  the expected  proportions  in the sample\n(of any given geography,  county,  etc.) are similar  to those of the entire population.\nSuppose  you try to use a linear regression  model, but it doesn’t  run, and communicates  that\nthere is an infinite  number  of best estimates  for the regression  coefficients  (through  an error\nmessage  of some sort). What  could be wrong?\nA few things could be happening  here. One is that the number  of features  exceeds  the number  of data\npoints. In this case, there will be features  that can have an infinite  range of values. To address  this,\nyou could look into PCA to reduce  dimensionality.  Another  possibility  is that there are some features\nthat are perfectly  correlated  (either  positively  or negatively)  with others, i.e., multicollinearity.  This\nwould  cause unstable  coefficients  — you'd get different  coefficients  per feature  for different  runs of\nthe regression.  To address  this, you could visualize  the correlations  between  features  and remove  any\nunnecessary  ones.\nYou run your regression  on different  subsets  of your data and find that in each subset,  the beta\nvalue for a certain  variable  varies  wildly.  What could be the issue here?\nThe dataset might be heterogeneous,  1.e., the distributions  underlying  the various subsets are\ndrastically  different.  This may be due to improper  data collection  or sampling  techniques,  in which\ncase, it is recommended  to cluster datasets into different  subsets wisely, and then draw different\nmodels  for different  subsets.  Or, use models  like nonparametric  models  (trees),  which can better deal\nwith heterogeneity.  Note that there’s no silver bullet -- trees may not entirely  solve the issue if the\nunderlying  variance  in the data 1s extremely  high.\nWhat are some limitations  of using a third-party  dataset of foot traffic data derived from\nmobile  phones  to model the retailer’s  store sales revenue?\nAt a high level, there’s always a risk when relying on an outside third party for a core part of your\nsystem or solution. In this case, since we are using a third-party  foot traffic dataset derived from\nmobile location  data, we likely don’t have granular  information  or transparency  into what apps are\nsending us location  data, how the apps in the underlying  panel are changing,  and how users in the\nunderlying  panel are churning  and shifting. Plus, foot traffic isn’t just a raw number  that 1s directly\nmeasured  —- it’s modeled  from seeing if mobile location data for a person is near or inside  a store\ngeofence.  The company  we are buying this alternative  data from could make changes in their visit\nattribution  algorithm,  which can make maintaining  consistent  models or backtesting  difficult.\nAnother  glaring limitation  is that foot traffic to a given store does not necessarily  mean the customer\nmade a purchase,  even if the traffic correlates  well with transaction  data.\nAce the Data Science  Interview 275"
  },
  {
    "page_number": 288,
    "content": "CHAPTER  11: CASE STUDIES\nTo address these limitations,  we could look to use other alternative  datasets, like receipts data, credit\ncard data, or point-of-sale  datasets. If you had access to such data, you could see how this foot\ntraffic data and the data from those transactions  compare by normalizing  both datasets and running\ncorrelations  between  them.\nSolution  #11.2\nAmazon  - Show Recommendation\nAssume that you are designing  a system whose purpose is to recommend  what shows a user\nshould watch on Amazon Prime Video. At a high level, what data and techniques  would you\nuse?\nAlthough many variations are possible, underlying  most recommender  systems is a collaborative\nfiltering approach.The  general strategy 1s to recommend  shows that are watched  by other users who\nare similar to the user at hand. In plain English,  the idea is that people tend to watch and like programs\nthat “similar”  people have also watched  and liked.\nWhat kind of data would you use for collaborative  filtering?\nMatrices  are typically  used to represent  consumer  preferences  with respect to programming.  Assume\nan m x n matrix, whose / = 1, ..., m rows represent  m users, or, in this case, Prime Video customers,\nas of some time interval, and whose / = I, .... n columns represent  shows and movies in the Prime\nVideo catalog. Each i-th to j-th position  of the matrix contains  either no value if the user did not watch\nthe program  or a rank that measures  the i-th person’s  attitude toward the j-th program  (1.e., | through\n5, where | — dislike greatly and 5 == like greatly) if this user watched  the program  and evaluated  it.\nshows\n5\nusers} 2 3 3\n4\n/\nHow would you calculate  if two users were similar?\nWe could use this matrix’s customer  rows to measure the similarity  between viewers’ tastes in\nprogramming  by calculating  the cosine similarities  between  customer  programming  ratings. Note,\nhowever,  that customers  can have biases (expressed  by, for example,  awarding  a particular  program  a\nvery high or a very low rating), and so normalization  of scores (per user or across the entire dataset)\nis recommended.\nHow would you use the user similarity  score to make show recommendations?\nOnce we have calculated  similarity  measures  for customers,  we can use them to estimate  reactions\nof other customers  to existing  programs  they have not yet seen, or to new programming.  The basis\nof this method  is the assumption  that customers  with close similarity  scores will share similar  tastes\nIn programming.  For a given customer  who has not viewed a certain set of movies  and TV shows,\nthis method generates  recommendations  by first identifying  customers  similar to that customer  in\nviewing  choices through  proximity  of similarity  score. Programs  liked by these similar customers\nand unwatched  by the customer  can then be recommended  to them. A more finely tuned procedure  is\nto employ  viewer  rankings  of similar  Amazon  customers  to calculate  weighted  averages  ofprograms,\nand these weighted  averages  can then be used to rank programs  to be recommended  to the user.\n276 Ace the Data Science  Interview  | Case Studies"
  },
  {
    "page_number": 289,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nHow would  you address  the cold-start  problem  — that new shows  don’t have any ratings,  and\nSimilarly,  that new users haven’t  yet rated any shows?\nTo address  these weaknesses,  which stem from an absence  of data, we can do the following:\n1. For new shows,  don’t include  them in recommendations,  but in the UI, have a separate  panel for\n“new to Amazon.”\n2. For new shows, construct  measures  of show similarity  using features  of shows (genre, actors,\nlanguage,  content  of the show, etc.).\n3. For new users, use factors outside of the given matrix (user demographics,  for example)  in\nconstructing  the similarity  measure  for new users.\n4. For new users, sunply recommend  what’s popular  or trending  on Amazon  until more data is\ncollected  on a user’s preferences.\nBesides  collaborative  filtering,  what other  techniques  can you look at?\nAn alternative  approach,  since collaborative  filtering  is based solely  on user preferences  with respect\nto content,  would be to use details of shows’ content  (metadata  related to them, for example)  to\ngenerate  recommendations.  Here, recommendations  to users would be based on content to which\nthey had responded  positively  in the past.\nSay that you have a new and improved  model  that you want to test. You decide  on total watch\ntime as the top-line  metric.  You run an A/B test and it show  a significant  increase  in watch  time\nwith a p-value  of 0.04. Do you ship the new model?\nBefore  deciding,  it would  be good to make sure that the experiment  lasted two weeks or more, as you\ndo not want to stop the test early, and want to account  for day-of-the-week  effects.\nAdditionally,  it is always good practice to consult with business  stakeholders  to properly  make\nthe decision.  Even if the top-line  metric of total watch time changed  positively,  what happened  to\nthe other metrics you were tracking?  For example,  you could measure  precision:  the percentage\nof recommended  shows a viewer watched?  You could also look at recall: of the shows someone\nwatched,  how many of them were recommended  to them? Discussing  relevant  counter  metrics  and\nevaluating  the nontechnical  aspects  of the A/B test are also critical for making  a launch decision.\nAll in all, just because the experiment  is statistically  significant  does not mean it is practically\nsignificant  (has enough  impact  to do something).\nWhat counter  metrics  would you also consult?\nA valid counter metric would be the average ratings of the recommended  movies versus ratings\nof user-selected  movies. We don’t want our recommendation  algorithm  to bait users into watching\nmovies  they ultimately  like less than what they would have picked  on their own.\nWhat are other model deployment  or product  considerations  you might have?\nA few more things we could address as we deploy the model, and iterate on the product:\n¢ How computationally  intensive  is this model to train?\n¢ How often should we retrain the model to account  for the shifting  desires of the user base as well\nas new media continuously  added to the content  library?\n© Should we favor Amazon originals? If so, how much should we boost this content in\nrecommendations?\nAce the Data Science  Interview 277"
  },
  {
    "page_number": 290,
    "content": "CHAPTER  11 : CASE STUDIES\n¢ Should  we take a more editorial  approach  for recommending  shows to new user, or recommending\nnewly released  shows to people?\n* Should we update the recommendations  when the user has not interacted with the initial\nrecommendations  for a certain time? If so, what events trigger a recommendations  refresh?\nSolution  #11.3\nAirbnb  - Listing  Revenue  Model\nAssume  you are building  a model to predict  the yearly revenue  of new properties  being listed.\nWhat property  features  would you use in making  such predictions?\nGreat question!  What’s the business  purpose  of this? For example,  are we going to be showing  new\nlisters their potential  revenue,  or are we using this internally  to decide on what markets  to expand  in?\nLet’s assume  it’s to show potential  customers  an expectation  on how much yearly revenue  they\ncan bring in.\nIn that case, I’m wondering  whether  ML is the right place to start. In my mind, it feels like what I’d\nreally care about is getting exact comps (for example,  my neighbor  Joe made $8k last year, renting\nout his room for 45 days). What’s the benefit  of using ML to surface  predictions  vs. directly  showing\nnearby  listings?\nLet’s assume  that for privacy  reasons,  we can’t show specific  comps.  We also, for legal reasons,\nneed to make our predictions  personalized  based on a user’s input data. What features  would\nyou use to model the expected  yearly  revenue?\nFor a particular  property,  relevant  features  I’d include:\n¢ Property  Details:  number  of bedrooms,  bathrooms,  square footage,  if it has a kitchen\n* — Pricing  Details:  owner’s  set nightly  rate, occupancy  limit, and minimum  nights required\n¢ Location:  zip code, distance  to nearby  attractions  like the airport  or convention  center\n¢ Local Market:  prices of nearby  listings,  occupancy  rates of nearby  listings\nWhat data preprocessing  and cleaning  would be necessary  to handle  duplicate  or inconsistent\nvalues  prior to incorporating  these features  into your analysis?\nDuplicate  values are often erroneous  (due to mistakes in recording  data or lack of system\nstandardization),  so we can generally  safely discard  the duplicates  (although  we should first verify\nthat they are, in fact, duplicates).  An example  of where there may be duplicates  is when the owner\nlists the whole house, as well as individual  rooms,  as different  listings.\nFor inconsistent  values, we can perform  basic checks  to ensure that all the data within the columns\nin which the inconsistent  numbers  appear  are uniform  in type (to check whether,  for instance,  price\nhasn’t been accidentally  swapped  with zip code or house number  fields).\nAfter ensuring  data quality and that the particular  models we want to use include  numerical  data\n(which  should  be normalized  to improve  the interpretability  of  the different  features),  we would  then\nencode categorical  variables  to represent  binary or ordinal variables,  depending  on the number  of\ncategories  needed.\nWhat if many features  are sparse  — how would  you deal with it?\nNote that various  algorithms  will be more or less fit to deal with sparsity for example,  random  forests\ncan handle  sparsity  better than linear regression.  To handle  sparsity,  if you were to use traditional  ML\n278 Ace the Data Science  Interview  | Case Studies"
  },
  {
    "page_number": 291,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\ntechniques  you could look into either  feature  selection  or PCA/SVD  for dimensionality  reduction.  On\nthe deep learning  side, you can look into tweaking  various  optimizers  (some are better at handling\nsparse features  — for example,  RMSprop  and Adam)  or using embedding  layers or autoencoders  to\nlearn a smaller  relevant  feature  space.\nWhat model  would  you want to use to model  the listing  revenue?\nPer Occam’s  Razor, and for baseline  purposes,  it’s probably  better to start simple,  so we could use\na linear regression.  This makes sense because  we can expect an increasing,  roughly  linear trend\nbetween  features  like number  of bathrooms,  number  of guests allowed,  square footage,  and revenue.\nWhat about using a neural  network?  Would  you use it over linear  regression?\nNeural  networks  can capture  arbitrarily  complex  relationships  but tend to overfit  on data. They also\ngenerally  require  a large amount  of data to be useful.\nSo, if we have a very large amount  of training  data, believe that the model does not need to be\nvery interpretable  since only its predictive  accuracy  matters,  and believe  that a linear regression  will\nunderfit  because  the relationship  between  the output and the predictors  is truly non-linear,  then we\ncan try using neural networks  over linear regression.  We can also add regularization  or different  cost\nfunctions  to better address  the high variance  (chance  of overfitting).\nAny other models,  besides  a linear  regression  or neural  network  you’d want to try?\nA random  forest  or boosting  model (XGBoost)  would  probably  be better  in terms of adding  complexity\nversus  a linear regression,  but without  overfitting  too much like a neural network.\nLet’s say your model endpoint  needs to support  a high number  of queries  per second  (QPS).\nHow would  this affect  your model  choice?\nDepending  on the complexity  of the model, the amount  of compute  resources  to support  the QPS\nrequirement  would vary. If the model complexity  is high, more computational  resources  are needed\nto support  the same level of QPS versus a less complex  model. However,  high complexity  models\nmay be more accurate.  Therefore,  we can assess the trade-off  between  the resource  consumption  and\nmodel accuracy,  and then decide what model to deploy  that meets the QPS requirement.\nSay you used linear regression  and found that square  footage  was crucial  at predicting  revenue\nmade. However,  a listing’s  square footage  was missing  for 10% of properties.  How would you\ndeal with these missing  values?\nMissing  data can either be eliminated  by dropping  the rows with the missing data or ignoring  the\ncolumn  with missing  data entirely.  Alternatively,  missing  data can be imputed  using simple methods\nlike replacing  missing values with that feature variable’s  mean, median, or mode. However,  for\nsuch an important  predictor,  a one-size-fits-all  imputation  of dropping  the data or imputing  it with a\nfixed value is likely not appropriate.  Besides,  the missing  user information  itself may be a signal, so\naveraging  would not make sense. For example,  the user may not disclose  their square footage  in cases\nwhere the listing is abnormally  small, and hence, imputing  the value with the average  listing's  square\nfootage  may lead to incorrect  results.\nIn general, the best approach is to build a model to predict the missing features given the other\nfeatures (for example,  square footage can be proxied  by the number  of bedrooms  and bathrooms).\nDo you have any outside-the-box  approaches  to sourcing  the missing  square footage  data?\nWe can use push notifications  on the app to get the end users to try and provide  the missing  information\nto get an updated  revenue  estimate.  Or tell them they're listings won't rank well until they fill out all\nAce the Data Science  Interview 279"
  },
  {
    "page_number": 292,
    "content": "CHAPTER  11 : CASE STUDIES\ntheir information.  Getting  the missing  data from the users would be the most direct way to get more\naccurate  data, but there may be a low response  rate or response  bias in the submissions.\nAnother  approach  could be to use third-party  datasets,  like parcel data or county records,  which have\nthe square footage  already.  This may not help if only a single room is being rented out, but for cases\nwhere the whole property  is being listed, likely by matching  the address to third-party  datasets,  we\ncan get the square footage  information  even if the owner  doesn’t  upload  it.\nWhat action would you take if there were too many features  for you to look at thoroughly,  say,\nhundreds  of them?\nThat case describes  the curse of dimensionality,  in which the number  of variables,  or features,  to be\nincluded  in an analysis  is so great that it makes performing  the analysis  extremely  challenging.  This\nphenomenon  not only renders  visualizing  data extremely  hard — or, more likely, impossible  — but it\nalso greatly increases  the difficulty  of assessing  features  to calculate  similarity  measures.\nIn this case, the first step would be some feature  selection,  in order to filter out features  with very low\nvariance  and without  any relationship  to the target. This can be done through  looking  at correlations\nand VIFs (variance  inflation  factors)  to look for features  that have predictive  value. Alternatively,\nwe can apply specific feature selection  methods such as recursive  feature elimination  (RFE),\nwhich continually  refits the model with features  and discards  those features  with the lowest feature\nimportance.\nThen we can run some sort of dimensionality  reduction  method  that could be used to combine  features\nthat explain much of the variability  exhibited  by the data. Principal  components  analysis  (PCA) is\none such method;  by combining  two or more of the original  variables  that are highly correlated,  a\nnew varlable  is created  that is uncorrelated  with the remaining  variables  and is assumed  to represent\na latent feature  underlying  the original  variables.\nSolution  #11.4\nWalmart  - Optimal  Product  Pricing\nHow would  you build an algorithm  to price products  sold physically  at Walmart  stores?\nTo start off. I'd like to make sure | understand  the scope of the problem.  How many products  are\nwe trying to price?  Are we determining  product  prices for a particular  Walmart  store, or all Walmart\nstores?\nGood questions.  Let’s say we are trying  to build an algorithm  which can price all the Walmart\nproducts  stocked  physically  at all stores  in North  America.\nInteresting.  So, just doing some back-of-the-envelope  math, I think I read on your website  you all\nhave about 5,000 stores in North America.  And how many items does a typical  store stock?\nI love how you are sizing up the problem.  You are correct,  we have about 5,000 stores,  and at\neach store, and to keep the math easy, say we have about 100,000  items stocked  per store.\nOkay...so,  5,000 stores times the 100,000  products  is...500  million. So we need to come up with\naround S00 million price recommendations  each time our algorithm  runs. That's a lot! My next\nquestion  is the frequency  — how often do we need to determine  prices?\nGood question.  I want to hear your take — how often would you run the pricing  algorithm?\nWhat are some advantages  of running  this pricing  algorithm  more or less often?\n280 Ace the Data Science  Interview  | Case Studies"
  },
  {
    "page_number": 293,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nWith the retail industry  so competitive,  and online  retailers  like Amazon  changing  and personalizing\nprices billions  of times per day, it makes  sense for a retailer  like Walmart  to try to update  their prices\nmore often — maybe every single day. This could allow Walmart  to respond  more effectively  to\nchanges  in consumer  spending  habits,  and quickly  react to seasonal  and holiday  trends.  For example,\nthey could mark down the price of Christmas  wreaths  on December  26th. Plus, for perishable  items,\nlike in the grocery  department,  depending  on the “best before”  date and available  stock, updating  the\nprices daily could help move inventory  that is on the verge of eXpiry.\nHowever,  there’s a computational  cost to making  500 million  predictions  per day. And, of course,\npricing  products  is in the land of atoms — not bits. There’a  real human  cost to changing  pricing  too\nfrequently.  People  would physically  have to go to the items with changed  prices and append  a new\nlabel. This has both human  labor and materials  costs. Then there’s  brand perception  — since Walmart\nprides itself on everyday  low prices, if prices fluctuate  too much and customers  start to notice,  they\nmay be hesitant  to purchase,  thinking  they could get a better deal later.\nA good compromise  could be to change  pricing  of all products  just twice — set an initial price, and\nthen a lesser price if it needs to be cleared  soon, and then  a final clearance  price.\nI love that you brought  up Walmart’s  “Every  Day Low Price” strategy.  Your hunch is right\nthat we don’t want our prices  to fluctuate  too much.  Now, let’s get back to the main problem.\nBefore I outline how I'd build the algorithm,  I need to know what’s the business  goal of this\nalgorithm?  Are we trying to maximize  revenue,  profit, or some other metric?  Also, I'm a bit unsure\nhow to incorporate  the everyday  low price strategy  into our algorithm,  since it seems that could be at\nodds with maximizing  profit.\nFor simplicity,  assume  that the singular  goal of this algorithm  is to maximize  profit. With this\ngoal in mind, how could you construct  a simple supply and demand  curve for each item to\ndetermine  an optimal  price point?\nPll assume  that for any given product,  the profit will be the difference  in price minus cost, multiplied\nby demand,  where demand  is a function  of a change in price (price elasticity).  Ill also assume  the\ndemand  curve relationship  for every product is approximately  linear, and so, for any given item,\nplotting  quantity  sold on the x-axis and price of the product  on the y-axis should display a linear\nrelationship  with a negative  slope.\nIn this way, we can use the underlying  demand  curve to identify  the optimal  price point for each item\nin terms of profit, i.e., (selling  price minus per-unit  cost) x number  of units sold.\nAll I need from a data perspective  is the history of units sold and selling prices for each item of\ninterest. To actually make the supply and demand curves, | could use a linear regression  model\nfor each item that would predict the number  of units that should sell depending  on a price. In this\nregression  model,  where we are predicting  demand  using price as an input, the coefficient  of the price\nvariable  would be a measure  of the product’s  price elasticity.\nHow do you interpret  that demand  curve regression  coefficient?\nIf the value of the coefficient  was negative,  this would mean that as the per-unit price were lo be\nincreased,  the demand  would drop. Most of the everyday  household  goods that Walmart  carries should\nhave a negative  coefficient.  However,  some luxury goods, such as wine, can actually benefit from\nhaving a higher price, as it makes the product seem more premium,  which stimulates  the demand.\nIf the coefficient  value was a large negative number, it means the demand for the product rapidly\ndeclines as the price increases.  This is known as an elastic good. An example of an elastic good\nAce the Data Science  Interview 281"
  },
  {
    "page_number": 294,
    "content": "CHAPTER  I1 : CASE STUDIES\nwould be beer, as it’s not a necessity,  and is easily substitutable  with wine or hard seltzers  (Whiteclaw\n> Truly). On the other hand, if the coefficient’s  absolute value were relatively  small, demand for\nit would not be as sensitive  to price changes. This product would be considered  to be inelastic.\nExamples  of inelastic  goods would be cigarettes  and prescription  drugs.\nSo how would you double-check  these elasticities?\nDetermining  whether  similar  products  have similar  elasticities  would provide  possible  validation  of\nour findings.  For example,  items in a product  category  such as home essentials  would be expected  to\nhave similar  elasticities,  as would shampoos  and conditioners.\nWhat are some limitations  of using a linear regression  model for building  demand  curves?\nThere are a few areas where pricing alone may not capture demand perfectly.  For example,\ncannibalization  does not get taken into account  during  pricing  using a linear regression  — if one item\n1s chosen to have a discount,  this can affect other items. Additionally,  choosing  varying  pricing  can\ninvolve  mechanisms  that have different  effects  on the optimal  price — consider  discounting  an item\nusing an ad versus using a physical  coupon.  Lastly,  on a technical  level, the linear model works well\nwhen selling  patterns  don’t vary much by location  and time period  across  products,  and this may not\nhold true in practice.\nGreat,  now let’s assume  you want to do more than use a simple  supply  and demand  curve with\nlinear regression.  What other kinds of data could be relevant  to train a “black-box”  pricing\nalgorithm?\nData we could use to train our model:\n¢ tem Details:  How much did it cost to procure  this item? What is the item’s shelf placement?\nWhat category  of item is it? How much is a consumer  getting  (40 fl. oz. vs. 120 fl. oz. of laundry\ndetergent)?\n* Competitor  Pricing:  How much do other e-commerce  and physical  retailers  charge?\n* Inventory  Constraints:  How many do we have stocked?  When is the next shipment  coming  in?\nAre we in a rush to clear out our inventory  anytime  soon (due to seasonality  or if the good is\nperishable)?\n* Historical  Prices: How much, and how quickly,  did we sell our inventory  in the past? Both at\nthe current  store, and also at stores nearby?\nHow would you test your “black-box”  algorithm?\nWe can run an A’B test as follows.  First, we can take two categories  of items that should  be roughly\ncomparable  from a price elasticity,  seasonality,  and revenue  perspective.  You should also avoid\ncategories  that may tend to be in the same basket,  as there could be interaction.  For example,  1t would\nbe bad for the control  group to price video game consoles  if the test group priced video games,  as\nchanges  in one affect the purchases  of the other. In our case. toothpaste  could be the control group\nand toilet paper  could be the test group. For the control  group  of  toothpaste,  we price using the status-\nquo strategy,  and for the test group of toilet paper, we use our black-box  pricing  algorithm.  At the\nend, we monitor  for lift in core metrics  like revenue,  profit, and sell-through  rates. We should  wait a\ndecent amount  of time - say a few months -- so that the Inventory  1s able to be turned over a few\nlimes.\n282 Ace the Data Science  Interview  | Case Studies"
  },
  {
    "page_number": 295,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nSolution  #11.5\nAccenture  - Hotel Review  Analytics\nAssume  that you want to help a major  hotel chain analyze  what people  say about their brand\non websites  like Facebook,  Twitter,  and Reddit.  Why might this be useful,  and how would you\ngo about  doing it?\nBefore  beginning,  just want to clarify the data — what kind of content  are we looking  at? Are we\nlooking  at reviews,  comments,  threads,  or a combination  of these? Also, do we need to go out and\ncollect  the data or is the data there and just needs to be analyzed?\nGreat question  — for this situation,  let’s assume  we’re talking  about just public online text\nand reviews  posted  on the biggest  social media sites, and that it’s already  been processed  and\ncleaned  and stored  in a database.  Why might  analyzing  this data be helpful  for the hotel chain?\nSocial listening  is important  — looking  at reviews from Yelp, Tripadvisor,  and Google Reviews,\nwhere there’s  an explicit  rating, isn’t enough.  In the viral era, a Tweet or Reddit  thread can blow up.\nIt would  be important  for the hotel chain to consider  both customer  feedback  and public opinion.\nPosts on social media,  whether  they be positive  or negative,  have the potential  to influence  possible\ncustomers.  Moreover,  the sheer volume  of opinions  posted online about any subject magnifies  and\nincreases  the scope  of their influence.  The client’s  ongoing  objective,  then, is to maintain  and improve\npublic perception  of the brand, and thus, obtaining  a better understanding  of public perception  is of\nvital importance.\nAnd what is the strategic  value of doing such an analysis?\nFirst, it can serve as another metric to monitor  the brand’s perception.  Also, understanding  the\noverall  themes  and commonalities  behind  low sentiment  posts can lead the hotelier  to fix commonly\noccurring  problems.  This can help raise metrics  like NPS (net promoter  score).\nWhat are some examples  of how the hotel could make the results of the sentiment  analysis\nactionable?\nResults of the sentiment  analysis would primarily  help make sure customer  satisfaction  remains\nhigh. There would be a focus on identifying  dissatisfied  customers,  and then making sure they\nfeel understood  and accounted  for. One way to do so would be to proactively  reach out to posters\nof negative  posts on social media platforms  and offer them refunds if the hotel or employee  was\nperceived  to be in the wrong. Another  would be to investigate  the causes that motivated  negative\nposts and make necessary  modifications  to address them. Properties  that were the source of the\nnegative  feedback  could be alerted, in close to real time, about the problem  so that changes  could be\nmade to address  the issue.\nHow would you do sentiment  analysis?\nSince sentiment  analysis is such a common  problem,  it’d be worth trying to find existing  APIs or\npackages  that can do this task, like NLTK and TextBlob.  You could also use transter learning,  since\nthere are great off-the-shelf  NLP models that can be extended  and tuned from places like spaCy and\nHugging  Face.\nWhat preprocessing  would you do with the data?\nSince most text online is messy, basic text preprocessing  techniques  like fixing text encoding,\nstripping  away HTML, removing  stopwords,  stemming  or lemmatization,  and finally, vectorization,\nwould likely be needed.\nAce the Data Science  Interview 283"
  },
  {
    "page_number": 296,
    "content": "CHAPTER  11: CASE STUDIES\nCan you describe  some ways of turning  the review text into numerical  features  that can be used\nfor machine  learning  models?\nSure, the general  process  is often called text vectorization.\nThere are several  possible  text vectorization  methods:\n¢ Bag-of-words:  represents  text by counts of the words comprising  it\n¢ N-grams:  allows analysis of text by identifying  contiguous  sequences  of n words or letters\npresent  within the text\n¢ TF-IDF:  an abbreviation  for frequency-inverse  document  frequency,  TF-IDF  is a technique  that\nindicates  the importance  of a word within a single document,  when compared  to the importance\nof the word in a larger corpus\nHow would you then run a model using those text vectors?\nFollowing  whatever  processing  technique  is used, each piece of content is represented  by a vector\ncontaining  the results of the step above. At this point, a variety of classification  methods  can be used\nto classify the sentiment  underlying  the text, including  logistic regression,  random forests, kernel-\nbased methods  such as support  vector machines,  discriminant  analysis,  and neural networks.  Finally,\nthe effectiveness  of our classifier  can be assessed  through a confusion  matrix and computation  of\nmetrics  such as precision  and recall.\nGood job explaining  how to categorize  reviews  based on sentiment.  Let’s switch gears to topic\nclassification.  Why might  categorizing  the reviews  into different  topics  be useful  to the business?\nCategorizing  reviews  into different  topics can be useful since feedback  can be more efficiently  routed\nto the different  subdepartments  of the hotelier.  For example,  if you could automatically  split reviews\ninto topics such as check-in  experience,  room quality,  or room service,  the corresponding  teams like\nthe front desk, housekeeping,  or kitchen  could more efficiently  act on customer  complaints.\nHow would  you go about grouping  the reviews  into categories?\nTo do the grouping,  we can do some basic clustering  on the content  based on vector representations,\nusing an algorithm  such as k-means.  Alternatively,  we can model  the text-related  data over an underlying\nset of topics using Latent Dirichlet  Allocation  (LDA). LDA assumes  each post to be a distribution  of\ntopics and that each topic is a distribution  of words. Ultimately,  we should  be able to characterize  posts\nand group them into appropriate  buckets,  1.e., as expressing  various  sentiments  regarding  the brand.\nSolution  #11.6\nFacebook  - People  You May Know\nSuppose  you are to build out Facebook’s  friend recommendation  product,  also known as its\n“People  You May Know”  (PY MK) feature.  How would you go about doing so?\nBetore  we begin -- just wanted  to clarify  - - what is the goal of this feature?  Is it to maximize  friend\ncount for existing  users? Or is it to drive engagement  on the product  -— whether  that be a shorter-  or\nlonger-term  basis?\nGreat questions.  Let’s say that we want to increase  the number  of meaningful  connections\nformed.  How would you go about building  the PYMK  feature  for this goal?\nGot it! Okay...well,  two approaches  are possible.  The first involves  recommendations  based on\nuploaded  contact information.  A signup flow typically  asks a user to import their phone contacts\nor email address  book (especially  important  for new users) and then recommends  you to friend the\n284 Ace the Data Science  Interview | Case Studies"
  },
  {
    "page_number": 297,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\npeople  within your contacts  who are on Facebook.  You could offer friend suggestions  to the people\nin the contact  book, to friend  the person  who just signed  up. The second  approach  to recommendation\ninvolves  leveraging  a user's current  social graph, recommending  friends  of current  friends. Likely,\nwe'd want to use a blend of both approaches.\nLet’s talk more about the second approach  — how would you go about leveraging  a user’s\nsocial graph  for the PYMK  feature?\nWe can rank the potential  friends  for any given user based on the social graph. For example,  you can\nmake  a candidate  list of all second-  and third-degree  connections.  Then, we can rank this candidate\nlist based on the likelihood  that the user and the candidate  become  friends.\nWhat are some features  you’d use to measure  the potential  for two people  being friends?\nTo look at the strength  of potential  friendship  between  two people  X and Y, we can start by simply\nlooking  at how many mutual  connections  they have. To generalize  this concept,  the more two users\nhave in common,  the more likely they are to have a potential  friendship.  Signals  like:\n¢ Profile  Similarities:  age, alma mater, employer,  hometown,  current  city, mutual friend count\n¢ In-App  Activity:  high engagement  with similar friends, attending  common  Facebook  events,\nvisiting  each other’s  profile  pages,  commenting  on the same post, being tagged  in photos  together\n¢ Ecosystem  Signals:  being connected  on Instagram,  or messaging  on Messenger,  Instagram\nDMs, and WhatsApp\n© = Off-App  Signals:  email, phone number,  mobile  GPS location\nCo-occurrences  of any online  presence  would serve as signals  for PYMK.\nCan you give examples  of specific  methods  you would use to rank potential  friends  for a given\nuser?\nWe could use classification  algorithms,  like logistic  regression  or naive Bayes, to predict  for a given\nuser the likelihood  ofa friendship  with another,  non-friend  user. That is, the target variable  1s whether\nthey will become  friends or not, and we use the various  features  aforementioned  to train a model to\nlearn that relationship.\nWhat about unsupervised  techniques  for ranking  potential  friends?\nUnsupervised  techniques  such as K-means  or principal components  analysis (PCA) can identify\nsimilar users who are not yet friends with that user, and then those non-friends  that overlap to the\ngreatest  extent with the user’s existing  friend base are ranked highest. By “overlap,”  we mean they\nhave the most mutual friends in common.  In both cases, for any given user, we end up with a set of\nrankings  for potential  friends.\nDoes the model setup you described  pose any potential  problems  for new users?\nOne major problem  with our model setup would be assessing  new users, especially  PYMK (People\nYou May Know), who choose not to upload email and contact information.  In the machine  learning\nsetup described  above, relevant rankings of PYMK would be difficult since there is a dearth of\nappropriate  data to feed into the algorithm  in this case.\nWhy do you think — from Facebook’s  perspective  — new users are important?\nFacebook  is a social product whose main value to a user is realized only after that user has added\na sufficient  number of friends; otherwise,  that user’s feed is practically  empty. Facebook differs\nfrom other social media platforms  such as Reddit or YouTube,  where much content Is available  for\nAce the Data Science  Interview 285"
  },
  {
    "page_number": 298,
    "content": "CHAPTER  11: CASE STUDIES\nconsumption  by new users without them having to form friendships  first. This is known as the cold-\nstart problem  — you need supply (friend content)  to get demand  (usage by new users), but can't get\nfriend content if new users churn out quickly  and never make friends.  Thus, helping  new users make\nfriends quickly is strategically  important  for Facebook.  A job well done here would significantly\nimprove  the new user onboarding  process  and reduce new user churn. This keeps the social graph the\nbest in the business,  which is one of the strategic  moats Facebook  has against  other social products.\nWhat are some product  ideas you have to help new users make more friendships?  Both PYMK\nand non-PYMK  related  ideas are okay — just want to brainstorm  with you.\nI love product  brainstorming.  Some ideas that come to mind are:\n¢ Boost New Users: Boost the odds that new users appear in existing  users’ PYMKs,  thereby\nincreasing  a new user’s inbound  friend requests.\n¢ Friend  Chaining:  \\f a new user accepts  all their inbound  friend requests,  instead  of leaving  the\nsurface  empty,  show PYMK  there to keep the friending  going.\n¢ Get More PYMK  Signal: Existing  users simply ignoring  or removing  new users suggested  in\ntheir PY MK recommendations  could also be a source  of training  data for the PYMK  algorithm.\n¢ Increase  PYMK  Units: Show more PYMK  news feed units in their first two weeks unless they\nhit a certain  number  of friends  made.\n¢ Use Gamification:  Add a progress  bar to push new users to make a certain  number  of friends.\nThis progress  bar can also incorporate  other important  steps like uploading  a profile picture\nand filling out the new user’s bio details. By pushing  for people to have fleshed  out profiles,\noutbound  friend requests  from new users are more likely to be accepted.\nSolution  #11.7\nStripe - Loan Approval  Modeling\nAssume  you are working  on a loan approval  model  for small businesses.  What metrics  would\nyou use to evaluate  the model?\nFirst, can I know more details about the loan approval  model?  Are we approving  businesses  for\ntheir requested  loan amount,  or are we instead recommending  an upper credit limit that we could\ncomfortably  loan out to a business?\nGreat point — let’s say that businesses  apply to us with a fixed loan amount  in mind.\nCool - another  question  then. I’m curious;  is this a binary situation  —- a loan is either approved  or\nnot, and a business  pays back the principal  in full or not? There aren’t any half-repayments  or debt\ncollection  to factor in?\nFor simplicity,  we will ignore  those cases and just assume  a binary  response  — loans are paid\nback in full or defaulted  on. What metrics  would you use to evaluate  this loan model?\nOur loan model can produce  a probability  score for whether  a particular  loan application  will default\nor not. So assuming  some threshold  (say 0.5), then each transaction  can be classified  accordingly.\nThat is, if the score is greater  than or equal to 0.5, then the application  is classified  as likely to default;\nif it is less than 0.5, then it is classified  as likely not to default.\nTo evaluate  the model, we look at precision,  recall, and the corresponding  precision-recall  curve.\nNote that we wouldn't  look at accuracy  since this is a highly unbalanced  problem,  as likely only a\nsmall percentage  of loans will be defaulted  on.\n286 Ace the Data Science  Interview  | Case Studies"
  },
  {
    "page_number": 299,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nWhat do false positives  and negatives  mean in this context?\nA false positive  is when the model predicts  the loan would default  when, in fact, it did not. A false\nnegative  is when the model predicts  that the application  will not default when, in reality, it does.\nThus, higher  precision  and recall indicate  smaller  numbers  of false positives  and false negatives,  so\nthis would minimize  the numbers  of defaulted  loans and maximize  the number  of valid loans that\nStripe has to deal with, thereby  improving  bottom-line  profit.\nFrom the point of view of the business,  should both false positives  and false negatives  be\nweighted  equally?  Why or why not?\nA false negative  in this context  means a loan was made when the user actually  defaults,  and so the\nloss incurred  would be the entire principal  amount.  For simplicity,  I’1l ignore how debt collection\nagencies  could technically  recoup  some part of the principal,  and just call it a total loss of the principal\n(1.e., a “write-off’).\nA lesser  monetary  issue would  be false positives,  where  no loan was issued when it should  have been.\nFor simplicity,  assume  a 10% interest  rate on the loan, and all the interest  is pure profit. That means\nfor each false positive,  the business  loses out on the opportunity  to make 10% of the loan principal.\nAs such, for this case, Stripe incurs a 10:] cost for false negatives  to false positives.  We can use this\nratio to evaluate  our classifier  by trying various  thresholds  and assessing  the weighted  precision  and\nrecall that would result from their use by constructing  a precision-recall  curve (or an ROC curve if\nthe false positive  rate rather than the false positive  is used).\nWould  relying  on this type of model  produce  any edge cases, especially  under  scenarios  having\nincreased  uncertainty?\nThe status of some loans would not be clear even if the model flagged  them as likely due to default,\npossibly  due to regulatory  issues,  and so would require  human  review.  These could involve  the size of\nthe loan being requested  or some aspect of it that appears  questionable.  Conducting  manual  reviews\non “borderline”  cases where the model generates  a likelihood  of default  that is barely above or barely\nbelow the threshold  would also be a wise precaution  and could be part of an overarching  tiered\nsystem, in which the model’s  rating comprises  the first layer and determines  how much additional\nhuman  review  of the loan is needed.\nOverall, implementation  of such a multitiered  system could further reduce the numbers  of false\npositives  and false negatives  the Stripe accepts  and acts on. Note that the cost of manual  interventions\nshould  be factored  into the total cost calculation  — human intervention  is a good way to avoid costly\nmistakes,  but in itself is also costly.\nWhat are some features  you would recommend  incorporating  into the model described  above?\nModel construction  would involve two feature dimensions.  The first is at the loan applicant level\nand includes  the applicant's  demographics  (assuming  it is legally compliant  and ethical to do so), IP\naddress, browser, and financial health: bank account balance, credit worthiness,  whether there are\nany outstanding  liens or judgements.\nThe second set of features is at the loan application  level and includes answers to questions  such as\nthe following:  how complete  (and reasonable)  were the answers provided  on the loan application,\nhow much money is the business  asking to borrow, what is the purpose of the loan, and how much\nare they putting  up for collateral?\nAce the Data Science  Interview 287"
  },
  {
    "page_number": 300,
    "content": "CHAPTER  11 : CASE STUDIES\nWhat other methods  would you recommend  using to improve  the model?\nOne way to improve  the model would be to utilize reject inference,  which is based on the idea that\nnot accounting  for rejected applications  introduces  bias into the generating  model. Models of loan\ndefaults are generally  trained only using data from previously  granted loans, thereby introducing\nsampling  bias into the modeling  process. In contrast,  reject inference  involves  using another  model\nthat has been trained using rejected-application  data. Both the original model and the reject model\ncan then be used in tandem  to obtain  a final score.\nAdditionally,  we could integrate  anomaly  detection  along with the model. What makes people  default\non loans may change  over time, yet models  are trained on historical  examples  of what encompassess\na loan default. By supplementing  the model with anomaly  detection,  we can flag more odd cases in\nreal time. However,  anomaly  detection  isn’t a silver bullet. With the large feature space and volume\nof data points coming in, there will almost always be at least several anomalies  on any one given\ndimension,  due to the curse of dimensionality.\nHow could you test that your new loan approval  model was better  than the baseline  model?\nYou could run an A/B test, but it may be difficult  because  of the long time horizon  of loan repayments\nand having a business  default on a loan. For a simpler way to test, you can do an offline model\ncomparison  on the various  metrics,  and look at a paired  t-test between  the two models.\nSay you did run an A/B test on the new loan approval  model, and you find that the revenue\nfrom loans increases,  but the p-value  is 0.06. What would you do?\nBecause  there can be a fair amount  of noise  with p-values,  we shouldn’t  hastily jump to the conclusion\nthat the test is not statisucally  significant  since p > 0.05. It’s likely that with a larger set of data, the\np-value  would be difterent.  Therefore,  it would be best to run the test longer  to observe  any drift in\nthe p-value  in order to get clues on longer-term  behavior.  Additionally,  we should look at the effect\nsize of the revenue  and the changes  in counter  metrics.  This would help us get better insight into the\nquality  and impact  of the experiment  before  any launch  decision  can be made.\nSolution  #11.8\nInstagram  - Ranking  for Instagram  Explore\nHow would you provide  content  recommendations  for Instagram  Explore?\nBetore we begin, just wanted to make sure | understand  the Instagram  Explore  feature.  This isn’t\nyour main newsfeed,  where you see posts from accounts  you follow,  right? This is a surface  where\nyou can see a feed of customized  photos  and videos -— often from accounts  you don’t follow  — that\nis continually  refreshing,  correct?\nYep, you've  understood  Instagram  Explore.  Any other questions?\nI’m curious  about any SLAs for our system,  and the scale we are dealing  with. I know that Instagram\nhas many hundreds  of millions  of users, so there are probably  many billions  of pieces of content\non Instagram  that are relevant  inventory  for Explore.  On top of that, I’m wondering  how real time\nthis needs to be? I’m guessing  we probably  want to serve the most relevant  content  in real time for\nevery user. With I billton Monthly  Active  Users. we might have to support  | million  concurrent  feed\nrefreshes  per minute?  Does that sound reasonable?\nThat sounds  reasonable  — let’s go with your assumptions.  So, what would be your high-level\napproach  for providing  recommendations  for Instagram  Explore?\n288 Ace the Data Science  Interview | Case Studies"
  },
  {
    "page_number": 301,
    "content": "ACE THE DATA SCIENCE  INTERVIEW  | HUO & SINGH\nAt a high level, we can focus on identifying  accounts  that have content  a user would find interesting,\nrather than at the media (content)  level. Said another  way, we use a collaborative-filtering-style\napproach  by recommending  content  from accounts  that are similar  to the ones the user interacts  with,\nrather  than recommending  content  based on topics.\nWe take an account-based  approach  because  the universe  of possible  media  is very large, and there is a\nlot of new content  coming  out every second.  By doing recommendations  mostly  based on the account\nlevel, and then pulling  the most recent or most relevant  media from the recommended  account,  we\ncan do a first pass for candidate  retrieval.  This helps to narrow  down the universe  to a manageable\nsubset  that we can then rank for each user.\nWhat  features  would  you use for candidate  retrieval?\nWe can come up with an “embedding”  per account,  which treats account  ID’s that a user interacts\nwith (e.g., a person  likes media from an account)  as a sequence  of words in a sentence,  analogous  to\nword2vec.  We can call this an “ig2vec”  embedding.  Under this setup, if an individual  interacts  with\na sequence  of accounts  in the same session,  it’s likely to be more topically  similar  to those accounts\nthan compared  to a random  sequence  of accounts  on Instagram,  which helps with finding  topically\nsimilar  accounts.\nAs an alternative  approach,  we can build a matrix that stores interactions  of users and other IG\naccounts,  and a factorized  version  of this matrix  can be used to explore  account  similarity.\nHow does the model utilize  those features  in order  to come up with candidates?\nWe can first calculate  a distance  metric between  two accounts  using either cosine distance  or dot\nproducts.  Then we can use KNN to find similar  accounts  for any account  in the embedding  and train a\nclassifier  to predict  a set of accounts’  topics based on that embedding.  By retrieving  accounts  similar\nto those that a particular  person expressed  interest  in, we narrow  down the universe  of content  to a\nsmaller  personalized  ranking  inventory.\nWhat models  would you use for the ranking  step?\nWe can consider  a variety of classification  models. At Instagram  scale, training  a neural network\nseems appropriate.  Rather than having a binary outcome  (such as recommending  the post or not)\nto reflect the variety of options a user has with regards to a post, we can treat this as a multi-class\nclassification  problem.  For each post, we can predict the probability  that it is liked, commented,\nshared, hidden, and reported  by a user. These probabilities  can be weighted  to come up with a final\nrecommendation  score. The weights for the actions can be defined based on a simple statistical\nanalysis  that links each action to some top-level  KPI, like user engagement.  This approach  gives us\nthe flexibility  for downstream  stakeholders  to tune what goals they want from the system.\nShould  this model be deployed  in batch or online?\nWe would want to run it online since the predictions  have to be real time and include users’ most\nrecently used activities.  For online, feature engineering  needs to be optimized,  since at inference\ntime the features need to be plugged into the model with low latency. Additionally,  we want to\nmake sure in this setting that the features,  which may come from a variety of data sources, can be\nprecomputed  and stored in a real-time storage environment  and readily accessible.  For example,\nwhile most features  are stored in batch in HDFS, it is not possible  to query directly very quickly, so\nthe features  should be stored in Cassandra  or an analogous  service.\nAce the Data Science  Interview 289"
  },
  {
    "page_number": 302,
    "content": "CHAPTER  11 : CASE STUDIES\nWhat are some challenges  and what rollout strategy  would you use at inference  time?\nIn terms of rollout strategy,  we can choose between  the following:\n1. asingle  deployment,  where all users see the changes  directly\n2. controlled  deployment,  where a smaller subset of users see the new model and the majority  see\nthe previous  model\n3. “silent” deployment,  where both are deployed  but users do not see the predictions  of the new\nmodel\n4. “flighting”  deployment,  where you can run online  A/B tests.\nThe first is the simplest  of the four, but costly if there are mistakes;  the second can be complex  to\nimplement,  and the third doesn’t  allow for seeing how users react to the model. The fourth is the best\nfit for this particular  use case.\nHow would you assess model performance  over time?\nThe general  problem  is that stale models  cannot  capture  changes  in user behaviors  or understand  new\ntrends. In terms of quantifying  model drift, we can (1) monitor  model performance  over time on a\nfrequent  basis, and (2) look at KL-divergences  —-- a measure  of how similar  two distributions  are —\nin key behavioral  distributions  of the models  as well.\nCan you describe  what KL divergence  is?\nKL divergence  measures  the similarity  in distributions  --- since machine  learning  algorithms  are\nparameterized  by the distributions  of input data, we can use KL divergence  to check the distributions\nof the input features  over time. If the feature  distributions  are changing  significantly  from when the\nfeatures  were launched,  then we want to retrain the models.  Note that 1n practice,  simply retraining\nthe models on a periodic  basis could be an easier approach  than monitoring  each feature’s  KL\ndivergence  over time,\nSay that you ran an A/B test on your improved  model  versus  a baseline  model  and did not see\nany correlation  between  the performance  of the model  and specific  business  metrics  of interest\n(engagement).  Why might this be?\nThere are several  possible  angles here, stemming  from the various  parts of the experiment.\nThere could be some over-optimization  of these specific  metrics  of interest,  which leads to saturation\nof metric improvements  from further  model improvements.  For example,  it could be that the A/B test\nmetric might be ranking  relevance,  BUT since the content  is already  relevant  enough  for folks, it’s\nnot strongly  correlating  with increased  user engagement  any more.\nOn the user side, it 1s possible  that model performance  past a certain point may have a negative\neffect on user experience.  For example,  the recommendations  are too niche or specific,  or people\nget weirded  out by the model, and then don’t use the product  because  it’s creepy. In that case, an\nimproved  model may lead to decreased  engagement.\nThis might not seem reasonable,  but how many times have you heard the false conspiracy  theory  that\nFacebook  and Instagram  ads are hyper-targeted  because  they listen to your conversations  through\nyour phone’s  microphone?  It is possible  for a recommendation  to be too good!\n290 Ace the Data Science  Interview  | Case Studies"
  },
  {
    "page_number": 303,
    "content": "AFTERWORD\nThank  you for reading  Ace the Data Science  Interview!\nIf you’ve  got any questions,  feedback,  or praise,  please  reach us at:\nhello@acethedatascienceinterview.com\ninstagram.com/acedatascienceinterviews\nYou should  also connect  with us personally!\nhello@nicksingh.com\nlinkedin.com/in/Nipun-Singh\ninstagram.com/DJLilSingh\ntwitter.com/NipunF  Singh\nkevin.w.huo@gmail.com\nlinkedin.com/in/Kevin-Huo\ninstagram.com/K  whuo\nWe couldn’t  end this book without  thanking  the many amazing  people who’ve  supported  us!\nThank  you to the people  who’ve  gone out of their way to help us in our careers:\nGreg Mand, Auren Hoffman,  Evan Barry, Lauren Spiegel,  Jonathan  Wolf, Ross Epstein,  Shaxun\nChen, Sheng Zheng, Josh Buffum,  Riu Kallivalappil,  Kristie Moi, Parth Detroja,  Aakash Shah,\nRipley  Carroll,  David Booth\nThank  you to our friends  who’ve  supported  us on the book:\nAdam Rosenberg,  Rohan Raval, Himanshu  Ohja, Neeraj Gandhi, Brandon Hohenberg,  Elakian\nKankaraj,  Atul Nambudri  Quinn Li, Anisha Marya, Naveen lyer, Vyay Edupuganti,  Knstina Hu,\nMayank  Mahajan,  Alex Wang,  Andrew  Jiang, Mat Samuel,  Kara Sheldon,  Noah Yonack,  Jaina Mehta,\nRucha Bhat, Leo Agnihotri,  Shivangi Mistry, Sri Vasamsetti,  Danil Kolesnikov,  Vishnu Kumar,\nHarnoor  Singh, Lawrence  Hook, Salomon  Lupo, Mehar Virdi, Richard  Liu, Philip Ruffin,  Nimisha\nJain, Hamza  Khawaja,  Hari Devanthan,  Shota Ono, Samraaj  Bath, Ashley Belfort,  Patrick Rivera\nAnd thank you to everyone  in the Data Science  community  who volunteered  their time to read early\nversions  of our book, or since publishing,  have found mistakes  or typos we’ ve corrected:\nAdvitya  Gemawat,  Jeffrey Ugochukwu,  Lars Hulstaert,  Michelle  Scarbrough,  Nicholas  Vadivelu,\nCatherine  Yeo, Jordan Pierre, Rayan Roy, Jack Morris, Neha Pusarla, Aishwarya  Srinivasan,\nSiddhartha  Sharan, Daliana Liu, Marco Sandoval,  Timothy  Wu, Prithika Hariharan,  Tania Dawood,\nFaith Chung,  Jai Raghuvanshi,  Devan Shanker,  Lindsay  Warrenburg,  Jie Cai. Alice Hau, Jon George,\nHarris Vijayagopal,  Thiru Veeran, Sourabh  Varshney,  Ayan Sengupta\nAnd thank you to our editors and book creation  gurus:\nWendy Martindale,  Rajiv Kumar, Mykola  Shelepa,  Mary Graybeal"
  },
  {
    "page_number": 304,
    "content": ""
  },
  {
    "page_number": 305,
    "content": ""
  },
  {
    "page_number": 306,
    "content": "Made inthe USA\nLas Vegas, NV\n07 April 2022\nqn\n47068759R00168"
  },
  {
    "page_number": 307,
    "content": ""
  }
]